{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/github/ga642381/ML2021-Spring/blob/main/HW01/HW01.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "def save_result(preds):\n",
    "    with open('submission.csv', 'w', encoding='utf-8') as f:\n",
    "        f.write('id,tested_positive\\n')\n",
    "        for i, pred in enumerate(preds):\n",
    "            f.write('%d,%f\\n' % (i, pred))\n",
    "    \n",
    "    return\n",
    "\n",
    "# loss_record = {\"train\": [338.8324902564143, 300.1206629882458, 309.41258023480356, 145.69407738887747, 289.8134118404678, 291.6760574651674, 334.3836256452923, 315.8737479800885, 209.23943781182967, 236.81603356977706, 189.86549218773973, 160.52760421077537, 280.0016513158014, 267.7548507515039, 278.15674118761734, 166.38160025578884, 319.30744387980326, 204.28195872446082, 181.99305093328454, 218.22128231221092, 162.26057086959662, 64.73518166531008, 110.40258774163587, 208.9251678678948, 182.64052071440454, 138.2169423235864, 48.413575080988345, 37.013469786220625, 62.99520617263815, 69.15946845151676, 110.16284090167868, 41.1942962857465, 46.32759518000252, 142.99812931242602, 58.73162433100042, 55.741664041161016, 49.190149569387515, 26.404623853955812, 18.912452561765697, 17.669911249111045, 20.72781174209063, 19.919526969337554, 39.410228585289396, 22.26082406120151, 40.471893575253404, 17.94784264800843, 30.74124783206616, 25.516015454757937, 84.51766995716228, 12.456371135225762, 14.916355149908838, 39.8936788554534, 32.389191253323276, 90.53185351897793, 46.0995767945348, 18.168463603125815, 10.794420508844489, 10.18186166360786, 10.259098709170187, 17.319344481075102, 37.41348503515055, 15.208015544289328, 24.550035608751806, 6.0516923346327705, 13.597652870867075, 8.548588873450464, 20.203206332631538, 34.185418266576434, 101.15420361278828, 43.30534397889758, 12.104288293881314, 6.9079855555659435, 11.631063355645985, 34.59621762418044, 17.665342942023553, 46.71013883192069, 17.886925583594394, 32.29982244201045, 9.346324818388478, 5.033926815280658, 7.9935383060262915, 5.315085720233034, 12.088883773529194, 33.44042677147008, 101.23013100210238, 33.60359628756596, 17.060327134306736, 5.070760844804232, 4.829672690335647, 6.5203425038814755, 7.679362754432013, 12.12700061009711, 9.146226402790425, 13.214848511170244, 22.076412487453517, 28.800103605506905, 4.8163071459594065, 4.4151502815184624, 4.593426453944634, 3.3734777140873513, 5.836599846037702, 6.078169516505016, 6.185683332588629, 12.68662644699382, 14.412390848977656, 37.649593458904825, 4.953026723853803, 3.78695703619407, 4.778810923145538, 3.2315823358099633, 5.48613161230946, 5.647293319228187, 4.706884808532317, 7.868641051882097, 10.299799199530614, 22.948683678946725, 3.149560684151326, 2.556178629706999, 2.553797902547779, 2.6397642984473277, 2.338417790399397, 3.032053577197205, 3.298178388926864, 3.6566909491310375, 2.7725958726210846, 2.19557134095202, 2.9903543561589805, 2.2274606042966716, 2.523463354915582, 2.9741672586312546, 3.3500289798218064, 4.018673068248991, 6.862403806092638, 14.691688853633618, 10.797449834068814, 22.54570148673751, 12.63021488214626, 31.280814934741993, 4.855538566256259, 6.19676942019173, 4.947974120020652, 7.144830023742216, 7.020977280273048, 16.829164625733963, 9.20165889591547, 18.767181840816615, 5.231822188083326, 4.312841907055251, 2.756975032879118, 3.00673263163634, 3.0690577547640467, 3.389446699092436, 2.8037595907918895, 2.4709425238820595, 1.6854016864923587, 2.2027052441748824, 2.338260172502632, 3.2526473142446197, 2.824131940401834, 2.3709247179451243, 1.8106800730126524, 2.4727933867050207, 2.280975189310591, 2.2806716723283316, 3.1487829412189736, 3.6615328175418593, 4.412943561715539, 5.814765116371505, 14.395957618379036, 12.411907185991561, 12.067936234966893, 4.4370873856858815, 2.981066989823312, 6.575822140698181, 4.992138071069727, 16.62753141240558, 19.67860888432716, 40.24788501500451, 6.820352192342027, 2.762915425671733, 4.870327421324715, 11.766566599650009, 16.64568249378288, 7.5187213829234025, 13.996754709195127, 36.9150145415871, 3.583460992957627, 2.2308804695987106, 2.5937633560009927, 3.402988703972714, 3.009843919162161, 3.554928753101159, 2.902871415878872, 1.6784578599706417, 1.9035268578268174, 1.4994215683164525, 2.212154976063716, 2.913067086973258, 2.89644793815801, 2.805163547413807, 3.1978842774976277, 3.3998559990996244, 2.408560725122295, 3.610500604296875, 5.76626710270368, 8.828009039672782, 5.903157650896437, 8.808886389821609, 7.4998162288074255, 32.316752056491474, 15.56654417585118, 31.55867070324998, 7.509705103686097, 9.680599796916788, 8.35078297009555, 15.966606762975719, 5.950185767705416, 12.608185701285569, 3.7232026981107817, 3.53089177187456, 3.5547475499944596, 3.663786270000208, 4.126064182256027, 5.174231558656941, 4.4237306418925355, 5.433056584131636, 3.470354543924751, 5.164539924347686, 4.233809537323239, 8.391455056756438, 6.690663208880638, 10.474103741852897, 1.613650192436233, 3.0146048313622824, 4.024272786458019, 2.7060164577500605, 1.8123976088056506, 1.5907337861321715, 2.203556214319332, 1.654234966566828, 1.766649085333844, 1.9679593495350594, 1.665219140202436, 1.9885870851142515, 2.3917139556634197, 1.999903891801406, 2.671947996203217, 3.0502915927220915, 3.942301594004851, 4.011532869583377, 3.162243843949412, 1.3468152595462952, 2.1681200419674074, 3.3878347956910284, 5.527790374441944, 10.936712649144898, 28.811973092317295, 4.538650928494108, 8.363785948562608, 2.9295432599225384, 2.646880741454667, 2.1735672111357935, 2.766817348539472, 2.8717450965745255, 2.9330304933989435, 3.318409123189922, 4.806196543367503, 1.9522280632833469, 1.3708521585762732, 1.829975849410967, 1.407096709547014, 2.196378200939091, 1.3288149330576953, 1.2743600183728472, 2.0869588737518, 1.7367286024683808, 1.6018070067959131, 1.5512993700688655, 1.2273916584228708, 2.2499543484082722, 3.83738183170415, 6.1099766986455135, 10.679657185821613, 7.76944763274611, 21.304239699275023, 4.702645436044126, 5.166137393291516, 2.047993197875022, 3.1065958919357257, 3.0948922024296968, 7.3516691827346765, 7.901220507002014, 15.485394785284958, 3.8154232363213074, 4.713157686134897, 2.9158238912346093, 5.034881269291264, 7.969565151028514, 8.327658490486161, 3.6833281374533264, 1.7485732878929219, 1.386503247681834, 1.970354567232902, 1.67495071784802, 1.6027755197200704, 1.4352633778698844, 2.224163215032316, 1.8930573623995617, 1.8000540953353896, 1.3286156779367588, 0.9708548527232406, 1.1894302280821867, 1.5281317953227482, 1.4750071739658654, 1.480193687000633, 1.0526797404784565, 1.3545740739972507, 1.4447871204908531, 1.748989458831172, 1.1069160901004726, 1.4055766222573134, 1.6134106989576296, 1.3619627389795599, 1.3153847626723734, 1.2055676990533408, 1.8147765658656039, 4.135409726108141, 6.146914944847218, 10.501950430049865, 11.905179437084705, 32.482997056250134, 2.20383688886092, 4.160669150814636, 6.4559550598550235, 8.982688130156323, 10.732979003098878, 14.409562197781927, 3.0282393007026416, 2.339565614372817, 1.8200000595431343, 2.3535825717999943, 3.1698306906827813, 2.2777988034058367, 1.2889046704602247, 1.340621090269041, 1.8303797086189368, 1.1979113734175837, 1.1141500436109422, 1.2903472328565377, 1.2228389874519257, 1.1774012241628653, 1.3172786928873408, 2.245039405233087, 3.085560771985923, 5.807804927047279, 8.889862198562625, 2.5514752007712493, 3.6178941722861717, 4.388028624884762, 5.211645885059664, 3.468907245211062, 4.0690766394448215, 2.349025837760106, 1.5287088622455483, 1.7085556127168755, 1.5504187125323758, 1.4397823574135038, 1.3027397419379494, 1.5559180216412596, 1.1435223091881748, 1.481474904665061, 1.212523817983449, 1.0058059205013534, 1.1750261830512176, 1.2946482648345339, 1.5499493839784517, 1.4137841826842537, 0.9464607912169936, 1.3365920344038935, 1.1020059166000453], \"dev\": [1.4857651925851194, 1.224646761269773, 1.841398691583461, 1.8329253902329778, 1.2566564951547168, 1.6972442943938906, 1.734018899823199, 1.7836332131760515, 0.985714850741864, 2.098369763892207, 1.9320754018422004, 1.8782117995315595, 1.307407662313678, 1.2786151500071823, 1.2037468813339653, 1.7649545465549212, 1.3718124899733175, 1.902692921338833, 1.498282390749725, 0.5571990645468765, 1.7232709331942684, 1.68511789762043, 1.3079610898902536, 1.432101530335346, 2.336814596325156, 1.65491977133395, 1.6745350326582158, 1.451490687212408, 1.405749553087569, 2.161986741459578, 1.9664424279208172, 1.2252144251511234, 1.1152825729641274, 2.0366173757151995, 1.4449044216464364, 1.9027467789148818, 1.1693799332674626, 1.2422343580263475, 2.106106553520988, 1.0933120926022648, 1.3781996727197097, 1.440145342467146, 1.7738356584882613, 1.4198945432363297, 2.9602488094856167, 1.6132376082999296, 1.602102654261877, 1.6159830834251352, 1.520130011123483, 1.40880588286172]}\n",
    "def plot_learning_curve(loss_record, title=''):\n",
    "    # 打开文件进行写入（'wb' 表示写入二进制模式）\n",
    "    # with open('new_data.json', 'w', encoding='utf-8') as file:\n",
    "    #     file.write(json.dumps(loss_record))\n",
    "\n",
    "    # return\n",
    "\n",
    "    ''' Plot learning curve of your DNN (train & dev loss) '''\n",
    "    total_steps = len(loss_record['train'])\n",
    "    x_1 = range(total_steps)\n",
    "    x_2 = x_1[::math.ceil(len(loss_record['train']) / len(loss_record['dev']))]\n",
    "    # plt.figure(figsize=(6, 4))\n",
    "    plt.plot(x_1, loss_record['train'], c='tab:red', label='train')\n",
    "    plt.plot(x_2, loss_record['dev'][:len(x_2)], c='tab:cyan', label='dev')\n",
    "    plt.ylim(0.0, 5.)\n",
    "    plt.xlabel('Training steps')\n",
    "    plt.ylabel('MSE loss')\n",
    "    plt.title('Learning curve of {}'.format(title))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_pred(preds=None, targets=None, lim=35.):\n",
    "    ''' Plot prediction of your DNN '''\n",
    "    if preds is None or targets is None:\n",
    "        return \n",
    "    \n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.scatter(targets, preds, c='r', alpha=0.5)\n",
    "    plt.plot([-0.2, lim], [-0.2, lim], c='b')\n",
    "    plt.xlim(-0.2, lim)\n",
    "    plt.ylim(-0.2, lim)\n",
    "    plt.xlabel('ground truth value')\n",
    "    plt.ylabel('predicted value')\n",
    "    plt.title('Ground Truth v.s. Prediction')\n",
    "    plt.show()\n",
    "\n",
    "class CovidDataset(Dataset):\n",
    "    def __init__(self, data_path, mode=\"train\", transform=None, flag=False):\n",
    "        self.data = pd.read_csv(data_path)\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "        self.data.drop(\"id\", axis=1, inplace=True)\n",
    "        self.data.iloc[:,40:93] = (self.data.iloc[:,40:93] - self.data.iloc[:, 40:93].mean())/self.data.iloc[:,40:93].std()\n",
    "\n",
    "        if flag:\n",
    "            self.data.drop(self.data.iloc[:, 76:93], axis=1, inplace=True)\n",
    "            self.data.drop(self.data.iloc[:, 58:75], axis=1, inplace=True)\n",
    "            self.data.drop(self.data.iloc[:, 40:57], axis=1, inplace=True)\n",
    "            \n",
    "        # 需要区分 train data 和 test data\n",
    "        if self.mode == \"train\":\n",
    "            indices = [i for i in range(len(self.data)) if i % 10 != 0]\n",
    "            self.data = self.data.iloc[indices, :]\n",
    "        elif self.mode == \"dev\":\n",
    "            indices = [i for i in range(len(self.data)) if i % 10 == 0]\n",
    "            self.data = self.data.iloc[indices, :]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        \n",
    "        if self.mode != \"test\":\n",
    "            end_index = len(self.data.columns) - 1\n",
    "            x = self.data.iloc[idx, :end_index].values\n",
    "            y = self.data.iloc[idx, -1] \n",
    "            return x, y\n",
    "        else:\n",
    "            x = self.data.iloc[idx, :].values\n",
    "            return x\n",
    "\n",
    "\n",
    "train_data = CovidDataset(\"./data/covid.train.csv\", \"train\", flag=False)\n",
    "dev_data = CovidDataset(\"./data/covid.train.csv\", \"dev\", flag=False)\n",
    "test_data = CovidDataset(\"./data/covid.test.csv\", \"test\", flag=False)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=260, shuffle=True)\n",
    "dev_dataloader = DataLoader(dev_data, batch_size=260, shuffle=False)\n",
    "test_dataloader = DataLoader(test_data, batch_size=260, shuffle=False)\n",
    "\n",
    "# train_features, train_labels = next(iter(train_dataloader))\n",
    "# print(f\"Feature batch shape: {train_features.size()}\")\n",
    "# print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "# print(train_features[0], train_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=93, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=32, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Epoch 1\n",
      "-------------------------------\n",
      "loss: 338.126086  [  260/ 2430]\n",
      "Test Epoch 1\n",
      "-------------------------------\n",
      "loss tensor(208.4327, dtype=torch.float64)\n",
      "loss tensor(208.5592, dtype=torch.float64)\n",
      "dev loss: 416.9919146531825\n",
      "\n",
      "Train Epoch 2\n",
      "-------------------------------\n",
      "loss: 206.361623  [  260/ 2430]\n",
      "Test Epoch 2\n",
      "-------------------------------\n",
      "loss tensor(70.5690, dtype=torch.float64)\n",
      "loss tensor(122.0262, dtype=torch.float64)\n",
      "dev loss: 192.59522131177937\n",
      "\n",
      "Train Epoch 3\n",
      "-------------------------------\n",
      "loss: 74.405374  [  260/ 2430]\n",
      "Test Epoch 3\n",
      "-------------------------------\n",
      "loss tensor(27.3768, dtype=torch.float64)\n",
      "loss tensor(16.8883, dtype=torch.float64)\n",
      "dev loss: 44.26508233925321\n",
      "\n",
      "Train Epoch 4\n",
      "-------------------------------\n",
      "loss: 29.819764  [  260/ 2430]\n",
      "Test Epoch 4\n",
      "-------------------------------\n",
      "loss tensor(12.6350, dtype=torch.float64)\n",
      "loss tensor(13.9033, dtype=torch.float64)\n",
      "dev loss: 26.538331695072273\n",
      "\n",
      "Train Epoch 5\n",
      "-------------------------------\n",
      "loss: 13.499082  [  260/ 2430]\n",
      "Test Epoch 5\n",
      "-------------------------------\n",
      "loss tensor(7.5779, dtype=torch.float64)\n",
      "loss tensor(3.8594, dtype=torch.float64)\n",
      "dev loss: 11.43729948616894\n",
      "\n",
      "Train Epoch 6\n",
      "-------------------------------\n",
      "loss: 8.067009  [  260/ 2430]\n",
      "Test Epoch 6\n",
      "-------------------------------\n",
      "loss tensor(4.4990, dtype=torch.float64)\n",
      "loss tensor(4.0112, dtype=torch.float64)\n",
      "dev loss: 8.510292869357947\n",
      "\n",
      "Train Epoch 7\n",
      "-------------------------------\n",
      "loss: 4.601962  [  260/ 2430]\n",
      "Test Epoch 7\n",
      "-------------------------------\n",
      "loss tensor(3.2884, dtype=torch.float64)\n",
      "loss tensor(2.2607, dtype=torch.float64)\n",
      "dev loss: 5.549119596201516\n",
      "\n",
      "Train Epoch 8\n",
      "-------------------------------\n",
      "loss: 3.299499  [  260/ 2430]\n",
      "Test Epoch 8\n",
      "-------------------------------\n",
      "loss tensor(2.5306, dtype=torch.float64)\n",
      "loss tensor(2.6153, dtype=torch.float64)\n",
      "dev loss: 5.145865822076946\n",
      "\n",
      "Train Epoch 9\n",
      "-------------------------------\n",
      "loss: 2.609477  [  260/ 2430]\n",
      "Test Epoch 9\n",
      "-------------------------------\n",
      "loss tensor(2.1400, dtype=torch.float64)\n",
      "loss tensor(2.3392, dtype=torch.float64)\n",
      "dev loss: 4.479212251804398\n",
      "\n",
      "Train Epoch 10\n",
      "-------------------------------\n",
      "loss: 2.179580  [  260/ 2430]\n",
      "Test Epoch 10\n",
      "-------------------------------\n",
      "loss tensor(1.8717, dtype=torch.float64)\n",
      "loss tensor(2.3083, dtype=torch.float64)\n",
      "dev loss: 4.180030306698661\n",
      "\n",
      "Train Epoch 11\n",
      "-------------------------------\n",
      "loss: 1.870074  [  260/ 2430]\n",
      "Test Epoch 11\n",
      "-------------------------------\n",
      "loss tensor(1.7351, dtype=torch.float64)\n",
      "loss tensor(2.1308, dtype=torch.float64)\n",
      "dev loss: 3.8659075936261416\n",
      "\n",
      "Train Epoch 12\n",
      "-------------------------------\n",
      "loss: 1.623612  [  260/ 2430]\n",
      "Test Epoch 12\n",
      "-------------------------------\n",
      "loss tensor(1.6077, dtype=torch.float64)\n",
      "loss tensor(1.9466, dtype=torch.float64)\n",
      "dev loss: 3.554319751188847\n",
      "\n",
      "Train Epoch 13\n",
      "-------------------------------\n",
      "loss: 1.585794  [  260/ 2430]\n",
      "Test Epoch 13\n",
      "-------------------------------\n",
      "loss tensor(1.5547, dtype=torch.float64)\n",
      "loss tensor(1.6456, dtype=torch.float64)\n",
      "dev loss: 3.2002924276772506\n",
      "\n",
      "Train Epoch 14\n",
      "-------------------------------\n",
      "loss: 1.389092  [  260/ 2430]\n",
      "Test Epoch 14\n",
      "-------------------------------\n",
      "loss tensor(1.4747, dtype=torch.float64)\n",
      "loss tensor(1.6608, dtype=torch.float64)\n",
      "dev loss: 3.1355199537125475\n",
      "\n",
      "Train Epoch 15\n",
      "-------------------------------\n",
      "loss: 1.187626  [  260/ 2430]\n",
      "Test Epoch 15\n",
      "-------------------------------\n",
      "loss tensor(1.4366, dtype=torch.float64)\n",
      "loss tensor(1.6211, dtype=torch.float64)\n",
      "dev loss: 3.0576466113028244\n",
      "\n",
      "Train Epoch 16\n",
      "-------------------------------\n",
      "loss: 1.255455  [  260/ 2430]\n",
      "Test Epoch 16\n",
      "-------------------------------\n",
      "loss tensor(1.3706, dtype=torch.float64)\n",
      "loss tensor(1.5149, dtype=torch.float64)\n",
      "dev loss: 2.8854834377001684\n",
      "\n",
      "Train Epoch 17\n",
      "-------------------------------\n",
      "loss: 1.150670  [  260/ 2430]\n",
      "Test Epoch 17\n",
      "-------------------------------\n",
      "loss tensor(1.3231, dtype=torch.float64)\n",
      "loss tensor(1.4255, dtype=torch.float64)\n",
      "dev loss: 2.748666075774115\n",
      "\n",
      "Train Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.997136  [  260/ 2430]\n",
      "Test Epoch 18\n",
      "-------------------------------\n",
      "loss tensor(1.3499, dtype=torch.float64)\n",
      "loss tensor(1.2373, dtype=torch.float64)\n",
      "dev loss: 2.5872660745375855\n",
      "\n",
      "Train Epoch 19\n",
      "-------------------------------\n",
      "loss: 1.053687  [  260/ 2430]\n",
      "Test Epoch 19\n",
      "-------------------------------\n",
      "loss tensor(1.3375, dtype=torch.float64)\n",
      "loss tensor(1.2697, dtype=torch.float64)\n",
      "dev loss: 2.607155124831216\n",
      "\n",
      "Train Epoch 20\n",
      "-------------------------------\n",
      "loss: 1.014087  [  260/ 2430]\n",
      "Test Epoch 20\n",
      "-------------------------------\n",
      "loss tensor(1.3688, dtype=torch.float64)\n",
      "loss tensor(1.2943, dtype=torch.float64)\n",
      "dev loss: 2.6631672915253395\n",
      "\n",
      "Train Epoch 21\n",
      "-------------------------------\n",
      "loss: 1.331749  [  260/ 2430]\n",
      "Test Epoch 21\n",
      "-------------------------------\n",
      "loss tensor(1.2550, dtype=torch.float64)\n",
      "loss tensor(1.1794, dtype=torch.float64)\n",
      "dev loss: 2.434414020433028\n",
      "\n",
      "Train Epoch 22\n",
      "-------------------------------\n",
      "loss: 1.418462  [  260/ 2430]\n",
      "Test Epoch 22\n",
      "-------------------------------\n",
      "loss tensor(1.2215, dtype=torch.float64)\n",
      "loss tensor(1.1300, dtype=torch.float64)\n",
      "dev loss: 2.3515346914693027\n",
      "\n",
      "Train Epoch 23\n",
      "-------------------------------\n",
      "loss: 1.266854  [  260/ 2430]\n",
      "Test Epoch 23\n",
      "-------------------------------\n",
      "loss tensor(1.1693, dtype=torch.float64)\n",
      "loss tensor(1.1342, dtype=torch.float64)\n",
      "dev loss: 2.3035096493550054\n",
      "\n",
      "Train Epoch 24\n",
      "-------------------------------\n",
      "loss: 1.052407  [  260/ 2430]\n",
      "Test Epoch 24\n",
      "-------------------------------\n",
      "loss tensor(1.1604, dtype=torch.float64)\n",
      "loss tensor(1.0841, dtype=torch.float64)\n",
      "dev loss: 2.2444509849880543\n",
      "\n",
      "Train Epoch 25\n",
      "-------------------------------\n",
      "loss: 1.192436  [  260/ 2430]\n",
      "Test Epoch 25\n",
      "-------------------------------\n",
      "loss tensor(1.1366, dtype=torch.float64)\n",
      "loss tensor(1.1031, dtype=torch.float64)\n",
      "dev loss: 2.2396547217709797\n",
      "\n",
      "Train Epoch 26\n",
      "-------------------------------\n",
      "loss: 1.312683  [  260/ 2430]\n",
      "Test Epoch 26\n",
      "-------------------------------\n",
      "loss tensor(1.1479, dtype=torch.float64)\n",
      "loss tensor(1.1111, dtype=torch.float64)\n",
      "dev loss: 2.258996651053295\n",
      "\n",
      "Train Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.969355  [  260/ 2430]\n",
      "Test Epoch 27\n",
      "-------------------------------\n",
      "loss tensor(1.1212, dtype=torch.float64)\n",
      "loss tensor(1.0090, dtype=torch.float64)\n",
      "dev loss: 2.1301944181472336\n",
      "\n",
      "Train Epoch 28\n",
      "-------------------------------\n",
      "loss: 1.138181  [  260/ 2430]\n",
      "Test Epoch 28\n",
      "-------------------------------\n",
      "loss tensor(1.0891, dtype=torch.float64)\n",
      "loss tensor(1.0236, dtype=torch.float64)\n",
      "dev loss: 2.1126377724360976\n",
      "\n",
      "Train Epoch 29\n",
      "-------------------------------\n",
      "loss: 1.231076  [  260/ 2430]\n",
      "Test Epoch 29\n",
      "-------------------------------\n",
      "loss tensor(1.0922, dtype=torch.float64)\n",
      "loss tensor(1.0160, dtype=torch.float64)\n",
      "dev loss: 2.1081789931995667\n",
      "\n",
      "Train Epoch 30\n",
      "-------------------------------\n",
      "loss: 1.105243  [  260/ 2430]\n",
      "Test Epoch 30\n",
      "-------------------------------\n",
      "loss tensor(1.0906, dtype=torch.float64)\n",
      "loss tensor(1.0532, dtype=torch.float64)\n",
      "dev loss: 2.1437882546523674\n",
      "\n",
      "Train Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.862040  [  260/ 2430]\n",
      "Test Epoch 31\n",
      "-------------------------------\n",
      "loss tensor(1.1739, dtype=torch.float64)\n",
      "loss tensor(1.1256, dtype=torch.float64)\n",
      "dev loss: 2.2995715579045792\n",
      "\n",
      "Train Epoch 32\n",
      "-------------------------------\n",
      "loss: 1.056720  [  260/ 2430]\n",
      "Test Epoch 32\n",
      "-------------------------------\n",
      "loss tensor(1.1527, dtype=torch.float64)\n",
      "loss tensor(1.1603, dtype=torch.float64)\n",
      "dev loss: 2.313009688909683\n",
      "\n",
      "Train Epoch 33\n",
      "-------------------------------\n",
      "loss: 1.081453  [  260/ 2430]\n",
      "Test Epoch 33\n",
      "-------------------------------\n",
      "loss tensor(1.1427, dtype=torch.float64)\n",
      "loss tensor(1.0890, dtype=torch.float64)\n",
      "dev loss: 2.23167479828784\n",
      "\n",
      "Train Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.901809  [  260/ 2430]\n",
      "Test Epoch 34\n",
      "-------------------------------\n",
      "loss tensor(1.0843, dtype=torch.float64)\n",
      "loss tensor(1.0516, dtype=torch.float64)\n",
      "dev loss: 2.135865609139482\n",
      "\n",
      "Train Epoch 35\n",
      "-------------------------------\n",
      "loss: 1.039411  [  260/ 2430]\n",
      "Test Epoch 35\n",
      "-------------------------------\n",
      "loss tensor(1.1018, dtype=torch.float64)\n",
      "loss tensor(1.0346, dtype=torch.float64)\n",
      "dev loss: 2.1364188352908338\n",
      "\n",
      "Train Epoch 36\n",
      "-------------------------------\n",
      "loss: 1.101821  [  260/ 2430]\n",
      "Test Epoch 36\n",
      "-------------------------------\n",
      "loss tensor(1.0568, dtype=torch.float64)\n",
      "loss tensor(0.9337, dtype=torch.float64)\n",
      "dev loss: 1.99046747496353\n",
      "\n",
      "Train Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.990769  [  260/ 2430]\n",
      "Test Epoch 37\n",
      "-------------------------------\n",
      "loss tensor(1.0715, dtype=torch.float64)\n",
      "loss tensor(0.9501, dtype=torch.float64)\n",
      "dev loss: 2.021561554080991\n",
      "\n",
      "Train Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.950438  [  260/ 2430]\n",
      "Test Epoch 38\n",
      "-------------------------------\n",
      "loss tensor(1.0871, dtype=torch.float64)\n",
      "loss tensor(1.0481, dtype=torch.float64)\n",
      "dev loss: 2.1351677500829247\n",
      "\n",
      "Train Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.914632  [  260/ 2430]\n",
      "Test Epoch 39\n",
      "-------------------------------\n",
      "loss tensor(1.1066, dtype=torch.float64)\n",
      "loss tensor(0.8798, dtype=torch.float64)\n",
      "dev loss: 1.9863662004586207\n",
      "\n",
      "Train Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.989102  [  260/ 2430]\n",
      "Test Epoch 40\n",
      "-------------------------------\n",
      "loss tensor(1.0179, dtype=torch.float64)\n",
      "loss tensor(0.9357, dtype=torch.float64)\n",
      "dev loss: 1.9536362445432043\n",
      "\n",
      "Train Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.942908  [  260/ 2430]\n",
      "Test Epoch 41\n",
      "-------------------------------\n",
      "loss tensor(1.0043, dtype=torch.float64)\n",
      "loss tensor(0.9703, dtype=torch.float64)\n",
      "dev loss: 1.9745778650892385\n",
      "\n",
      "Train Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.938371  [  260/ 2430]\n",
      "Test Epoch 42\n",
      "-------------------------------\n",
      "loss tensor(1.0385, dtype=torch.float64)\n",
      "loss tensor(0.8762, dtype=torch.float64)\n",
      "dev loss: 1.91473347757552\n",
      "\n",
      "Train Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.665515  [  260/ 2430]\n",
      "Test Epoch 43\n",
      "-------------------------------\n",
      "loss tensor(0.9912, dtype=torch.float64)\n",
      "loss tensor(0.8601, dtype=torch.float64)\n",
      "dev loss: 1.8512195612548505\n",
      "\n",
      "Train Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.837254  [  260/ 2430]\n",
      "Test Epoch 44\n",
      "-------------------------------\n",
      "loss tensor(0.9933, dtype=torch.float64)\n",
      "loss tensor(0.8999, dtype=torch.float64)\n",
      "dev loss: 1.8932223336579832\n",
      "\n",
      "Train Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.860866  [  260/ 2430]\n",
      "Test Epoch 45\n",
      "-------------------------------\n",
      "loss tensor(1.0293, dtype=torch.float64)\n",
      "loss tensor(0.8457, dtype=torch.float64)\n",
      "dev loss: 1.8750083628764034\n",
      "\n",
      "Train Epoch 46\n",
      "-------------------------------\n",
      "loss: 1.095681  [  260/ 2430]\n",
      "Test Epoch 46\n",
      "-------------------------------\n",
      "loss tensor(1.0222, dtype=torch.float64)\n",
      "loss tensor(0.8121, dtype=torch.float64)\n",
      "dev loss: 1.8342468781506232\n",
      "\n",
      "Train Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.829519  [  260/ 2430]\n",
      "Test Epoch 47\n",
      "-------------------------------\n",
      "loss tensor(0.9912, dtype=torch.float64)\n",
      "loss tensor(0.8844, dtype=torch.float64)\n",
      "dev loss: 1.8755789795298967\n",
      "\n",
      "Train Epoch 48\n",
      "-------------------------------\n",
      "loss: 1.118209  [  260/ 2430]\n",
      "Test Epoch 48\n",
      "-------------------------------\n",
      "loss tensor(0.9792, dtype=torch.float64)\n",
      "loss tensor(0.7880, dtype=torch.float64)\n",
      "dev loss: 1.767161123926982\n",
      "\n",
      "Train Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.868858  [  260/ 2430]\n",
      "Test Epoch 49\n",
      "-------------------------------\n",
      "loss tensor(0.9754, dtype=torch.float64)\n",
      "loss tensor(0.8483, dtype=torch.float64)\n",
      "dev loss: 1.8237028013228131\n",
      "\n",
      "Train Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.728256  [  260/ 2430]\n",
      "Test Epoch 50\n",
      "-------------------------------\n",
      "loss tensor(0.9606, dtype=torch.float64)\n",
      "loss tensor(0.8760, dtype=torch.float64)\n",
      "dev loss: 1.8365236057810908\n",
      "\n",
      "Train Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.831888  [  260/ 2430]\n",
      "Test Epoch 51\n",
      "-------------------------------\n",
      "loss tensor(0.9560, dtype=torch.float64)\n",
      "loss tensor(0.8580, dtype=torch.float64)\n",
      "dev loss: 1.8139695247121774\n",
      "\n",
      "Train Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.823928  [  260/ 2430]\n",
      "Test Epoch 52\n",
      "-------------------------------\n",
      "loss tensor(1.0159, dtype=torch.float64)\n",
      "loss tensor(0.9066, dtype=torch.float64)\n",
      "dev loss: 1.9224921048815342\n",
      "\n",
      "Train Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.914916  [  260/ 2430]\n",
      "Test Epoch 53\n",
      "-------------------------------\n",
      "loss tensor(0.9758, dtype=torch.float64)\n",
      "loss tensor(0.7678, dtype=torch.float64)\n",
      "dev loss: 1.7436843539371618\n",
      "\n",
      "Train Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.687653  [  260/ 2430]\n",
      "Test Epoch 54\n",
      "-------------------------------\n",
      "loss tensor(0.9378, dtype=torch.float64)\n",
      "loss tensor(0.8518, dtype=torch.float64)\n",
      "dev loss: 1.789643609250442\n",
      "\n",
      "Train Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.836320  [  260/ 2430]\n",
      "Test Epoch 55\n",
      "-------------------------------\n",
      "loss tensor(0.9556, dtype=torch.float64)\n",
      "loss tensor(0.8111, dtype=torch.float64)\n",
      "dev loss: 1.7666456104746158\n",
      "\n",
      "Train Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.882136  [  260/ 2430]\n",
      "Test Epoch 56\n",
      "-------------------------------\n",
      "loss tensor(0.9798, dtype=torch.float64)\n",
      "loss tensor(0.7786, dtype=torch.float64)\n",
      "dev loss: 1.758467773209369\n",
      "\n",
      "Train Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.784173  [  260/ 2430]\n",
      "Test Epoch 57\n",
      "-------------------------------\n",
      "loss tensor(0.9512, dtype=torch.float64)\n",
      "loss tensor(0.8306, dtype=torch.float64)\n",
      "dev loss: 1.78176273663884\n",
      "\n",
      "Train Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.761070  [  260/ 2430]\n",
      "Test Epoch 58\n",
      "-------------------------------\n",
      "loss tensor(0.9340, dtype=torch.float64)\n",
      "loss tensor(0.8132, dtype=torch.float64)\n",
      "dev loss: 1.7471751031949143\n",
      "\n",
      "Train Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.874855  [  260/ 2430]\n",
      "Test Epoch 59\n",
      "-------------------------------\n",
      "loss tensor(0.9438, dtype=torch.float64)\n",
      "loss tensor(0.7820, dtype=torch.float64)\n",
      "dev loss: 1.7258783711741401\n",
      "\n",
      "Train Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.606359  [  260/ 2430]\n",
      "Test Epoch 60\n",
      "-------------------------------\n",
      "loss tensor(0.9805, dtype=torch.float64)\n",
      "loss tensor(0.7952, dtype=torch.float64)\n",
      "dev loss: 1.7756404343507053\n",
      "\n",
      "Train Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.864488  [  260/ 2430]\n",
      "Test Epoch 61\n",
      "-------------------------------\n",
      "loss tensor(0.9633, dtype=torch.float64)\n",
      "loss tensor(0.7732, dtype=torch.float64)\n",
      "dev loss: 1.7364960012401276\n",
      "\n",
      "Train Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.810724  [  260/ 2430]\n",
      "Test Epoch 62\n",
      "-------------------------------\n",
      "loss tensor(0.9435, dtype=torch.float64)\n",
      "loss tensor(0.7762, dtype=torch.float64)\n",
      "dev loss: 1.719655485248817\n",
      "\n",
      "Train Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.926433  [  260/ 2430]\n",
      "Test Epoch 63\n",
      "-------------------------------\n",
      "loss tensor(0.9316, dtype=torch.float64)\n",
      "loss tensor(0.8127, dtype=torch.float64)\n",
      "dev loss: 1.7442409934134422\n",
      "\n",
      "Train Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.766994  [  260/ 2430]\n",
      "Test Epoch 64\n",
      "-------------------------------\n",
      "loss tensor(0.9270, dtype=torch.float64)\n",
      "loss tensor(0.8780, dtype=torch.float64)\n",
      "dev loss: 1.8050145124362664\n",
      "\n",
      "Train Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.854998  [  260/ 2430]\n",
      "Test Epoch 65\n",
      "-------------------------------\n",
      "loss tensor(0.9037, dtype=torch.float64)\n",
      "loss tensor(0.7587, dtype=torch.float64)\n",
      "dev loss: 1.662313314506188\n",
      "\n",
      "Train Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.749825  [  260/ 2430]\n",
      "Test Epoch 66\n",
      "-------------------------------\n",
      "loss tensor(0.9700, dtype=torch.float64)\n",
      "loss tensor(0.7621, dtype=torch.float64)\n",
      "dev loss: 1.7320947581996822\n",
      "\n",
      "Train Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.748349  [  260/ 2430]\n",
      "Test Epoch 67\n",
      "-------------------------------\n",
      "loss tensor(0.9887, dtype=torch.float64)\n",
      "loss tensor(0.8670, dtype=torch.float64)\n",
      "dev loss: 1.8557124776042813\n",
      "\n",
      "Train Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.865082  [  260/ 2430]\n",
      "Test Epoch 68\n",
      "-------------------------------\n",
      "loss tensor(0.9372, dtype=torch.float64)\n",
      "loss tensor(0.9550, dtype=torch.float64)\n",
      "dev loss: 1.8921412587015654\n",
      "\n",
      "Train Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.836751  [  260/ 2430]\n",
      "Test Epoch 69\n",
      "-------------------------------\n",
      "loss tensor(0.9393, dtype=torch.float64)\n",
      "loss tensor(0.7573, dtype=torch.float64)\n",
      "dev loss: 1.696541404163352\n",
      "\n",
      "Train Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.795560  [  260/ 2430]\n",
      "Test Epoch 70\n",
      "-------------------------------\n",
      "loss tensor(0.9066, dtype=torch.float64)\n",
      "loss tensor(0.7484, dtype=torch.float64)\n",
      "dev loss: 1.655024835703422\n",
      "\n",
      "Train Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.746963  [  260/ 2430]\n",
      "Test Epoch 71\n",
      "-------------------------------\n",
      "loss tensor(0.9180, dtype=torch.float64)\n",
      "loss tensor(0.8011, dtype=torch.float64)\n",
      "dev loss: 1.7191511391810352\n",
      "\n",
      "Train Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.755731  [  260/ 2430]\n",
      "Test Epoch 72\n",
      "-------------------------------\n",
      "loss tensor(0.9025, dtype=torch.float64)\n",
      "loss tensor(0.7252, dtype=torch.float64)\n",
      "dev loss: 1.6277267088859662\n",
      "\n",
      "Train Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.798532  [  260/ 2430]\n",
      "Test Epoch 73\n",
      "-------------------------------\n",
      "loss tensor(0.9167, dtype=torch.float64)\n",
      "loss tensor(0.7838, dtype=torch.float64)\n",
      "dev loss: 1.7004582992253634\n",
      "\n",
      "Train Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.934739  [  260/ 2430]\n",
      "Test Epoch 74\n",
      "-------------------------------\n",
      "loss tensor(1.0120, dtype=torch.float64)\n",
      "loss tensor(0.9771, dtype=torch.float64)\n",
      "dev loss: 1.989103418585396\n",
      "\n",
      "Train Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.965147  [  260/ 2430]\n",
      "Test Epoch 75\n",
      "-------------------------------\n",
      "loss tensor(0.9256, dtype=torch.float64)\n",
      "loss tensor(0.7743, dtype=torch.float64)\n",
      "dev loss: 1.699905115586839\n",
      "\n",
      "Train Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.832857  [  260/ 2430]\n",
      "Test Epoch 76\n",
      "-------------------------------\n",
      "loss tensor(0.9185, dtype=torch.float64)\n",
      "loss tensor(0.7512, dtype=torch.float64)\n",
      "dev loss: 1.6696882963926405\n",
      "\n",
      "Train Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.754560  [  260/ 2430]\n",
      "Test Epoch 77\n",
      "-------------------------------\n",
      "loss tensor(0.8958, dtype=torch.float64)\n",
      "loss tensor(0.7348, dtype=torch.float64)\n",
      "dev loss: 1.6306225095971418\n",
      "\n",
      "Train Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.997554  [  260/ 2430]\n",
      "Test Epoch 78\n",
      "-------------------------------\n",
      "loss tensor(0.9549, dtype=torch.float64)\n",
      "loss tensor(0.7653, dtype=torch.float64)\n",
      "dev loss: 1.7201893139349216\n",
      "\n",
      "Train Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.960825  [  260/ 2430]\n",
      "Test Epoch 79\n",
      "-------------------------------\n",
      "loss tensor(0.8997, dtype=torch.float64)\n",
      "loss tensor(0.7672, dtype=torch.float64)\n",
      "dev loss: 1.6669140633752388\n",
      "\n",
      "Train Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.742768  [  260/ 2430]\n",
      "Test Epoch 80\n",
      "-------------------------------\n",
      "loss tensor(0.8674, dtype=torch.float64)\n",
      "loss tensor(0.7702, dtype=torch.float64)\n",
      "dev loss: 1.6375264931268658\n",
      "\n",
      "Train Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.947740  [  260/ 2430]\n",
      "Test Epoch 81\n",
      "-------------------------------\n",
      "loss tensor(0.8932, dtype=torch.float64)\n",
      "loss tensor(0.7693, dtype=torch.float64)\n",
      "dev loss: 1.6624907428128144\n",
      "\n",
      "Train Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.716561  [  260/ 2430]\n",
      "Test Epoch 82\n",
      "-------------------------------\n",
      "loss tensor(0.8968, dtype=torch.float64)\n",
      "loss tensor(0.8014, dtype=torch.float64)\n",
      "dev loss: 1.6981943309984908\n",
      "\n",
      "Train Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.877481  [  260/ 2430]\n",
      "Test Epoch 83\n",
      "-------------------------------\n",
      "loss tensor(0.8780, dtype=torch.float64)\n",
      "loss tensor(0.6956, dtype=torch.float64)\n",
      "dev loss: 1.5736114346384942\n",
      "\n",
      "Train Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.616010  [  260/ 2430]\n",
      "Test Epoch 84\n",
      "-------------------------------\n",
      "loss tensor(0.8672, dtype=torch.float64)\n",
      "loss tensor(0.8084, dtype=torch.float64)\n",
      "dev loss: 1.6756341375921506\n",
      "\n",
      "Train Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.810385  [  260/ 2430]\n",
      "Test Epoch 85\n",
      "-------------------------------\n",
      "loss tensor(0.9426, dtype=torch.float64)\n",
      "loss tensor(0.8044, dtype=torch.float64)\n",
      "dev loss: 1.7469973008161324\n",
      "\n",
      "Train Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.993130  [  260/ 2430]\n",
      "Test Epoch 86\n",
      "-------------------------------\n",
      "loss tensor(0.9506, dtype=torch.float64)\n",
      "loss tensor(0.9526, dtype=torch.float64)\n",
      "dev loss: 1.90318188404629\n",
      "\n",
      "Train Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.940393  [  260/ 2430]\n",
      "Test Epoch 87\n",
      "-------------------------------\n",
      "loss tensor(0.8895, dtype=torch.float64)\n",
      "loss tensor(0.7871, dtype=torch.float64)\n",
      "dev loss: 1.6765811725702395\n",
      "\n",
      "Train Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.659126  [  260/ 2430]\n",
      "Test Epoch 88\n",
      "-------------------------------\n",
      "loss tensor(0.8851, dtype=torch.float64)\n",
      "loss tensor(0.7901, dtype=torch.float64)\n",
      "dev loss: 1.6752061128842892\n",
      "\n",
      "Train Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.745262  [  260/ 2430]\n",
      "Test Epoch 89\n",
      "-------------------------------\n",
      "loss tensor(0.9004, dtype=torch.float64)\n",
      "loss tensor(0.7788, dtype=torch.float64)\n",
      "dev loss: 1.6791991061471334\n",
      "\n",
      "Train Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.815627  [  260/ 2430]\n",
      "Test Epoch 90\n",
      "-------------------------------\n",
      "loss tensor(0.8899, dtype=torch.float64)\n",
      "loss tensor(0.7969, dtype=torch.float64)\n",
      "dev loss: 1.6867340269708704\n",
      "\n",
      "Train Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.879719  [  260/ 2430]\n",
      "Test Epoch 91\n",
      "-------------------------------\n",
      "loss tensor(0.9128, dtype=torch.float64)\n",
      "loss tensor(0.7496, dtype=torch.float64)\n",
      "dev loss: 1.6623324386605192\n",
      "\n",
      "Train Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.700472  [  260/ 2430]\n",
      "Test Epoch 92\n",
      "-------------------------------\n",
      "loss tensor(0.8619, dtype=torch.float64)\n",
      "loss tensor(0.8178, dtype=torch.float64)\n",
      "dev loss: 1.6797114488443388\n",
      "\n",
      "Train Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.821209  [  260/ 2430]\n",
      "Test Epoch 93\n",
      "-------------------------------\n",
      "loss tensor(0.8468, dtype=torch.float64)\n",
      "loss tensor(0.7697, dtype=torch.float64)\n",
      "dev loss: 1.6164530736599434\n",
      "\n",
      "Train Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.791911  [  260/ 2430]\n",
      "Test Epoch 94\n",
      "-------------------------------\n",
      "loss tensor(0.8946, dtype=torch.float64)\n",
      "loss tensor(0.7733, dtype=torch.float64)\n",
      "dev loss: 1.6679511414423798\n",
      "\n",
      "Train Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.736883  [  260/ 2430]\n",
      "Test Epoch 95\n",
      "-------------------------------\n",
      "loss tensor(0.8648, dtype=torch.float64)\n",
      "loss tensor(0.7417, dtype=torch.float64)\n",
      "dev loss: 1.6065021437398679\n",
      "\n",
      "Train Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.662531  [  260/ 2430]\n",
      "Test Epoch 96\n",
      "-------------------------------\n",
      "loss tensor(0.8749, dtype=torch.float64)\n",
      "loss tensor(0.7675, dtype=torch.float64)\n",
      "dev loss: 1.642366164724838\n",
      "\n",
      "Train Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.719856  [  260/ 2430]\n",
      "Test Epoch 97\n",
      "-------------------------------\n",
      "loss tensor(0.9005, dtype=torch.float64)\n",
      "loss tensor(0.7655, dtype=torch.float64)\n",
      "dev loss: 1.6659860058806997\n",
      "\n",
      "Train Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.726415  [  260/ 2430]\n",
      "Test Epoch 98\n",
      "-------------------------------\n",
      "loss tensor(0.8516, dtype=torch.float64)\n",
      "loss tensor(0.7328, dtype=torch.float64)\n",
      "dev loss: 1.584382256397328\n",
      "\n",
      "Train Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.648734  [  260/ 2430]\n",
      "Test Epoch 99\n",
      "-------------------------------\n",
      "loss tensor(0.9061, dtype=torch.float64)\n",
      "loss tensor(0.7616, dtype=torch.float64)\n",
      "dev loss: 1.6677815951047332\n",
      "\n",
      "Train Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.707593  [  260/ 2430]\n",
      "Test Epoch 100\n",
      "-------------------------------\n",
      "loss tensor(0.9423, dtype=torch.float64)\n",
      "loss tensor(0.7731, dtype=torch.float64)\n",
      "dev loss: 1.7154020304747863\n",
      "\n",
      "Train Epoch 101\n",
      "-------------------------------\n",
      "loss: 0.922921  [  260/ 2430]\n",
      "Test Epoch 101\n",
      "-------------------------------\n",
      "loss tensor(0.8828, dtype=torch.float64)\n",
      "loss tensor(0.7215, dtype=torch.float64)\n",
      "dev loss: 1.6043140904547775\n",
      "\n",
      "Train Epoch 102\n",
      "-------------------------------\n",
      "loss: 0.735369  [  260/ 2430]\n",
      "Test Epoch 102\n",
      "-------------------------------\n",
      "loss tensor(0.8394, dtype=torch.float64)\n",
      "loss tensor(0.6893, dtype=torch.float64)\n",
      "dev loss: 1.5287158851169989\n",
      "\n",
      "Train Epoch 103\n",
      "-------------------------------\n",
      "loss: 0.622969  [  260/ 2430]\n",
      "Test Epoch 103\n",
      "-------------------------------\n",
      "loss tensor(0.8566, dtype=torch.float64)\n",
      "loss tensor(0.7044, dtype=torch.float64)\n",
      "dev loss: 1.5610085601217625\n",
      "\n",
      "Train Epoch 104\n",
      "-------------------------------\n",
      "loss: 0.684593  [  260/ 2430]\n",
      "Test Epoch 104\n",
      "-------------------------------\n",
      "loss tensor(0.8740, dtype=torch.float64)\n",
      "loss tensor(0.8488, dtype=torch.float64)\n",
      "dev loss: 1.722810532479076\n",
      "\n",
      "Train Epoch 105\n",
      "-------------------------------\n",
      "loss: 0.787776  [  260/ 2430]\n",
      "Test Epoch 105\n",
      "-------------------------------\n",
      "loss tensor(0.9059, dtype=torch.float64)\n",
      "loss tensor(0.8419, dtype=torch.float64)\n",
      "dev loss: 1.7477636186568102\n",
      "\n",
      "Train Epoch 106\n",
      "-------------------------------\n",
      "loss: 0.756604  [  260/ 2430]\n",
      "Test Epoch 106\n",
      "-------------------------------\n",
      "loss tensor(0.8825, dtype=torch.float64)\n",
      "loss tensor(0.8000, dtype=torch.float64)\n",
      "dev loss: 1.6824925678789757\n",
      "\n",
      "Train Epoch 107\n",
      "-------------------------------\n",
      "loss: 0.725281  [  260/ 2430]\n",
      "Test Epoch 107\n",
      "-------------------------------\n",
      "loss tensor(0.8975, dtype=torch.float64)\n",
      "loss tensor(0.6715, dtype=torch.float64)\n",
      "dev loss: 1.5689929958290332\n",
      "\n",
      "Train Epoch 108\n",
      "-------------------------------\n",
      "loss: 0.807778  [  260/ 2430]\n",
      "Test Epoch 108\n",
      "-------------------------------\n",
      "loss tensor(0.8949, dtype=torch.float64)\n",
      "loss tensor(0.7521, dtype=torch.float64)\n",
      "dev loss: 1.646961727473364\n",
      "\n",
      "Train Epoch 109\n",
      "-------------------------------\n",
      "loss: 0.618784  [  260/ 2430]\n",
      "Test Epoch 109\n",
      "-------------------------------\n",
      "loss tensor(0.8635, dtype=torch.float64)\n",
      "loss tensor(0.6924, dtype=torch.float64)\n",
      "dev loss: 1.555905716806496\n",
      "\n",
      "Train Epoch 110\n",
      "-------------------------------\n",
      "loss: 0.720687  [  260/ 2430]\n",
      "Test Epoch 110\n",
      "-------------------------------\n",
      "loss tensor(0.8410, dtype=torch.float64)\n",
      "loss tensor(0.7738, dtype=torch.float64)\n",
      "dev loss: 1.6147563502975784\n",
      "\n",
      "Train Epoch 111\n",
      "-------------------------------\n",
      "loss: 0.652000  [  260/ 2430]\n",
      "Test Epoch 111\n",
      "-------------------------------\n",
      "loss tensor(0.8342, dtype=torch.float64)\n",
      "loss tensor(0.7043, dtype=torch.float64)\n",
      "dev loss: 1.5385412731094936\n",
      "\n",
      "Train Epoch 112\n",
      "-------------------------------\n",
      "loss: 0.884276  [  260/ 2430]\n",
      "Test Epoch 112\n",
      "-------------------------------\n",
      "loss tensor(0.8279, dtype=torch.float64)\n",
      "loss tensor(0.6997, dtype=torch.float64)\n",
      "dev loss: 1.5275344861613638\n",
      "\n",
      "Train Epoch 113\n",
      "-------------------------------\n",
      "loss: 0.728911  [  260/ 2430]\n",
      "Test Epoch 113\n",
      "-------------------------------\n",
      "loss tensor(0.9068, dtype=torch.float64)\n",
      "loss tensor(0.7548, dtype=torch.float64)\n",
      "dev loss: 1.6616808763921926\n",
      "\n",
      "Train Epoch 114\n",
      "-------------------------------\n",
      "loss: 0.812107  [  260/ 2430]\n",
      "Test Epoch 114\n",
      "-------------------------------\n",
      "loss tensor(0.8787, dtype=torch.float64)\n",
      "loss tensor(0.8936, dtype=torch.float64)\n",
      "dev loss: 1.772242724436505\n",
      "\n",
      "Train Epoch 115\n",
      "-------------------------------\n",
      "loss: 0.756971  [  260/ 2430]\n",
      "Test Epoch 115\n",
      "-------------------------------\n",
      "loss tensor(0.8574, dtype=torch.float64)\n",
      "loss tensor(0.7327, dtype=torch.float64)\n",
      "dev loss: 1.5901021508577404\n",
      "\n",
      "Train Epoch 116\n",
      "-------------------------------\n",
      "loss: 0.598586  [  260/ 2430]\n",
      "Test Epoch 116\n",
      "-------------------------------\n",
      "loss tensor(0.8522, dtype=torch.float64)\n",
      "loss tensor(0.7250, dtype=torch.float64)\n",
      "dev loss: 1.5771533360116656\n",
      "\n",
      "Train Epoch 117\n",
      "-------------------------------\n",
      "loss: 0.700896  [  260/ 2430]\n",
      "Test Epoch 117\n",
      "-------------------------------\n",
      "loss tensor(0.8711, dtype=torch.float64)\n",
      "loss tensor(0.8868, dtype=torch.float64)\n",
      "dev loss: 1.7579391292553128\n",
      "\n",
      "Train Epoch 118\n",
      "-------------------------------\n",
      "loss: 0.507320  [  260/ 2430]\n",
      "Test Epoch 118\n",
      "-------------------------------\n",
      "loss tensor(0.8509, dtype=torch.float64)\n",
      "loss tensor(0.7383, dtype=torch.float64)\n",
      "dev loss: 1.5891639645188116\n",
      "\n",
      "Train Epoch 119\n",
      "-------------------------------\n",
      "loss: 0.665438  [  260/ 2430]\n",
      "Test Epoch 119\n",
      "-------------------------------\n",
      "loss tensor(0.8284, dtype=torch.float64)\n",
      "loss tensor(0.7528, dtype=torch.float64)\n",
      "dev loss: 1.5812531309212203\n",
      "\n",
      "Train Epoch 120\n",
      "-------------------------------\n",
      "loss: 0.700222  [  260/ 2430]\n",
      "Test Epoch 120\n",
      "-------------------------------\n",
      "loss tensor(0.8243, dtype=torch.float64)\n",
      "loss tensor(0.7243, dtype=torch.float64)\n",
      "dev loss: 1.5485980700346955\n",
      "\n",
      "Train Epoch 121\n",
      "-------------------------------\n",
      "loss: 0.555421  [  260/ 2430]\n",
      "Test Epoch 121\n",
      "-------------------------------\n",
      "loss tensor(0.8337, dtype=torch.float64)\n",
      "loss tensor(0.7508, dtype=torch.float64)\n",
      "dev loss: 1.584442721796529\n",
      "\n",
      "Train Epoch 122\n",
      "-------------------------------\n",
      "loss: 0.621993  [  260/ 2430]\n",
      "Test Epoch 122\n",
      "-------------------------------\n",
      "loss tensor(0.8265, dtype=torch.float64)\n",
      "loss tensor(0.7074, dtype=torch.float64)\n",
      "dev loss: 1.5338113084840066\n",
      "\n",
      "Train Epoch 123\n",
      "-------------------------------\n",
      "loss: 0.773832  [  260/ 2430]\n",
      "Test Epoch 123\n",
      "-------------------------------\n",
      "loss tensor(0.8783, dtype=torch.float64)\n",
      "loss tensor(0.7201, dtype=torch.float64)\n",
      "dev loss: 1.5984962622940864\n",
      "\n",
      "Train Epoch 124\n",
      "-------------------------------\n",
      "loss: 0.853857  [  260/ 2430]\n",
      "Test Epoch 124\n",
      "-------------------------------\n",
      "loss tensor(0.8667, dtype=torch.float64)\n",
      "loss tensor(0.8354, dtype=torch.float64)\n",
      "dev loss: 1.7021087056130488\n",
      "\n",
      "Train Epoch 125\n",
      "-------------------------------\n",
      "loss: 0.712882  [  260/ 2430]\n",
      "Test Epoch 125\n",
      "-------------------------------\n",
      "loss tensor(0.8780, dtype=torch.float64)\n",
      "loss tensor(0.7670, dtype=torch.float64)\n",
      "dev loss: 1.6449945855944827\n",
      "\n",
      "Train Epoch 126\n",
      "-------------------------------\n",
      "loss: 0.548576  [  260/ 2430]\n",
      "Test Epoch 126\n",
      "-------------------------------\n",
      "loss tensor(0.8673, dtype=torch.float64)\n",
      "loss tensor(0.7202, dtype=torch.float64)\n",
      "dev loss: 1.5875761266532602\n",
      "\n",
      "Train Epoch 127\n",
      "-------------------------------\n",
      "loss: 0.811433  [  260/ 2430]\n",
      "Test Epoch 127\n",
      "-------------------------------\n",
      "loss tensor(0.8576, dtype=torch.float64)\n",
      "loss tensor(0.7404, dtype=torch.float64)\n",
      "dev loss: 1.5979671309311652\n",
      "\n",
      "Train Epoch 128\n",
      "-------------------------------\n",
      "loss: 0.754583  [  260/ 2430]\n",
      "Test Epoch 128\n",
      "-------------------------------\n",
      "loss tensor(0.8557, dtype=torch.float64)\n",
      "loss tensor(0.7762, dtype=torch.float64)\n",
      "dev loss: 1.631872134502408\n",
      "\n",
      "Train Epoch 129\n",
      "-------------------------------\n",
      "loss: 0.807320  [  260/ 2430]\n",
      "Test Epoch 129\n",
      "-------------------------------\n",
      "loss tensor(0.8469, dtype=torch.float64)\n",
      "loss tensor(0.7940, dtype=torch.float64)\n",
      "dev loss: 1.6409618432408484\n",
      "\n",
      "Train Epoch 130\n",
      "-------------------------------\n",
      "loss: 0.877209  [  260/ 2430]\n",
      "Test Epoch 130\n",
      "-------------------------------\n",
      "loss tensor(0.8615, dtype=torch.float64)\n",
      "loss tensor(0.8731, dtype=torch.float64)\n",
      "dev loss: 1.7345689336860604\n",
      "\n",
      "Train Epoch 131\n",
      "-------------------------------\n",
      "loss: 0.816859  [  260/ 2430]\n",
      "Test Epoch 131\n",
      "-------------------------------\n",
      "loss tensor(0.8296, dtype=torch.float64)\n",
      "loss tensor(0.7962, dtype=torch.float64)\n",
      "dev loss: 1.6258765760955993\n",
      "\n",
      "Train Epoch 132\n",
      "-------------------------------\n",
      "loss: 0.543736  [  260/ 2430]\n",
      "Test Epoch 132\n",
      "-------------------------------\n",
      "loss tensor(0.8095, dtype=torch.float64)\n",
      "loss tensor(0.6842, dtype=torch.float64)\n",
      "dev loss: 1.493696589831274\n",
      "\n",
      "Train Epoch 133\n",
      "-------------------------------\n",
      "loss: 0.621281  [  260/ 2430]\n",
      "Test Epoch 133\n",
      "-------------------------------\n",
      "loss tensor(0.8339, dtype=torch.float64)\n",
      "loss tensor(0.7511, dtype=torch.float64)\n",
      "dev loss: 1.584972558015596\n",
      "\n",
      "Train Epoch 134\n",
      "-------------------------------\n",
      "loss: 0.712428  [  260/ 2430]\n",
      "Test Epoch 134\n",
      "-------------------------------\n",
      "loss tensor(0.8544, dtype=torch.float64)\n",
      "loss tensor(0.7475, dtype=torch.float64)\n",
      "dev loss: 1.601884659257304\n",
      "\n",
      "Train Epoch 135\n",
      "-------------------------------\n",
      "loss: 0.634139  [  260/ 2430]\n",
      "Test Epoch 135\n",
      "-------------------------------\n",
      "loss tensor(0.8235, dtype=torch.float64)\n",
      "loss tensor(0.6739, dtype=torch.float64)\n",
      "dev loss: 1.497325404777318\n",
      "\n",
      "Train Epoch 136\n",
      "-------------------------------\n",
      "loss: 0.637607  [  260/ 2430]\n",
      "Test Epoch 136\n",
      "-------------------------------\n",
      "loss tensor(0.8080, dtype=torch.float64)\n",
      "loss tensor(0.7665, dtype=torch.float64)\n",
      "dev loss: 1.574518001476239\n",
      "\n",
      "Train Epoch 137\n",
      "-------------------------------\n",
      "loss: 0.667967  [  260/ 2430]\n",
      "Test Epoch 137\n",
      "-------------------------------\n",
      "loss tensor(0.8176, dtype=torch.float64)\n",
      "loss tensor(0.7067, dtype=torch.float64)\n",
      "dev loss: 1.5242468522307946\n",
      "\n",
      "Train Epoch 138\n",
      "-------------------------------\n",
      "loss: 0.609043  [  260/ 2430]\n",
      "Test Epoch 138\n",
      "-------------------------------\n",
      "loss tensor(0.8134, dtype=torch.float64)\n",
      "loss tensor(0.7331, dtype=torch.float64)\n",
      "dev loss: 1.5465007066061252\n",
      "\n",
      "Train Epoch 139\n",
      "-------------------------------\n",
      "loss: 0.690350  [  260/ 2430]\n",
      "Test Epoch 139\n",
      "-------------------------------\n",
      "loss tensor(0.8490, dtype=torch.float64)\n",
      "loss tensor(0.7453, dtype=torch.float64)\n",
      "dev loss: 1.5942927810384586\n",
      "\n",
      "Train Epoch 140\n",
      "-------------------------------\n",
      "loss: 0.608224  [  260/ 2430]\n",
      "Test Epoch 140\n",
      "-------------------------------\n",
      "loss tensor(0.8635, dtype=torch.float64)\n",
      "loss tensor(0.8695, dtype=torch.float64)\n",
      "dev loss: 1.7329544573848912\n",
      "\n",
      "Train Epoch 141\n",
      "-------------------------------\n",
      "loss: 0.628076  [  260/ 2430]\n",
      "Test Epoch 141\n",
      "-------------------------------\n",
      "loss tensor(0.8740, dtype=torch.float64)\n",
      "loss tensor(0.8643, dtype=torch.float64)\n",
      "dev loss: 1.7383372828487693\n",
      "\n",
      "Train Epoch 142\n",
      "-------------------------------\n",
      "loss: 0.697289  [  260/ 2430]\n",
      "Test Epoch 142\n",
      "-------------------------------\n",
      "loss tensor(0.8570, dtype=torch.float64)\n",
      "loss tensor(0.8041, dtype=torch.float64)\n",
      "dev loss: 1.661045681782932\n",
      "\n",
      "Train Epoch 143\n",
      "-------------------------------\n",
      "loss: 0.559237  [  260/ 2430]\n",
      "Test Epoch 143\n",
      "-------------------------------\n",
      "loss tensor(0.8264, dtype=torch.float64)\n",
      "loss tensor(0.7025, dtype=torch.float64)\n",
      "dev loss: 1.5289364458035348\n",
      "\n",
      "Train Epoch 144\n",
      "-------------------------------\n",
      "loss: 0.631788  [  260/ 2430]\n",
      "Test Epoch 144\n",
      "-------------------------------\n",
      "loss tensor(0.8816, dtype=torch.float64)\n",
      "loss tensor(0.7971, dtype=torch.float64)\n",
      "dev loss: 1.678653656226209\n",
      "\n",
      "Train Epoch 145\n",
      "-------------------------------\n",
      "loss: 0.712574  [  260/ 2430]\n",
      "Test Epoch 145\n",
      "-------------------------------\n",
      "loss tensor(0.8635, dtype=torch.float64)\n",
      "loss tensor(0.7376, dtype=torch.float64)\n",
      "dev loss: 1.6010192902457223\n",
      "\n",
      "Train Epoch 146\n",
      "-------------------------------\n",
      "loss: 0.713327  [  260/ 2430]\n",
      "Test Epoch 146\n",
      "-------------------------------\n",
      "loss tensor(0.8614, dtype=torch.float64)\n",
      "loss tensor(0.6730, dtype=torch.float64)\n",
      "dev loss: 1.534405544053553\n",
      "\n",
      "Train Epoch 147\n",
      "-------------------------------\n",
      "loss: 0.815709  [  260/ 2430]\n",
      "Test Epoch 147\n",
      "-------------------------------\n",
      "loss tensor(0.9466, dtype=torch.float64)\n",
      "loss tensor(0.7181, dtype=torch.float64)\n",
      "dev loss: 1.6647261796004749\n",
      "\n",
      "Train Epoch 148\n",
      "-------------------------------\n",
      "loss: 0.783036  [  260/ 2430]\n",
      "Test Epoch 148\n",
      "-------------------------------\n",
      "loss tensor(0.8102, dtype=torch.float64)\n",
      "loss tensor(0.7444, dtype=torch.float64)\n",
      "dev loss: 1.554596101470573\n",
      "\n",
      "Train Epoch 149\n",
      "-------------------------------\n",
      "loss: 0.584053  [  260/ 2430]\n",
      "Test Epoch 149\n",
      "-------------------------------\n",
      "loss tensor(0.8448, dtype=torch.float64)\n",
      "loss tensor(0.8843, dtype=torch.float64)\n",
      "dev loss: 1.729039162596405\n",
      "\n",
      "Train Epoch 150\n",
      "-------------------------------\n",
      "loss: 0.536647  [  260/ 2430]\n",
      "Test Epoch 150\n",
      "-------------------------------\n",
      "loss tensor(0.8674, dtype=torch.float64)\n",
      "loss tensor(0.6810, dtype=torch.float64)\n",
      "dev loss: 1.5484306933738936\n",
      "\n",
      "Train Epoch 151\n",
      "-------------------------------\n",
      "loss: 0.723001  [  260/ 2430]\n",
      "Test Epoch 151\n",
      "-------------------------------\n",
      "loss tensor(0.8427, dtype=torch.float64)\n",
      "loss tensor(0.8258, dtype=torch.float64)\n",
      "dev loss: 1.6684888772800939\n",
      "\n",
      "Train Epoch 152\n",
      "-------------------------------\n",
      "loss: 0.614778  [  260/ 2430]\n",
      "Test Epoch 152\n",
      "-------------------------------\n",
      "loss tensor(0.9353, dtype=torch.float64)\n",
      "loss tensor(0.9602, dtype=torch.float64)\n",
      "dev loss: 1.895428974987961\n",
      "\n",
      "Train Epoch 153\n",
      "-------------------------------\n",
      "loss: 0.690494  [  260/ 2430]\n",
      "Test Epoch 153\n",
      "-------------------------------\n",
      "loss tensor(0.9184, dtype=torch.float64)\n",
      "loss tensor(1.0621, dtype=torch.float64)\n",
      "dev loss: 1.9804942666089627\n",
      "\n",
      "Train Epoch 154\n",
      "-------------------------------\n",
      "loss: 0.760322  [  260/ 2430]\n",
      "Test Epoch 154\n",
      "-------------------------------\n",
      "loss tensor(0.8177, dtype=torch.float64)\n",
      "loss tensor(0.8232, dtype=torch.float64)\n",
      "dev loss: 1.6409411215260015\n",
      "\n",
      "Train Epoch 155\n",
      "-------------------------------\n",
      "loss: 0.510047  [  260/ 2430]\n",
      "Test Epoch 155\n",
      "-------------------------------\n",
      "loss tensor(0.8407, dtype=torch.float64)\n",
      "loss tensor(0.7703, dtype=torch.float64)\n",
      "dev loss: 1.6109658213438365\n",
      "\n",
      "Train Epoch 156\n",
      "-------------------------------\n",
      "loss: 0.764234  [  260/ 2430]\n",
      "Test Epoch 156\n",
      "-------------------------------\n",
      "loss tensor(0.8219, dtype=torch.float64)\n",
      "loss tensor(0.6518, dtype=torch.float64)\n",
      "dev loss: 1.4736360859635869\n",
      "\n",
      "Train Epoch 157\n",
      "-------------------------------\n",
      "loss: 0.663230  [  260/ 2430]\n",
      "Test Epoch 157\n",
      "-------------------------------\n",
      "loss tensor(0.8527, dtype=torch.float64)\n",
      "loss tensor(0.8681, dtype=torch.float64)\n",
      "dev loss: 1.7208603220588699\n",
      "\n",
      "Train Epoch 158\n",
      "-------------------------------\n",
      "loss: 0.623110  [  260/ 2430]\n",
      "Test Epoch 158\n",
      "-------------------------------\n",
      "loss tensor(0.8719, dtype=torch.float64)\n",
      "loss tensor(0.7499, dtype=torch.float64)\n",
      "dev loss: 1.6217194565075408\n",
      "\n",
      "Train Epoch 159\n",
      "-------------------------------\n",
      "loss: 0.598966  [  260/ 2430]\n",
      "Test Epoch 159\n",
      "-------------------------------\n",
      "loss tensor(0.8328, dtype=torch.float64)\n",
      "loss tensor(0.7566, dtype=torch.float64)\n",
      "dev loss: 1.5893428696004128\n",
      "\n",
      "Train Epoch 160\n",
      "-------------------------------\n",
      "loss: 0.699365  [  260/ 2430]\n",
      "Test Epoch 160\n",
      "-------------------------------\n",
      "loss tensor(0.8366, dtype=torch.float64)\n",
      "loss tensor(0.6713, dtype=torch.float64)\n",
      "dev loss: 1.50787327054327\n",
      "\n",
      "Train Epoch 161\n",
      "-------------------------------\n",
      "loss: 0.617603  [  260/ 2430]\n",
      "Test Epoch 161\n",
      "-------------------------------\n",
      "loss tensor(0.8618, dtype=torch.float64)\n",
      "loss tensor(0.6458, dtype=torch.float64)\n",
      "dev loss: 1.5075791239186622\n",
      "\n",
      "Train Epoch 162\n",
      "-------------------------------\n",
      "loss: 0.713495  [  260/ 2430]\n",
      "Test Epoch 162\n",
      "-------------------------------\n",
      "loss tensor(0.8712, dtype=torch.float64)\n",
      "loss tensor(0.7210, dtype=torch.float64)\n",
      "dev loss: 1.5921328808554542\n",
      "\n",
      "Train Epoch 163\n",
      "-------------------------------\n",
      "loss: 0.690578  [  260/ 2430]\n",
      "Test Epoch 163\n",
      "-------------------------------\n",
      "loss tensor(0.8620, dtype=torch.float64)\n",
      "loss tensor(0.8129, dtype=torch.float64)\n",
      "dev loss: 1.6749596374707894\n",
      "\n",
      "Train Epoch 164\n",
      "-------------------------------\n",
      "loss: 0.755987  [  260/ 2430]\n",
      "Test Epoch 164\n",
      "-------------------------------\n",
      "loss tensor(0.8256, dtype=torch.float64)\n",
      "loss tensor(0.7832, dtype=torch.float64)\n",
      "dev loss: 1.6088079003690336\n",
      "\n",
      "Train Epoch 165\n",
      "-------------------------------\n",
      "loss: 0.688009  [  260/ 2430]\n",
      "Test Epoch 165\n",
      "-------------------------------\n",
      "loss tensor(0.8384, dtype=torch.float64)\n",
      "loss tensor(0.7020, dtype=torch.float64)\n",
      "dev loss: 1.540397684191856\n",
      "\n",
      "Train Epoch 166\n",
      "-------------------------------\n",
      "loss: 0.641431  [  260/ 2430]\n",
      "Test Epoch 166\n",
      "-------------------------------\n",
      "loss tensor(0.7950, dtype=torch.float64)\n",
      "loss tensor(0.7003, dtype=torch.float64)\n",
      "dev loss: 1.4953169578322711\n",
      "\n",
      "Train Epoch 167\n",
      "-------------------------------\n",
      "loss: 0.636348  [  260/ 2430]\n",
      "Test Epoch 167\n",
      "-------------------------------\n",
      "loss tensor(0.8040, dtype=torch.float64)\n",
      "loss tensor(0.7511, dtype=torch.float64)\n",
      "dev loss: 1.55506866000144\n",
      "\n",
      "Train Epoch 168\n",
      "-------------------------------\n",
      "loss: 0.558958  [  260/ 2430]\n",
      "Test Epoch 168\n",
      "-------------------------------\n",
      "loss tensor(0.8304, dtype=torch.float64)\n",
      "loss tensor(0.7400, dtype=torch.float64)\n",
      "dev loss: 1.5704083563879006\n",
      "\n",
      "Train Epoch 169\n",
      "-------------------------------\n",
      "loss: 0.754620  [  260/ 2430]\n",
      "Test Epoch 169\n",
      "-------------------------------\n",
      "loss tensor(0.8304, dtype=torch.float64)\n",
      "loss tensor(0.6746, dtype=torch.float64)\n",
      "dev loss: 1.5050390054914282\n",
      "\n",
      "Train Epoch 170\n",
      "-------------------------------\n",
      "loss: 0.703282  [  260/ 2430]\n",
      "Test Epoch 170\n",
      "-------------------------------\n",
      "loss tensor(0.8216, dtype=torch.float64)\n",
      "loss tensor(0.6959, dtype=torch.float64)\n",
      "dev loss: 1.5174235007966757\n",
      "\n",
      "Train Epoch 171\n",
      "-------------------------------\n",
      "loss: 0.642472  [  260/ 2430]\n",
      "Test Epoch 171\n",
      "-------------------------------\n",
      "loss tensor(0.8814, dtype=torch.float64)\n",
      "loss tensor(0.7361, dtype=torch.float64)\n",
      "dev loss: 1.6174870841119038\n",
      "\n",
      "Train Epoch 172\n",
      "-------------------------------\n",
      "loss: 0.640304  [  260/ 2430]\n",
      "Test Epoch 172\n",
      "-------------------------------\n",
      "loss tensor(0.8417, dtype=torch.float64)\n",
      "loss tensor(0.6895, dtype=torch.float64)\n",
      "dev loss: 1.531175742776984\n",
      "\n",
      "Train Epoch 173\n",
      "-------------------------------\n",
      "loss: 0.637754  [  260/ 2430]\n",
      "Test Epoch 173\n",
      "-------------------------------\n",
      "loss tensor(0.8126, dtype=torch.float64)\n",
      "loss tensor(0.7816, dtype=torch.float64)\n",
      "dev loss: 1.594220373167092\n",
      "\n",
      "Train Epoch 174\n",
      "-------------------------------\n",
      "loss: 0.644844  [  260/ 2430]\n",
      "Test Epoch 174\n",
      "-------------------------------\n",
      "loss tensor(0.7971, dtype=torch.float64)\n",
      "loss tensor(0.7886, dtype=torch.float64)\n",
      "dev loss: 1.5857167258172387\n",
      "\n",
      "Train Epoch 175\n",
      "-------------------------------\n",
      "loss: 0.596248  [  260/ 2430]\n",
      "Test Epoch 175\n",
      "-------------------------------\n",
      "loss tensor(0.7916, dtype=torch.float64)\n",
      "loss tensor(0.6913, dtype=torch.float64)\n",
      "dev loss: 1.4829035055000146\n",
      "\n",
      "Train Epoch 176\n",
      "-------------------------------\n",
      "loss: 0.596023  [  260/ 2430]\n",
      "Test Epoch 176\n",
      "-------------------------------\n",
      "loss tensor(0.8221, dtype=torch.float64)\n",
      "loss tensor(0.7569, dtype=torch.float64)\n",
      "dev loss: 1.5790172966480185\n",
      "\n",
      "Train Epoch 177\n",
      "-------------------------------\n",
      "loss: 0.688292  [  260/ 2430]\n",
      "Test Epoch 177\n",
      "-------------------------------\n",
      "loss tensor(0.8350, dtype=torch.float64)\n",
      "loss tensor(0.7571, dtype=torch.float64)\n",
      "dev loss: 1.5921461336929796\n",
      "\n",
      "Train Epoch 178\n",
      "-------------------------------\n",
      "loss: 0.680402  [  260/ 2430]\n",
      "Test Epoch 178\n",
      "-------------------------------\n",
      "loss tensor(0.8541, dtype=torch.float64)\n",
      "loss tensor(0.7087, dtype=torch.float64)\n",
      "dev loss: 1.5627660248003226\n",
      "\n",
      "Train Epoch 179\n",
      "-------------------------------\n",
      "loss: 0.601137  [  260/ 2430]\n",
      "Test Epoch 179\n",
      "-------------------------------\n",
      "loss tensor(0.8641, dtype=torch.float64)\n",
      "loss tensor(0.7197, dtype=torch.float64)\n",
      "dev loss: 1.5838118957305167\n",
      "\n",
      "Train Epoch 180\n",
      "-------------------------------\n",
      "loss: 0.702568  [  260/ 2430]\n",
      "Test Epoch 180\n",
      "-------------------------------\n",
      "loss tensor(0.8294, dtype=torch.float64)\n",
      "loss tensor(0.6836, dtype=torch.float64)\n",
      "dev loss: 1.5130035534571413\n",
      "\n",
      "Train Epoch 181\n",
      "-------------------------------\n",
      "loss: 0.632300  [  260/ 2430]\n",
      "Test Epoch 181\n",
      "-------------------------------\n",
      "loss tensor(0.8254, dtype=torch.float64)\n",
      "loss tensor(0.9474, dtype=torch.float64)\n",
      "dev loss: 1.7728542819947932\n",
      "\n",
      "Train Epoch 182\n",
      "-------------------------------\n",
      "loss: 0.608064  [  260/ 2430]\n",
      "Test Epoch 182\n",
      "-------------------------------\n",
      "loss tensor(0.8331, dtype=torch.float64)\n",
      "loss tensor(0.8283, dtype=torch.float64)\n",
      "dev loss: 1.661461998166227\n",
      "\n",
      "Train Epoch 183\n",
      "-------------------------------\n",
      "loss: 0.729658  [  260/ 2430]\n",
      "Test Epoch 183\n",
      "-------------------------------\n",
      "loss tensor(0.8083, dtype=torch.float64)\n",
      "loss tensor(0.7428, dtype=torch.float64)\n",
      "dev loss: 1.5511254211724412\n",
      "\n",
      "Train Epoch 184\n",
      "-------------------------------\n",
      "loss: 0.640745  [  260/ 2430]\n",
      "Test Epoch 184\n",
      "-------------------------------\n",
      "loss tensor(0.8565, dtype=torch.float64)\n",
      "loss tensor(0.7299, dtype=torch.float64)\n",
      "dev loss: 1.586433439758029\n",
      "\n",
      "Train Epoch 185\n",
      "-------------------------------\n",
      "loss: 0.817155  [  260/ 2430]\n",
      "Test Epoch 185\n",
      "-------------------------------\n",
      "loss tensor(0.8140, dtype=torch.float64)\n",
      "loss tensor(0.6846, dtype=torch.float64)\n",
      "dev loss: 1.4986191525653054\n",
      "\n",
      "Train Epoch 186\n",
      "-------------------------------\n",
      "loss: 0.675634  [  260/ 2430]\n",
      "Test Epoch 186\n",
      "-------------------------------\n",
      "loss tensor(0.8025, dtype=torch.float64)\n",
      "loss tensor(0.8476, dtype=torch.float64)\n",
      "dev loss: 1.6500253168238554\n",
      "\n",
      "Train Epoch 187\n",
      "-------------------------------\n",
      "loss: 0.554801  [  260/ 2430]\n",
      "Test Epoch 187\n",
      "-------------------------------\n",
      "loss tensor(0.8411, dtype=torch.float64)\n",
      "loss tensor(0.7203, dtype=torch.float64)\n",
      "dev loss: 1.5614476238159583\n",
      "\n",
      "Train Epoch 188\n",
      "-------------------------------\n",
      "loss: 0.602892  [  260/ 2430]\n",
      "Test Epoch 188\n",
      "-------------------------------\n",
      "loss tensor(0.8082, dtype=torch.float64)\n",
      "loss tensor(0.7270, dtype=torch.float64)\n",
      "dev loss: 1.5352070923326013\n",
      "\n",
      "Train Epoch 189\n",
      "-------------------------------\n",
      "loss: 0.737476  [  260/ 2430]\n",
      "Test Epoch 189\n",
      "-------------------------------\n",
      "loss tensor(0.8020, dtype=torch.float64)\n",
      "loss tensor(0.7459, dtype=torch.float64)\n",
      "dev loss: 1.5479040321092765\n",
      "\n",
      "Train Epoch 190\n",
      "-------------------------------\n",
      "loss: 0.670418  [  260/ 2430]\n",
      "Test Epoch 190\n",
      "-------------------------------\n",
      "loss tensor(0.8271, dtype=torch.float64)\n",
      "loss tensor(0.7132, dtype=torch.float64)\n",
      "dev loss: 1.540344111101875\n",
      "\n",
      "Train Epoch 191\n",
      "-------------------------------\n",
      "loss: 0.754405  [  260/ 2430]\n",
      "Test Epoch 191\n",
      "-------------------------------\n",
      "loss tensor(0.8229, dtype=torch.float64)\n",
      "loss tensor(0.6917, dtype=torch.float64)\n",
      "dev loss: 1.5145197109500566\n",
      "\n",
      "Train Epoch 192\n",
      "-------------------------------\n",
      "loss: 0.562067  [  260/ 2430]\n",
      "Test Epoch 192\n",
      "-------------------------------\n",
      "loss tensor(0.7793, dtype=torch.float64)\n",
      "loss tensor(0.7392, dtype=torch.float64)\n",
      "dev loss: 1.5185640311875481\n",
      "\n",
      "Train Epoch 193\n",
      "-------------------------------\n",
      "loss: 0.578776  [  260/ 2430]\n",
      "Test Epoch 193\n",
      "-------------------------------\n",
      "loss tensor(0.8161, dtype=torch.float64)\n",
      "loss tensor(0.8768, dtype=torch.float64)\n",
      "dev loss: 1.6929414807684906\n",
      "\n",
      "Train Epoch 194\n",
      "-------------------------------\n",
      "loss: 0.681747  [  260/ 2430]\n",
      "Test Epoch 194\n",
      "-------------------------------\n",
      "loss tensor(0.8203, dtype=torch.float64)\n",
      "loss tensor(0.8420, dtype=torch.float64)\n",
      "dev loss: 1.6622157924084475\n",
      "\n",
      "Train Epoch 195\n",
      "-------------------------------\n",
      "loss: 0.676658  [  260/ 2430]\n",
      "Test Epoch 195\n",
      "-------------------------------\n",
      "loss tensor(0.7972, dtype=torch.float64)\n",
      "loss tensor(0.7473, dtype=torch.float64)\n",
      "dev loss: 1.5445521971089484\n",
      "\n",
      "Train Epoch 196\n",
      "-------------------------------\n",
      "loss: 0.614809  [  260/ 2430]\n",
      "Test Epoch 196\n",
      "-------------------------------\n",
      "loss tensor(0.7856, dtype=torch.float64)\n",
      "loss tensor(0.7562, dtype=torch.float64)\n",
      "dev loss: 1.5417061621915749\n",
      "\n",
      "Train Epoch 197\n",
      "-------------------------------\n",
      "loss: 0.437148  [  260/ 2430]\n",
      "Test Epoch 197\n",
      "-------------------------------\n",
      "loss tensor(0.7901, dtype=torch.float64)\n",
      "loss tensor(0.6759, dtype=torch.float64)\n",
      "dev loss: 1.466069858738047\n",
      "\n",
      "Train Epoch 198\n",
      "-------------------------------\n",
      "loss: 0.613361  [  260/ 2430]\n",
      "Test Epoch 198\n",
      "-------------------------------\n",
      "loss tensor(0.8293, dtype=torch.float64)\n",
      "loss tensor(0.9341, dtype=torch.float64)\n",
      "dev loss: 1.7634429952392316\n",
      "\n",
      "Train Epoch 199\n",
      "-------------------------------\n",
      "loss: 0.833452  [  260/ 2430]\n",
      "Test Epoch 199\n",
      "-------------------------------\n",
      "loss tensor(0.8242, dtype=torch.float64)\n",
      "loss tensor(0.8399, dtype=torch.float64)\n",
      "dev loss: 1.6641576328679504\n",
      "\n",
      "Train Epoch 200\n",
      "-------------------------------\n",
      "loss: 0.602823  [  260/ 2430]\n",
      "Test Epoch 200\n",
      "-------------------------------\n",
      "loss tensor(0.8174, dtype=torch.float64)\n",
      "loss tensor(0.7279, dtype=torch.float64)\n",
      "dev loss: 1.5453032990345712\n",
      "\n",
      "Train Epoch 201\n",
      "-------------------------------\n",
      "loss: 0.758296  [  260/ 2430]\n",
      "Test Epoch 201\n",
      "-------------------------------\n",
      "loss tensor(0.8693, dtype=torch.float64)\n",
      "loss tensor(0.7796, dtype=torch.float64)\n",
      "dev loss: 1.6488971863511122\n",
      "\n",
      "Train Epoch 202\n",
      "-------------------------------\n",
      "loss: 0.767708  [  260/ 2430]\n",
      "Test Epoch 202\n",
      "-------------------------------\n",
      "loss tensor(0.8657, dtype=torch.float64)\n",
      "loss tensor(0.6630, dtype=torch.float64)\n",
      "dev loss: 1.5286722828537904\n",
      "\n",
      "Train Epoch 203\n",
      "-------------------------------\n",
      "loss: 0.720918  [  260/ 2430]\n",
      "Test Epoch 203\n",
      "-------------------------------\n",
      "loss tensor(0.8192, dtype=torch.float64)\n",
      "loss tensor(0.7979, dtype=torch.float64)\n",
      "dev loss: 1.6171130761757162\n",
      "\n",
      "Train Epoch 204\n",
      "-------------------------------\n",
      "loss: 0.568054  [  260/ 2430]\n",
      "Test Epoch 204\n",
      "-------------------------------\n",
      "loss tensor(0.7985, dtype=torch.float64)\n",
      "loss tensor(0.7458, dtype=torch.float64)\n",
      "dev loss: 1.5443322037186649\n",
      "\n",
      "Train Epoch 205\n",
      "-------------------------------\n",
      "loss: 0.667771  [  260/ 2430]\n",
      "Test Epoch 205\n",
      "-------------------------------\n",
      "loss tensor(0.7780, dtype=torch.float64)\n",
      "loss tensor(0.7417, dtype=torch.float64)\n",
      "dev loss: 1.519684665062366\n",
      "\n",
      "Train Epoch 206\n",
      "-------------------------------\n",
      "loss: 0.663794  [  260/ 2430]\n",
      "Test Epoch 206\n",
      "-------------------------------\n",
      "loss tensor(0.8131, dtype=torch.float64)\n",
      "loss tensor(0.7085, dtype=torch.float64)\n",
      "dev loss: 1.5216246646625198\n",
      "\n",
      "Train Epoch 207\n",
      "-------------------------------\n",
      "loss: 0.665848  [  260/ 2430]\n",
      "Test Epoch 207\n",
      "-------------------------------\n",
      "loss tensor(0.8392, dtype=torch.float64)\n",
      "loss tensor(0.8833, dtype=torch.float64)\n",
      "dev loss: 1.7224938861263364\n",
      "\n",
      "Train Epoch 208\n",
      "-------------------------------\n",
      "loss: 0.690417  [  260/ 2430]\n",
      "Test Epoch 208\n",
      "-------------------------------\n",
      "loss tensor(0.8355, dtype=torch.float64)\n",
      "loss tensor(0.7086, dtype=torch.float64)\n",
      "dev loss: 1.544179366205839\n",
      "\n",
      "Train Epoch 209\n",
      "-------------------------------\n",
      "loss: 0.659391  [  260/ 2430]\n",
      "Test Epoch 209\n",
      "-------------------------------\n",
      "loss tensor(0.8183, dtype=torch.float64)\n",
      "loss tensor(0.8212, dtype=torch.float64)\n",
      "dev loss: 1.6395181281599205\n",
      "\n",
      "Train Epoch 210\n",
      "-------------------------------\n",
      "loss: 0.652779  [  260/ 2430]\n",
      "Test Epoch 210\n",
      "-------------------------------\n",
      "loss tensor(0.8390, dtype=torch.float64)\n",
      "loss tensor(0.8167, dtype=torch.float64)\n",
      "dev loss: 1.655643100245713\n",
      "\n",
      "Train Epoch 211\n",
      "-------------------------------\n",
      "loss: 0.674622  [  260/ 2430]\n",
      "Test Epoch 211\n",
      "-------------------------------\n",
      "loss tensor(0.8230, dtype=torch.float64)\n",
      "loss tensor(0.9216, dtype=torch.float64)\n",
      "dev loss: 1.744577925920892\n",
      "\n",
      "Train Epoch 212\n",
      "-------------------------------\n",
      "loss: 0.656401  [  260/ 2430]\n",
      "Test Epoch 212\n",
      "-------------------------------\n",
      "loss tensor(0.8160, dtype=torch.float64)\n",
      "loss tensor(0.7701, dtype=torch.float64)\n",
      "dev loss: 1.5860848538341912\n",
      "\n",
      "Train Epoch 213\n",
      "-------------------------------\n",
      "loss: 0.666416  [  260/ 2430]\n",
      "Test Epoch 213\n",
      "-------------------------------\n",
      "loss tensor(0.8120, dtype=torch.float64)\n",
      "loss tensor(0.8302, dtype=torch.float64)\n",
      "dev loss: 1.6422219273879395\n",
      "\n",
      "Train Epoch 214\n",
      "-------------------------------\n",
      "loss: 0.575177  [  260/ 2430]\n",
      "Test Epoch 214\n",
      "-------------------------------\n",
      "loss tensor(0.8130, dtype=torch.float64)\n",
      "loss tensor(0.8090, dtype=torch.float64)\n",
      "dev loss: 1.6219860215298625\n",
      "\n",
      "Train Epoch 215\n",
      "-------------------------------\n",
      "loss: 0.761430  [  260/ 2430]\n",
      "Test Epoch 215\n",
      "-------------------------------\n",
      "loss tensor(0.8560, dtype=torch.float64)\n",
      "loss tensor(0.6923, dtype=torch.float64)\n",
      "dev loss: 1.5482612307651071\n",
      "\n",
      "Train Epoch 216\n",
      "-------------------------------\n",
      "loss: 0.614291  [  260/ 2430]\n",
      "Test Epoch 216\n",
      "-------------------------------\n",
      "loss tensor(0.8449, dtype=torch.float64)\n",
      "loss tensor(0.6889, dtype=torch.float64)\n",
      "dev loss: 1.5338034208381126\n",
      "\n",
      "Train Epoch 217\n",
      "-------------------------------\n",
      "loss: 0.670020  [  260/ 2430]\n",
      "Test Epoch 217\n",
      "-------------------------------\n",
      "loss tensor(0.7980, dtype=torch.float64)\n",
      "loss tensor(1.0310, dtype=torch.float64)\n",
      "dev loss: 1.8289690423140472\n",
      "\n",
      "Train Epoch 218\n",
      "-------------------------------\n",
      "loss: 0.454999  [  260/ 2430]\n",
      "Test Epoch 218\n",
      "-------------------------------\n",
      "loss tensor(0.8061, dtype=torch.float64)\n",
      "loss tensor(0.7220, dtype=torch.float64)\n",
      "dev loss: 1.528066801902414\n",
      "\n",
      "Train Epoch 219\n",
      "-------------------------------\n",
      "loss: 0.761019  [  260/ 2430]\n",
      "Test Epoch 219\n",
      "-------------------------------\n",
      "loss tensor(0.8046, dtype=torch.float64)\n",
      "loss tensor(0.8740, dtype=torch.float64)\n",
      "dev loss: 1.6786440476765223\n",
      "\n",
      "Train Epoch 220\n",
      "-------------------------------\n",
      "loss: 0.662487  [  260/ 2430]\n",
      "Test Epoch 220\n",
      "-------------------------------\n",
      "loss tensor(0.7944, dtype=torch.float64)\n",
      "loss tensor(0.7630, dtype=torch.float64)\n",
      "dev loss: 1.5574215591082394\n",
      "\n",
      "Train Epoch 221\n",
      "-------------------------------\n",
      "loss: 0.634040  [  260/ 2430]\n",
      "Test Epoch 221\n",
      "-------------------------------\n",
      "loss tensor(0.7978, dtype=torch.float64)\n",
      "loss tensor(0.7599, dtype=torch.float64)\n",
      "dev loss: 1.5577191266524948\n",
      "\n",
      "Train Epoch 222\n",
      "-------------------------------\n",
      "loss: 0.585337  [  260/ 2430]\n",
      "Test Epoch 222\n",
      "-------------------------------\n",
      "loss tensor(0.8078, dtype=torch.float64)\n",
      "loss tensor(0.7222, dtype=torch.float64)\n",
      "dev loss: 1.530016598203488\n",
      "\n",
      "Train Epoch 223\n",
      "-------------------------------\n",
      "loss: 0.538703  [  260/ 2430]\n",
      "Test Epoch 223\n",
      "-------------------------------\n",
      "loss tensor(0.8190, dtype=torch.float64)\n",
      "loss tensor(0.7532, dtype=torch.float64)\n",
      "dev loss: 1.5721861337592742\n",
      "\n",
      "Train Epoch 224\n",
      "-------------------------------\n",
      "loss: 0.607880  [  260/ 2430]\n",
      "Test Epoch 224\n",
      "-------------------------------\n",
      "loss tensor(0.8202, dtype=torch.float64)\n",
      "loss tensor(0.9257, dtype=torch.float64)\n",
      "dev loss: 1.7458531183858386\n",
      "\n",
      "Train Epoch 225\n",
      "-------------------------------\n",
      "loss: 0.608887  [  260/ 2430]\n",
      "Test Epoch 225\n",
      "-------------------------------\n",
      "loss tensor(0.8061, dtype=torch.float64)\n",
      "loss tensor(0.6641, dtype=torch.float64)\n",
      "dev loss: 1.4702206573843335\n",
      "\n",
      "Train Epoch 226\n",
      "-------------------------------\n",
      "loss: 0.526922  [  260/ 2430]\n",
      "Test Epoch 226\n",
      "-------------------------------\n",
      "loss tensor(0.8182, dtype=torch.float64)\n",
      "loss tensor(0.7152, dtype=torch.float64)\n",
      "dev loss: 1.5333812874059534\n",
      "\n",
      "Train Epoch 227\n",
      "-------------------------------\n",
      "loss: 0.728446  [  260/ 2430]\n",
      "Test Epoch 227\n",
      "-------------------------------\n",
      "loss tensor(0.8017, dtype=torch.float64)\n",
      "loss tensor(0.7983, dtype=torch.float64)\n",
      "dev loss: 1.5999847118921546\n",
      "\n",
      "Train Epoch 228\n",
      "-------------------------------\n",
      "loss: 0.640653  [  260/ 2430]\n",
      "Test Epoch 228\n",
      "-------------------------------\n",
      "loss tensor(0.8273, dtype=torch.float64)\n",
      "loss tensor(0.7924, dtype=torch.float64)\n",
      "dev loss: 1.619759507412951\n",
      "\n",
      "Train Epoch 229\n",
      "-------------------------------\n",
      "loss: 0.639439  [  260/ 2430]\n",
      "Test Epoch 229\n",
      "-------------------------------\n",
      "loss tensor(0.8724, dtype=torch.float64)\n",
      "loss tensor(1.0309, dtype=torch.float64)\n",
      "dev loss: 1.9032875234797977\n",
      "\n",
      "Train Epoch 230\n",
      "-------------------------------\n",
      "loss: 0.667265  [  260/ 2430]\n",
      "Test Epoch 230\n",
      "-------------------------------\n",
      "loss tensor(0.8344, dtype=torch.float64)\n",
      "loss tensor(0.8048, dtype=torch.float64)\n",
      "dev loss: 1.6391723962256646\n",
      "\n",
      "Train Epoch 231\n",
      "-------------------------------\n",
      "loss: 0.685156  [  260/ 2430]\n",
      "Test Epoch 231\n",
      "-------------------------------\n",
      "loss tensor(0.7872, dtype=torch.float64)\n",
      "loss tensor(0.7071, dtype=torch.float64)\n",
      "dev loss: 1.4943021272878265\n",
      "\n",
      "Train Epoch 232\n",
      "-------------------------------\n",
      "loss: 0.758523  [  260/ 2430]\n",
      "Test Epoch 232\n",
      "-------------------------------\n",
      "loss tensor(0.8028, dtype=torch.float64)\n",
      "loss tensor(0.8729, dtype=torch.float64)\n",
      "dev loss: 1.6757028802911824\n",
      "\n",
      "Train Epoch 233\n",
      "-------------------------------\n",
      "loss: 0.822262  [  260/ 2430]\n",
      "Test Epoch 233\n",
      "-------------------------------\n",
      "loss tensor(0.8014, dtype=torch.float64)\n",
      "loss tensor(0.8117, dtype=torch.float64)\n",
      "dev loss: 1.613138959434952\n",
      "\n",
      "Train Epoch 234\n",
      "-------------------------------\n",
      "loss: 0.611666  [  260/ 2430]\n",
      "Test Epoch 234\n",
      "-------------------------------\n",
      "loss tensor(0.7995, dtype=torch.float64)\n",
      "loss tensor(0.7249, dtype=torch.float64)\n",
      "dev loss: 1.5244449616093108\n",
      "\n",
      "Train Epoch 235\n",
      "-------------------------------\n",
      "loss: 0.576645  [  260/ 2430]\n",
      "Test Epoch 235\n",
      "-------------------------------\n",
      "loss tensor(0.8198, dtype=torch.float64)\n",
      "loss tensor(0.7351, dtype=torch.float64)\n",
      "dev loss: 1.5549114413076173\n",
      "\n",
      "Train Epoch 236\n",
      "-------------------------------\n",
      "loss: 0.569336  [  260/ 2430]\n",
      "Test Epoch 236\n",
      "-------------------------------\n",
      "loss tensor(0.8127, dtype=torch.float64)\n",
      "loss tensor(0.9531, dtype=torch.float64)\n",
      "dev loss: 1.7657592938372464\n",
      "\n",
      "Train Epoch 237\n",
      "-------------------------------\n",
      "loss: 0.616151  [  260/ 2430]\n",
      "Test Epoch 237\n",
      "-------------------------------\n",
      "loss tensor(0.8362, dtype=torch.float64)\n",
      "loss tensor(1.0538, dtype=torch.float64)\n",
      "dev loss: 1.8900693241198598\n",
      "\n",
      "Train Epoch 238\n",
      "-------------------------------\n",
      "loss: 0.555031  [  260/ 2430]\n",
      "Test Epoch 238\n",
      "-------------------------------\n",
      "loss tensor(0.8118, dtype=torch.float64)\n",
      "loss tensor(0.8354, dtype=torch.float64)\n",
      "dev loss: 1.647241686527499\n",
      "\n",
      "Train Epoch 239\n",
      "-------------------------------\n",
      "loss: 0.596726  [  260/ 2430]\n",
      "Test Epoch 239\n",
      "-------------------------------\n",
      "loss tensor(0.8571, dtype=torch.float64)\n",
      "loss tensor(0.6777, dtype=torch.float64)\n",
      "dev loss: 1.5347407699366014\n",
      "\n",
      "Train Epoch 240\n",
      "-------------------------------\n",
      "loss: 0.579876  [  260/ 2430]\n",
      "Test Epoch 240\n",
      "-------------------------------\n",
      "loss tensor(0.7919, dtype=torch.float64)\n",
      "loss tensor(0.7379, dtype=torch.float64)\n",
      "dev loss: 1.5298038009024795\n",
      "\n",
      "Train Epoch 241\n",
      "-------------------------------\n",
      "loss: 0.584582  [  260/ 2430]\n",
      "Test Epoch 241\n",
      "-------------------------------\n",
      "loss tensor(0.8197, dtype=torch.float64)\n",
      "loss tensor(0.7300, dtype=torch.float64)\n",
      "dev loss: 1.5496909082500085\n",
      "\n",
      "Train Epoch 242\n",
      "-------------------------------\n",
      "loss: 0.643544  [  260/ 2430]\n",
      "Test Epoch 242\n",
      "-------------------------------\n",
      "loss tensor(0.8127, dtype=torch.float64)\n",
      "loss tensor(0.9557, dtype=torch.float64)\n",
      "dev loss: 1.7683827717024143\n",
      "\n",
      "Train Epoch 243\n",
      "-------------------------------\n",
      "loss: 0.657213  [  260/ 2430]\n",
      "Test Epoch 243\n",
      "-------------------------------\n",
      "loss tensor(0.8152, dtype=torch.float64)\n",
      "loss tensor(0.8019, dtype=torch.float64)\n",
      "dev loss: 1.6171308988794\n",
      "\n",
      "Train Epoch 244\n",
      "-------------------------------\n",
      "loss: 0.604595  [  260/ 2430]\n",
      "Test Epoch 244\n",
      "-------------------------------\n",
      "loss tensor(0.8289, dtype=torch.float64)\n",
      "loss tensor(0.7082, dtype=torch.float64)\n",
      "dev loss: 1.5370279550784103\n",
      "\n",
      "Train Epoch 245\n",
      "-------------------------------\n",
      "loss: 0.642860  [  260/ 2430]\n",
      "Test Epoch 245\n",
      "-------------------------------\n",
      "loss tensor(0.7739, dtype=torch.float64)\n",
      "loss tensor(0.7790, dtype=torch.float64)\n",
      "dev loss: 1.5528753743910122\n",
      "\n",
      "Train Epoch 246\n",
      "-------------------------------\n",
      "loss: 0.581430  [  260/ 2430]\n",
      "Test Epoch 246\n",
      "-------------------------------\n",
      "loss tensor(0.8716, dtype=torch.float64)\n",
      "loss tensor(1.0165, dtype=torch.float64)\n",
      "dev loss: 1.8880550647258225\n",
      "\n",
      "Train Epoch 247\n",
      "-------------------------------\n",
      "loss: 0.842124  [  260/ 2430]\n",
      "Test Epoch 247\n",
      "-------------------------------\n",
      "loss tensor(0.8210, dtype=torch.float64)\n",
      "loss tensor(0.7494, dtype=torch.float64)\n",
      "dev loss: 1.5704286331312702\n",
      "\n",
      "Train Epoch 248\n",
      "-------------------------------\n",
      "loss: 0.573055  [  260/ 2430]\n",
      "Test Epoch 248\n",
      "-------------------------------\n",
      "loss tensor(0.8330, dtype=torch.float64)\n",
      "loss tensor(0.7758, dtype=torch.float64)\n",
      "dev loss: 1.60882513013386\n",
      "\n",
      "Train Epoch 249\n",
      "-------------------------------\n",
      "loss: 0.585644  [  260/ 2430]\n",
      "Test Epoch 249\n",
      "-------------------------------\n",
      "loss tensor(0.8231, dtype=torch.float64)\n",
      "loss tensor(0.7519, dtype=torch.float64)\n",
      "dev loss: 1.574952336319603\n",
      "\n",
      "Train Epoch 250\n",
      "-------------------------------\n",
      "loss: 0.607081  [  260/ 2430]\n",
      "Test Epoch 250\n",
      "-------------------------------\n",
      "loss tensor(0.8001, dtype=torch.float64)\n",
      "loss tensor(0.8883, dtype=torch.float64)\n",
      "dev loss: 1.6883944706247798\n",
      "\n",
      "Train Epoch 251\n",
      "-------------------------------\n",
      "loss: 0.490941  [  260/ 2430]\n",
      "Test Epoch 251\n",
      "-------------------------------\n",
      "loss tensor(0.8206, dtype=torch.float64)\n",
      "loss tensor(0.8865, dtype=torch.float64)\n",
      "dev loss: 1.7070972003733853\n",
      "\n",
      "Train Epoch 252\n",
      "-------------------------------\n",
      "loss: 0.498678  [  260/ 2430]\n",
      "Test Epoch 252\n",
      "-------------------------------\n",
      "loss tensor(0.7812, dtype=torch.float64)\n",
      "loss tensor(0.8167, dtype=torch.float64)\n",
      "dev loss: 1.5979634124344761\n",
      "\n",
      "Train Epoch 253\n",
      "-------------------------------\n",
      "loss: 0.747297  [  260/ 2430]\n",
      "Test Epoch 253\n",
      "-------------------------------\n",
      "loss tensor(0.8400, dtype=torch.float64)\n",
      "loss tensor(0.6973, dtype=torch.float64)\n",
      "dev loss: 1.5372715627922915\n",
      "\n",
      "Train Epoch 254\n",
      "-------------------------------\n",
      "loss: 0.611105  [  260/ 2430]\n",
      "Test Epoch 254\n",
      "-------------------------------\n",
      "loss tensor(0.8254, dtype=torch.float64)\n",
      "loss tensor(0.9225, dtype=torch.float64)\n",
      "dev loss: 1.7479189923930951\n",
      "\n",
      "Train Epoch 255\n",
      "-------------------------------\n",
      "loss: 0.641939  [  260/ 2430]\n",
      "Test Epoch 255\n",
      "-------------------------------\n",
      "loss tensor(0.7961, dtype=torch.float64)\n",
      "loss tensor(0.8570, dtype=torch.float64)\n",
      "dev loss: 1.6530881501575478\n",
      "\n",
      "Train Epoch 256\n",
      "-------------------------------\n",
      "loss: 0.612266  [  260/ 2430]\n",
      "Test Epoch 256\n",
      "-------------------------------\n",
      "loss tensor(0.7800, dtype=torch.float64)\n",
      "loss tensor(0.6684, dtype=torch.float64)\n",
      "dev loss: 1.4484294480714568\n",
      "\n",
      "Train Epoch 257\n",
      "-------------------------------\n",
      "loss: 0.570596  [  260/ 2430]\n",
      "Test Epoch 257\n",
      "-------------------------------\n",
      "loss tensor(0.7776, dtype=torch.float64)\n",
      "loss tensor(0.7471, dtype=torch.float64)\n",
      "dev loss: 1.524698689143472\n",
      "\n",
      "Train Epoch 258\n",
      "-------------------------------\n",
      "loss: 0.651453  [  260/ 2430]\n",
      "Test Epoch 258\n",
      "-------------------------------\n",
      "loss tensor(0.8009, dtype=torch.float64)\n",
      "loss tensor(0.7400, dtype=torch.float64)\n",
      "dev loss: 1.5408449971045477\n",
      "\n",
      "Train Epoch 259\n",
      "-------------------------------\n",
      "loss: 0.743384  [  260/ 2430]\n",
      "Test Epoch 259\n",
      "-------------------------------\n",
      "loss tensor(0.8178, dtype=torch.float64)\n",
      "loss tensor(0.9523, dtype=torch.float64)\n",
      "dev loss: 1.7701044989926915\n",
      "\n",
      "Train Epoch 260\n",
      "-------------------------------\n",
      "loss: 0.671191  [  260/ 2430]\n",
      "Test Epoch 260\n",
      "-------------------------------\n",
      "loss tensor(0.8194, dtype=torch.float64)\n",
      "loss tensor(0.8263, dtype=torch.float64)\n",
      "dev loss: 1.6457639197382488\n",
      "\n",
      "Train Epoch 261\n",
      "-------------------------------\n",
      "loss: 0.667888  [  260/ 2430]\n",
      "Test Epoch 261\n",
      "-------------------------------\n",
      "loss tensor(0.8541, dtype=torch.float64)\n",
      "loss tensor(0.7288, dtype=torch.float64)\n",
      "dev loss: 1.5828550147721674\n",
      "\n",
      "Train Epoch 262\n",
      "-------------------------------\n",
      "loss: 0.527859  [  260/ 2430]\n",
      "Test Epoch 262\n",
      "-------------------------------\n",
      "loss tensor(0.8271, dtype=torch.float64)\n",
      "loss tensor(0.7981, dtype=torch.float64)\n",
      "dev loss: 1.625139735852858\n",
      "\n",
      "Train Epoch 263\n",
      "-------------------------------\n",
      "loss: 0.452996  [  260/ 2430]\n",
      "Test Epoch 263\n",
      "-------------------------------\n",
      "loss tensor(0.8310, dtype=torch.float64)\n",
      "loss tensor(0.9225, dtype=torch.float64)\n",
      "dev loss: 1.7534938404620952\n",
      "\n",
      "Train Epoch 264\n",
      "-------------------------------\n",
      "loss: 0.610085  [  260/ 2430]\n",
      "Test Epoch 264\n",
      "-------------------------------\n",
      "loss tensor(0.8000, dtype=torch.float64)\n",
      "loss tensor(0.8138, dtype=torch.float64)\n",
      "dev loss: 1.6137640709059549\n",
      "\n",
      "Train Epoch 265\n",
      "-------------------------------\n",
      "loss: 0.513403  [  260/ 2430]\n",
      "Test Epoch 265\n",
      "-------------------------------\n",
      "loss tensor(0.7825, dtype=torch.float64)\n",
      "loss tensor(0.7535, dtype=torch.float64)\n",
      "dev loss: 1.5359107765917532\n",
      "\n",
      "Train Epoch 266\n",
      "-------------------------------\n",
      "loss: 0.551192  [  260/ 2430]\n",
      "Test Epoch 266\n",
      "-------------------------------\n",
      "loss tensor(0.8441, dtype=torch.float64)\n",
      "loss tensor(1.0272, dtype=torch.float64)\n",
      "dev loss: 1.8712694422930496\n",
      "\n",
      "Train Epoch 267\n",
      "-------------------------------\n",
      "loss: 0.700946  [  260/ 2430]\n",
      "Test Epoch 267\n",
      "-------------------------------\n",
      "loss tensor(0.7983, dtype=torch.float64)\n",
      "loss tensor(0.7188, dtype=torch.float64)\n",
      "dev loss: 1.5170331966199342\n",
      "\n",
      "Train Epoch 268\n",
      "-------------------------------\n",
      "loss: 0.522090  [  260/ 2430]\n",
      "Test Epoch 268\n",
      "-------------------------------\n",
      "loss tensor(0.8199, dtype=torch.float64)\n",
      "loss tensor(0.6605, dtype=torch.float64)\n",
      "dev loss: 1.4803472136826705\n",
      "\n",
      "Train Epoch 269\n",
      "-------------------------------\n",
      "loss: 0.614908  [  260/ 2430]\n",
      "Test Epoch 269\n",
      "-------------------------------\n",
      "loss tensor(0.7994, dtype=torch.float64)\n",
      "loss tensor(0.7734, dtype=torch.float64)\n",
      "dev loss: 1.5728578379911182\n",
      "\n",
      "Train Epoch 270\n",
      "-------------------------------\n",
      "loss: 0.686043  [  260/ 2430]\n",
      "Test Epoch 270\n",
      "-------------------------------\n",
      "loss tensor(0.7986, dtype=torch.float64)\n",
      "loss tensor(0.9309, dtype=torch.float64)\n",
      "dev loss: 1.7294410589020481\n",
      "\n",
      "Train Epoch 271\n",
      "-------------------------------\n",
      "loss: 0.665830  [  260/ 2430]\n",
      "Test Epoch 271\n",
      "-------------------------------\n",
      "loss tensor(0.7951, dtype=torch.float64)\n",
      "loss tensor(0.7734, dtype=torch.float64)\n",
      "dev loss: 1.5685174220674059\n",
      "\n",
      "Train Epoch 272\n",
      "-------------------------------\n",
      "loss: 0.488188  [  260/ 2430]\n",
      "Test Epoch 272\n",
      "-------------------------------\n",
      "loss tensor(0.7975, dtype=torch.float64)\n",
      "loss tensor(0.7319, dtype=torch.float64)\n",
      "dev loss: 1.5294149303222444\n",
      "\n",
      "Train Epoch 273\n",
      "-------------------------------\n",
      "loss: 0.653488  [  260/ 2430]\n",
      "Test Epoch 273\n",
      "-------------------------------\n",
      "loss tensor(0.8192, dtype=torch.float64)\n",
      "loss tensor(0.9076, dtype=torch.float64)\n",
      "dev loss: 1.7268520737038362\n",
      "\n",
      "Train Epoch 274\n",
      "-------------------------------\n",
      "loss: 0.594920  [  260/ 2430]\n",
      "Test Epoch 274\n",
      "-------------------------------\n",
      "loss tensor(0.7839, dtype=torch.float64)\n",
      "loss tensor(0.7137, dtype=torch.float64)\n",
      "dev loss: 1.4975891301243922\n",
      "\n",
      "Train Epoch 275\n",
      "-------------------------------\n",
      "loss: 0.622272  [  260/ 2430]\n",
      "Test Epoch 275\n",
      "-------------------------------\n",
      "loss tensor(0.8619, dtype=torch.float64)\n",
      "loss tensor(0.7655, dtype=torch.float64)\n",
      "dev loss: 1.6273581200742067\n",
      "\n",
      "Train Epoch 276\n",
      "-------------------------------\n",
      "loss: 0.673406  [  260/ 2430]\n",
      "Test Epoch 276\n",
      "-------------------------------\n",
      "loss tensor(0.8263, dtype=torch.float64)\n",
      "loss tensor(0.7534, dtype=torch.float64)\n",
      "dev loss: 1.5796791778327353\n",
      "\n",
      "Train Epoch 277\n",
      "-------------------------------\n",
      "loss: 0.595583  [  260/ 2430]\n",
      "Test Epoch 277\n",
      "-------------------------------\n",
      "loss tensor(0.7982, dtype=torch.float64)\n",
      "loss tensor(0.7109, dtype=torch.float64)\n",
      "dev loss: 1.5090720565708915\n",
      "\n",
      "Train Epoch 278\n",
      "-------------------------------\n",
      "loss: 0.562159  [  260/ 2430]\n",
      "Test Epoch 278\n",
      "-------------------------------\n",
      "loss tensor(0.8155, dtype=torch.float64)\n",
      "loss tensor(0.8351, dtype=torch.float64)\n",
      "dev loss: 1.6506721267131037\n",
      "\n",
      "Train Epoch 279\n",
      "-------------------------------\n",
      "loss: 0.510149  [  260/ 2430]\n",
      "Test Epoch 279\n",
      "-------------------------------\n",
      "loss tensor(0.8229, dtype=torch.float64)\n",
      "loss tensor(0.7891, dtype=torch.float64)\n",
      "dev loss: 1.6119298006734177\n",
      "\n",
      "Train Epoch 280\n",
      "-------------------------------\n",
      "loss: 0.546062  [  260/ 2430]\n",
      "Test Epoch 280\n",
      "-------------------------------\n",
      "loss tensor(0.8430, dtype=torch.float64)\n",
      "loss tensor(0.6993, dtype=torch.float64)\n",
      "dev loss: 1.5422918108129138\n",
      "\n",
      "Train Epoch 281\n",
      "-------------------------------\n",
      "loss: 0.687733  [  260/ 2430]\n",
      "Test Epoch 281\n",
      "-------------------------------\n",
      "loss tensor(0.8185, dtype=torch.float64)\n",
      "loss tensor(0.7689, dtype=torch.float64)\n",
      "dev loss: 1.5873859211100214\n",
      "\n",
      "Train Epoch 282\n",
      "-------------------------------\n",
      "loss: 0.567448  [  260/ 2430]\n",
      "Test Epoch 282\n",
      "-------------------------------\n",
      "loss tensor(0.8085, dtype=torch.float64)\n",
      "loss tensor(0.8830, dtype=torch.float64)\n",
      "dev loss: 1.6914591351707076\n",
      "\n",
      "Train Epoch 283\n",
      "-------------------------------\n",
      "loss: 0.550308  [  260/ 2430]\n",
      "Test Epoch 283\n",
      "-------------------------------\n",
      "loss tensor(0.7918, dtype=torch.float64)\n",
      "loss tensor(0.8065, dtype=torch.float64)\n",
      "dev loss: 1.5983631075650502\n",
      "\n",
      "Train Epoch 284\n",
      "-------------------------------\n",
      "loss: 0.593105  [  260/ 2430]\n",
      "Test Epoch 284\n",
      "-------------------------------\n",
      "loss tensor(0.8131, dtype=torch.float64)\n",
      "loss tensor(0.7143, dtype=torch.float64)\n",
      "dev loss: 1.5274300567885566\n",
      "\n",
      "Train Epoch 285\n",
      "-------------------------------\n",
      "loss: 0.545021  [  260/ 2430]\n",
      "Test Epoch 285\n",
      "-------------------------------\n",
      "loss tensor(0.8216, dtype=torch.float64)\n",
      "loss tensor(0.7280, dtype=torch.float64)\n",
      "dev loss: 1.5495761089511704\n",
      "\n",
      "Train Epoch 286\n",
      "-------------------------------\n",
      "loss: 0.473285  [  260/ 2430]\n",
      "Test Epoch 286\n",
      "-------------------------------\n",
      "loss tensor(0.8548, dtype=torch.float64)\n",
      "loss tensor(0.8445, dtype=torch.float64)\n",
      "dev loss: 1.6993613656592852\n",
      "\n",
      "Train Epoch 287\n",
      "-------------------------------\n",
      "loss: 0.593458  [  260/ 2430]\n",
      "Test Epoch 287\n",
      "-------------------------------\n",
      "loss tensor(0.8563, dtype=torch.float64)\n",
      "loss tensor(0.9442, dtype=torch.float64)\n",
      "dev loss: 1.8004804529342517\n",
      "\n",
      "Train Epoch 288\n",
      "-------------------------------\n",
      "loss: 0.708046  [  260/ 2430]\n",
      "Test Epoch 288\n",
      "-------------------------------\n",
      "loss tensor(0.8226, dtype=torch.float64)\n",
      "loss tensor(0.8184, dtype=torch.float64)\n",
      "dev loss: 1.6410648343431586\n",
      "\n",
      "Train Epoch 289\n",
      "-------------------------------\n",
      "loss: 0.517699  [  260/ 2430]\n",
      "Test Epoch 289\n",
      "-------------------------------\n",
      "loss tensor(0.8262, dtype=torch.float64)\n",
      "loss tensor(0.8134, dtype=torch.float64)\n",
      "dev loss: 1.6396257203107303\n",
      "\n",
      "Train Epoch 290\n",
      "-------------------------------\n",
      "loss: 0.741690  [  260/ 2430]\n",
      "Test Epoch 290\n",
      "-------------------------------\n",
      "loss tensor(0.8048, dtype=torch.float64)\n",
      "loss tensor(0.8287, dtype=torch.float64)\n",
      "dev loss: 1.6335143005562054\n",
      "\n",
      "Train Epoch 291\n",
      "-------------------------------\n",
      "loss: 0.593356  [  260/ 2430]\n",
      "Test Epoch 291\n",
      "-------------------------------\n",
      "loss tensor(0.8621, dtype=torch.float64)\n",
      "loss tensor(0.8900, dtype=torch.float64)\n",
      "dev loss: 1.75210436448434\n",
      "\n",
      "Train Epoch 292\n",
      "-------------------------------\n",
      "loss: 0.525044  [  260/ 2430]\n",
      "Test Epoch 292\n",
      "-------------------------------\n",
      "loss tensor(0.8535, dtype=torch.float64)\n",
      "loss tensor(0.8159, dtype=torch.float64)\n",
      "dev loss: 1.669443591643278\n",
      "\n",
      "Train Epoch 293\n",
      "-------------------------------\n",
      "loss: 0.611834  [  260/ 2430]\n",
      "Test Epoch 293\n",
      "-------------------------------\n",
      "loss tensor(0.8801, dtype=torch.float64)\n",
      "loss tensor(0.7123, dtype=torch.float64)\n",
      "dev loss: 1.5924381622293469\n",
      "\n",
      "Train Epoch 294\n",
      "-------------------------------\n",
      "loss: 0.677448  [  260/ 2430]\n",
      "Test Epoch 294\n",
      "-------------------------------\n",
      "loss tensor(0.7764, dtype=torch.float64)\n",
      "loss tensor(0.8522, dtype=torch.float64)\n",
      "dev loss: 1.6286532046008662\n",
      "\n",
      "Train Epoch 295\n",
      "-------------------------------\n",
      "loss: 0.545686  [  260/ 2430]\n",
      "Test Epoch 295\n",
      "-------------------------------\n",
      "loss tensor(0.8271, dtype=torch.float64)\n",
      "loss tensor(0.8788, dtype=torch.float64)\n",
      "dev loss: 1.7058699467497456\n",
      "\n",
      "Train Epoch 296\n",
      "-------------------------------\n",
      "loss: 0.582486  [  260/ 2430]\n",
      "Test Epoch 296\n",
      "-------------------------------\n",
      "loss tensor(0.8174, dtype=torch.float64)\n",
      "loss tensor(0.9207, dtype=torch.float64)\n",
      "dev loss: 1.7380653440175657\n",
      "\n",
      "Train Epoch 297\n",
      "-------------------------------\n",
      "loss: 0.572646  [  260/ 2430]\n",
      "Test Epoch 297\n",
      "-------------------------------\n",
      "loss tensor(0.8011, dtype=torch.float64)\n",
      "loss tensor(0.8303, dtype=torch.float64)\n",
      "dev loss: 1.6313788283407535\n",
      "\n",
      "Train Epoch 298\n",
      "-------------------------------\n",
      "loss: 0.703695  [  260/ 2430]\n",
      "Test Epoch 298\n",
      "-------------------------------\n",
      "loss tensor(0.7767, dtype=torch.float64)\n",
      "loss tensor(0.7499, dtype=torch.float64)\n",
      "dev loss: 1.5266042571863316\n",
      "\n",
      "Train Epoch 299\n",
      "-------------------------------\n",
      "loss: 0.564427  [  260/ 2430]\n",
      "Test Epoch 299\n",
      "-------------------------------\n",
      "loss tensor(0.7813, dtype=torch.float64)\n",
      "loss tensor(0.7793, dtype=torch.float64)\n",
      "dev loss: 1.5606610512076875\n",
      "\n",
      "Train Epoch 300\n",
      "-------------------------------\n",
      "loss: 0.574563  [  260/ 2430]\n",
      "Test Epoch 300\n",
      "-------------------------------\n",
      "loss tensor(0.8344, dtype=torch.float64)\n",
      "loss tensor(1.1527, dtype=torch.float64)\n",
      "dev loss: 1.9870974192542679\n",
      "\n",
      "Train Epoch 301\n",
      "-------------------------------\n",
      "loss: 0.577323  [  260/ 2430]\n",
      "Test Epoch 301\n",
      "-------------------------------\n",
      "loss tensor(0.8074, dtype=torch.float64)\n",
      "loss tensor(0.8519, dtype=torch.float64)\n",
      "dev loss: 1.659358244418224\n",
      "\n",
      "Train Epoch 302\n",
      "-------------------------------\n",
      "loss: 0.570897  [  260/ 2430]\n",
      "Test Epoch 302\n",
      "-------------------------------\n",
      "loss tensor(0.8200, dtype=torch.float64)\n",
      "loss tensor(0.7963, dtype=torch.float64)\n",
      "dev loss: 1.6163027832181474\n",
      "\n",
      "Train Epoch 303\n",
      "-------------------------------\n",
      "loss: 0.576501  [  260/ 2430]\n",
      "Test Epoch 303\n",
      "-------------------------------\n",
      "loss tensor(0.7988, dtype=torch.float64)\n",
      "loss tensor(0.7830, dtype=torch.float64)\n",
      "dev loss: 1.5818227355460386\n",
      "\n",
      "Train Epoch 304\n",
      "-------------------------------\n",
      "loss: 0.592398  [  260/ 2430]\n",
      "Test Epoch 304\n",
      "-------------------------------\n",
      "loss tensor(0.8120, dtype=torch.float64)\n",
      "loss tensor(0.8868, dtype=torch.float64)\n",
      "dev loss: 1.6987676485599774\n",
      "\n",
      "Train Epoch 305\n",
      "-------------------------------\n",
      "loss: 0.690278  [  260/ 2430]\n",
      "Test Epoch 305\n",
      "-------------------------------\n",
      "loss tensor(0.8177, dtype=torch.float64)\n",
      "loss tensor(0.7156, dtype=torch.float64)\n",
      "dev loss: 1.5333031801962158\n",
      "\n",
      "Train Epoch 306\n",
      "-------------------------------\n",
      "loss: 0.716701  [  260/ 2430]\n",
      "Test Epoch 306\n",
      "-------------------------------\n",
      "loss tensor(0.8669, dtype=torch.float64)\n",
      "loss tensor(1.0982, dtype=torch.float64)\n",
      "dev loss: 1.9651223350636084\n",
      "\n",
      "Train Epoch 307\n",
      "-------------------------------\n",
      "loss: 0.672584  [  260/ 2430]\n",
      "Test Epoch 307\n",
      "-------------------------------\n",
      "loss tensor(0.7948, dtype=torch.float64)\n",
      "loss tensor(0.7782, dtype=torch.float64)\n",
      "dev loss: 1.5730052659777933\n",
      "\n",
      "Train Epoch 308\n",
      "-------------------------------\n",
      "loss: 0.595801  [  260/ 2430]\n",
      "Test Epoch 308\n",
      "-------------------------------\n",
      "loss tensor(0.8385, dtype=torch.float64)\n",
      "loss tensor(0.8310, dtype=torch.float64)\n",
      "dev loss: 1.6695490873883307\n",
      "\n",
      "Train Epoch 309\n",
      "-------------------------------\n",
      "loss: 0.507648  [  260/ 2430]\n",
      "Test Epoch 309\n",
      "-------------------------------\n",
      "loss tensor(0.7764, dtype=torch.float64)\n",
      "loss tensor(0.8055, dtype=torch.float64)\n",
      "dev loss: 1.581913422452206\n",
      "\n",
      "Train Epoch 310\n",
      "-------------------------------\n",
      "loss: 0.583471  [  260/ 2430]\n",
      "Test Epoch 310\n",
      "-------------------------------\n",
      "loss tensor(0.7843, dtype=torch.float64)\n",
      "loss tensor(0.7875, dtype=torch.float64)\n",
      "dev loss: 1.5717401967179754\n",
      "\n",
      "Train Epoch 311\n",
      "-------------------------------\n",
      "loss: 0.578928  [  260/ 2430]\n",
      "Test Epoch 311\n",
      "-------------------------------\n",
      "loss tensor(0.7905, dtype=torch.float64)\n",
      "loss tensor(0.7640, dtype=torch.float64)\n",
      "dev loss: 1.5545054656039394\n",
      "\n",
      "Train Epoch 312\n",
      "-------------------------------\n",
      "loss: 0.644357  [  260/ 2430]\n",
      "Test Epoch 312\n",
      "-------------------------------\n",
      "loss tensor(0.7832, dtype=torch.float64)\n",
      "loss tensor(0.8042, dtype=torch.float64)\n",
      "dev loss: 1.5873816163174825\n",
      "\n",
      "Train Epoch 313\n",
      "-------------------------------\n",
      "loss: 0.633877  [  260/ 2430]\n",
      "Test Epoch 313\n",
      "-------------------------------\n",
      "loss tensor(0.7931, dtype=torch.float64)\n",
      "loss tensor(0.8217, dtype=torch.float64)\n",
      "dev loss: 1.6148240958566884\n",
      "\n",
      "Train Epoch 314\n",
      "-------------------------------\n",
      "loss: 0.581442  [  260/ 2430]\n",
      "Test Epoch 314\n",
      "-------------------------------\n",
      "loss tensor(0.7996, dtype=torch.float64)\n",
      "loss tensor(0.8846, dtype=torch.float64)\n",
      "dev loss: 1.6842233042834107\n",
      "\n",
      "Train Epoch 315\n",
      "-------------------------------\n",
      "loss: 0.505113  [  260/ 2430]\n",
      "Test Epoch 315\n",
      "-------------------------------\n",
      "loss tensor(0.7897, dtype=torch.float64)\n",
      "loss tensor(0.8408, dtype=torch.float64)\n",
      "dev loss: 1.6305308971495824\n",
      "\n",
      "Train Epoch 316\n",
      "-------------------------------\n",
      "loss: 0.634559  [  260/ 2430]\n",
      "Test Epoch 316\n",
      "-------------------------------\n",
      "loss tensor(0.8613, dtype=torch.float64)\n",
      "loss tensor(0.9155, dtype=torch.float64)\n",
      "dev loss: 1.7768384997534739\n",
      "\n",
      "Train Epoch 317\n",
      "-------------------------------\n",
      "loss: 0.634825  [  260/ 2430]\n",
      "Test Epoch 317\n",
      "-------------------------------\n",
      "loss tensor(0.8897, dtype=torch.float64)\n",
      "loss tensor(0.9156, dtype=torch.float64)\n",
      "dev loss: 1.8053272650736374\n",
      "\n",
      "Train Epoch 318\n",
      "-------------------------------\n",
      "loss: 0.682626  [  260/ 2430]\n",
      "Test Epoch 318\n",
      "-------------------------------\n",
      "loss tensor(0.8488, dtype=torch.float64)\n",
      "loss tensor(0.7382, dtype=torch.float64)\n",
      "dev loss: 1.5869969150868313\n",
      "\n",
      "Train Epoch 319\n",
      "-------------------------------\n",
      "loss: 0.605039  [  260/ 2430]\n",
      "Test Epoch 319\n",
      "-------------------------------\n",
      "loss tensor(0.8020, dtype=torch.float64)\n",
      "loss tensor(0.8093, dtype=torch.float64)\n",
      "dev loss: 1.611314394812361\n",
      "\n",
      "Train Epoch 320\n",
      "-------------------------------\n",
      "loss: 0.531777  [  260/ 2430]\n",
      "Test Epoch 320\n",
      "-------------------------------\n",
      "loss tensor(0.8250, dtype=torch.float64)\n",
      "loss tensor(0.9258, dtype=torch.float64)\n",
      "dev loss: 1.7507327492538178\n",
      "\n",
      "Train Epoch 321\n",
      "-------------------------------\n",
      "loss: 0.598864  [  260/ 2430]\n",
      "Test Epoch 321\n",
      "-------------------------------\n",
      "loss tensor(0.7846, dtype=torch.float64)\n",
      "loss tensor(0.8345, dtype=torch.float64)\n",
      "dev loss: 1.6190913686828612\n",
      "\n",
      "Train Epoch 322\n",
      "-------------------------------\n",
      "loss: 0.628471  [  260/ 2430]\n",
      "Test Epoch 322\n",
      "-------------------------------\n",
      "loss tensor(0.7896, dtype=torch.float64)\n",
      "loss tensor(0.8965, dtype=torch.float64)\n",
      "dev loss: 1.6860651601646475\n",
      "\n",
      "Train Epoch 323\n",
      "-------------------------------\n",
      "loss: 0.572393  [  260/ 2430]\n",
      "Test Epoch 323\n",
      "-------------------------------\n",
      "loss tensor(0.8132, dtype=torch.float64)\n",
      "loss tensor(0.8115, dtype=torch.float64)\n",
      "dev loss: 1.6246368765232173\n",
      "\n",
      "Train Epoch 324\n",
      "-------------------------------\n",
      "loss: 0.441143  [  260/ 2430]\n",
      "Test Epoch 324\n",
      "-------------------------------\n",
      "loss tensor(0.7682, dtype=torch.float64)\n",
      "loss tensor(0.8495, dtype=torch.float64)\n",
      "dev loss: 1.6176577883955\n",
      "\n",
      "Train Epoch 325\n",
      "-------------------------------\n",
      "loss: 0.504779  [  260/ 2430]\n",
      "Test Epoch 325\n",
      "-------------------------------\n",
      "loss tensor(0.8194, dtype=torch.float64)\n",
      "loss tensor(0.7569, dtype=torch.float64)\n",
      "dev loss: 1.576280146071395\n",
      "\n",
      "Train Epoch 326\n",
      "-------------------------------\n",
      "loss: 0.505113  [  260/ 2430]\n",
      "Test Epoch 326\n",
      "-------------------------------\n",
      "loss tensor(0.8547, dtype=torch.float64)\n",
      "loss tensor(1.0504, dtype=torch.float64)\n",
      "dev loss: 1.90514892175901\n",
      "\n",
      "Train Epoch 327\n",
      "-------------------------------\n",
      "loss: 0.594417  [  260/ 2430]\n",
      "Test Epoch 327\n",
      "-------------------------------\n",
      "loss tensor(0.7867, dtype=torch.float64)\n",
      "loss tensor(0.8209, dtype=torch.float64)\n",
      "dev loss: 1.60760378365499\n",
      "\n",
      "Train Epoch 328\n",
      "-------------------------------\n",
      "loss: 0.506114  [  260/ 2430]\n",
      "Test Epoch 328\n",
      "-------------------------------\n",
      "loss tensor(0.8083, dtype=torch.float64)\n",
      "loss tensor(0.7908, dtype=torch.float64)\n",
      "dev loss: 1.5991300446988772\n",
      "\n",
      "Train Epoch 329\n",
      "-------------------------------\n",
      "loss: 0.522682  [  260/ 2430]\n",
      "Test Epoch 329\n",
      "-------------------------------\n",
      "loss tensor(0.7972, dtype=torch.float64)\n",
      "loss tensor(0.9373, dtype=torch.float64)\n",
      "dev loss: 1.7345061170887384\n",
      "\n",
      "Train Epoch 330\n",
      "-------------------------------\n",
      "loss: 0.591785  [  260/ 2430]\n",
      "Test Epoch 330\n",
      "-------------------------------\n",
      "loss tensor(0.7711, dtype=torch.float64)\n",
      "loss tensor(0.8934, dtype=torch.float64)\n",
      "dev loss: 1.66449726625807\n",
      "\n",
      "Train Epoch 331\n",
      "-------------------------------\n",
      "loss: 0.548751  [  260/ 2430]\n",
      "Test Epoch 331\n",
      "-------------------------------\n",
      "loss tensor(0.8299, dtype=torch.float64)\n",
      "loss tensor(0.7164, dtype=torch.float64)\n",
      "dev loss: 1.5462871171968198\n",
      "\n",
      "Train Epoch 332\n",
      "-------------------------------\n",
      "loss: 0.598981  [  260/ 2430]\n",
      "Test Epoch 332\n",
      "-------------------------------\n",
      "loss tensor(0.7929, dtype=torch.float64)\n",
      "loss tensor(1.0104, dtype=torch.float64)\n",
      "dev loss: 1.8033529988253307\n",
      "\n",
      "Train Epoch 333\n",
      "-------------------------------\n",
      "loss: 0.684020  [  260/ 2430]\n",
      "Test Epoch 333\n",
      "-------------------------------\n",
      "loss tensor(0.8383, dtype=torch.float64)\n",
      "loss tensor(0.8429, dtype=torch.float64)\n",
      "dev loss: 1.6812095590033729\n",
      "\n",
      "Train Epoch 334\n",
      "-------------------------------\n",
      "loss: 0.673098  [  260/ 2430]\n",
      "Test Epoch 334\n",
      "-------------------------------\n",
      "loss tensor(0.8360, dtype=torch.float64)\n",
      "loss tensor(0.8005, dtype=torch.float64)\n",
      "dev loss: 1.6364736109648512\n",
      "\n",
      "Train Epoch 335\n",
      "-------------------------------\n",
      "loss: 0.602900  [  260/ 2430]\n",
      "Test Epoch 335\n",
      "-------------------------------\n",
      "loss tensor(0.8464, dtype=torch.float64)\n",
      "loss tensor(1.0730, dtype=torch.float64)\n",
      "dev loss: 1.9194089877535638\n",
      "\n",
      "Train Epoch 336\n",
      "-------------------------------\n",
      "loss: 0.515082  [  260/ 2430]\n",
      "Test Epoch 336\n",
      "-------------------------------\n",
      "loss tensor(0.8088, dtype=torch.float64)\n",
      "loss tensor(0.7179, dtype=torch.float64)\n",
      "dev loss: 1.5266988176660354\n",
      "\n",
      "Train Epoch 337\n",
      "-------------------------------\n",
      "loss: 0.634081  [  260/ 2430]\n",
      "Test Epoch 337\n",
      "-------------------------------\n",
      "loss tensor(0.8069, dtype=torch.float64)\n",
      "loss tensor(0.9122, dtype=torch.float64)\n",
      "dev loss: 1.7190946080588363\n",
      "\n",
      "Train Epoch 338\n",
      "-------------------------------\n",
      "loss: 0.550564  [  260/ 2430]\n",
      "Test Epoch 338\n",
      "-------------------------------\n",
      "loss tensor(0.7862, dtype=torch.float64)\n",
      "loss tensor(0.7812, dtype=torch.float64)\n",
      "dev loss: 1.5673536199610671\n",
      "\n",
      "Train Epoch 339\n",
      "-------------------------------\n",
      "loss: 0.567076  [  260/ 2430]\n",
      "Test Epoch 339\n",
      "-------------------------------\n",
      "loss tensor(0.7867, dtype=torch.float64)\n",
      "loss tensor(0.8160, dtype=torch.float64)\n",
      "dev loss: 1.602688204685562\n",
      "\n",
      "Train Epoch 340\n",
      "-------------------------------\n",
      "loss: 0.686090  [  260/ 2430]\n",
      "Test Epoch 340\n",
      "-------------------------------\n",
      "loss tensor(0.7741, dtype=torch.float64)\n",
      "loss tensor(0.8336, dtype=torch.float64)\n",
      "dev loss: 1.607692544606275\n",
      "\n",
      "Train Epoch 341\n",
      "-------------------------------\n",
      "loss: 0.472407  [  260/ 2430]\n",
      "Test Epoch 341\n",
      "-------------------------------\n",
      "loss tensor(0.7837, dtype=torch.float64)\n",
      "loss tensor(0.7917, dtype=torch.float64)\n",
      "dev loss: 1.5753831790142911\n",
      "\n",
      "Train Epoch 342\n",
      "-------------------------------\n",
      "loss: 0.558583  [  260/ 2430]\n",
      "Test Epoch 342\n",
      "-------------------------------\n",
      "loss tensor(0.8134, dtype=torch.float64)\n",
      "loss tensor(0.9127, dtype=torch.float64)\n",
      "dev loss: 1.7261392497839294\n",
      "\n",
      "Train Epoch 343\n",
      "-------------------------------\n",
      "loss: 0.492696  [  260/ 2430]\n",
      "Test Epoch 343\n",
      "-------------------------------\n",
      "loss tensor(0.8092, dtype=torch.float64)\n",
      "loss tensor(0.9397, dtype=torch.float64)\n",
      "dev loss: 1.7489017254652506\n",
      "\n",
      "Train Epoch 344\n",
      "-------------------------------\n",
      "loss: 0.592855  [  260/ 2430]\n",
      "Test Epoch 344\n",
      "-------------------------------\n",
      "loss tensor(0.8209, dtype=torch.float64)\n",
      "loss tensor(0.7889, dtype=torch.float64)\n",
      "dev loss: 1.6098059223681007\n",
      "\n",
      "Train Epoch 345\n",
      "-------------------------------\n",
      "loss: 0.643030  [  260/ 2430]\n",
      "Test Epoch 345\n",
      "-------------------------------\n",
      "loss tensor(0.8245, dtype=torch.float64)\n",
      "loss tensor(0.7905, dtype=torch.float64)\n",
      "dev loss: 1.6150191235931048\n",
      "\n",
      "Train Epoch 346\n",
      "-------------------------------\n",
      "loss: 0.630230  [  260/ 2430]\n",
      "Test Epoch 346\n",
      "-------------------------------\n",
      "loss tensor(0.7993, dtype=torch.float64)\n",
      "loss tensor(0.9102, dtype=torch.float64)\n",
      "dev loss: 1.709465780108665\n",
      "\n",
      "Train Epoch 347\n",
      "-------------------------------\n",
      "loss: 0.584936  [  260/ 2430]\n",
      "Test Epoch 347\n",
      "-------------------------------\n",
      "loss tensor(0.7828, dtype=torch.float64)\n",
      "loss tensor(0.9515, dtype=torch.float64)\n",
      "dev loss: 1.7342254617830482\n",
      "\n",
      "Train Epoch 348\n",
      "-------------------------------\n",
      "loss: 0.562066  [  260/ 2430]\n",
      "Test Epoch 348\n",
      "-------------------------------\n",
      "loss tensor(0.7940, dtype=torch.float64)\n",
      "loss tensor(0.7870, dtype=torch.float64)\n",
      "dev loss: 1.5810772891729465\n",
      "\n",
      "Train Epoch 349\n",
      "-------------------------------\n",
      "loss: 0.484861  [  260/ 2430]\n",
      "Test Epoch 349\n",
      "-------------------------------\n",
      "loss tensor(0.8001, dtype=torch.float64)\n",
      "loss tensor(0.8932, dtype=torch.float64)\n",
      "dev loss: 1.693253269933857\n",
      "\n",
      "Train Epoch 350\n",
      "-------------------------------\n",
      "loss: 0.501846  [  260/ 2430]\n",
      "Test Epoch 350\n",
      "-------------------------------\n",
      "loss tensor(0.7823, dtype=torch.float64)\n",
      "loss tensor(0.8990, dtype=torch.float64)\n",
      "dev loss: 1.6813272674704076\n",
      "\n",
      "Train Epoch 351\n",
      "-------------------------------\n",
      "loss: 0.592575  [  260/ 2430]\n",
      "Test Epoch 351\n",
      "-------------------------------\n",
      "loss tensor(0.8164, dtype=torch.float64)\n",
      "loss tensor(0.8421, dtype=torch.float64)\n",
      "dev loss: 1.6584729650634624\n",
      "\n",
      "Train Epoch 352\n",
      "-------------------------------\n",
      "loss: 0.619309  [  260/ 2430]\n",
      "Test Epoch 352\n",
      "-------------------------------\n",
      "loss tensor(0.8227, dtype=torch.float64)\n",
      "loss tensor(0.8402, dtype=torch.float64)\n",
      "dev loss: 1.6629702783289009\n",
      "\n",
      "Train Epoch 353\n",
      "-------------------------------\n",
      "loss: 0.583878  [  260/ 2430]\n",
      "Test Epoch 353\n",
      "-------------------------------\n",
      "loss tensor(0.8729, dtype=torch.float64)\n",
      "loss tensor(0.7553, dtype=torch.float64)\n",
      "dev loss: 1.6281423263432147\n",
      "\n",
      "Train Epoch 354\n",
      "-------------------------------\n",
      "loss: 0.508324  [  260/ 2430]\n",
      "Test Epoch 354\n",
      "-------------------------------\n",
      "loss tensor(0.7993, dtype=torch.float64)\n",
      "loss tensor(0.7965, dtype=torch.float64)\n",
      "dev loss: 1.5958460730136768\n",
      "\n",
      "Train Epoch 355\n",
      "-------------------------------\n",
      "loss: 0.556202  [  260/ 2430]\n",
      "Test Epoch 355\n",
      "-------------------------------\n",
      "loss tensor(0.7952, dtype=torch.float64)\n",
      "loss tensor(0.8969, dtype=torch.float64)\n",
      "dev loss: 1.6920528215758814\n",
      "\n",
      "Train Epoch 356\n",
      "-------------------------------\n",
      "loss: 0.488215  [  260/ 2430]\n",
      "Test Epoch 356\n",
      "-------------------------------\n",
      "loss tensor(0.7951, dtype=torch.float64)\n",
      "loss tensor(0.7727, dtype=torch.float64)\n",
      "dev loss: 1.5677742146854161\n",
      "\n",
      "Train Epoch 357\n",
      "-------------------------------\n",
      "loss: 0.622732  [  260/ 2430]\n",
      "Test Epoch 357\n",
      "-------------------------------\n",
      "loss tensor(0.8055, dtype=torch.float64)\n",
      "loss tensor(0.8629, dtype=torch.float64)\n",
      "dev loss: 1.6683721372204303\n",
      "\n",
      "Train Epoch 358\n",
      "-------------------------------\n",
      "loss: 0.490497  [  260/ 2430]\n",
      "Test Epoch 358\n",
      "-------------------------------\n",
      "loss tensor(0.7750, dtype=torch.float64)\n",
      "loss tensor(0.7647, dtype=torch.float64)\n",
      "dev loss: 1.5397331481373517\n",
      "\n",
      "Train Epoch 359\n",
      "-------------------------------\n",
      "loss: 0.559926  [  260/ 2430]\n",
      "Test Epoch 359\n",
      "-------------------------------\n",
      "loss tensor(0.7853, dtype=torch.float64)\n",
      "loss tensor(0.9063, dtype=torch.float64)\n",
      "dev loss: 1.6915776468169492\n",
      "\n",
      "Train Epoch 360\n",
      "-------------------------------\n",
      "loss: 0.580718  [  260/ 2430]\n",
      "Test Epoch 360\n",
      "-------------------------------\n",
      "loss tensor(0.7886, dtype=torch.float64)\n",
      "loss tensor(0.9094, dtype=torch.float64)\n",
      "dev loss: 1.6979676537546258\n",
      "\n",
      "Train Epoch 361\n",
      "-------------------------------\n",
      "loss: 0.620027  [  260/ 2430]\n",
      "Test Epoch 361\n",
      "-------------------------------\n",
      "loss tensor(0.7954, dtype=torch.float64)\n",
      "loss tensor(1.0206, dtype=torch.float64)\n",
      "dev loss: 1.816002205580375\n",
      "\n",
      "Train Epoch 362\n",
      "-------------------------------\n",
      "loss: 0.560242  [  260/ 2430]\n",
      "Test Epoch 362\n",
      "-------------------------------\n",
      "loss tensor(0.8250, dtype=torch.float64)\n",
      "loss tensor(0.7964, dtype=torch.float64)\n",
      "dev loss: 1.6213807451590299\n",
      "\n",
      "Train Epoch 363\n",
      "-------------------------------\n",
      "loss: 0.406478  [  260/ 2430]\n",
      "Test Epoch 363\n",
      "-------------------------------\n",
      "loss tensor(0.8754, dtype=torch.float64)\n",
      "loss tensor(0.9654, dtype=torch.float64)\n",
      "dev loss: 1.8407722277581129\n",
      "\n",
      "Train Epoch 364\n",
      "-------------------------------\n",
      "loss: 0.597492  [  260/ 2430]\n",
      "Test Epoch 364\n",
      "-------------------------------\n",
      "loss tensor(0.7978, dtype=torch.float64)\n",
      "loss tensor(0.7758, dtype=torch.float64)\n",
      "dev loss: 1.5736322040561044\n",
      "\n",
      "Train Epoch 365\n",
      "-------------------------------\n",
      "loss: 0.590296  [  260/ 2430]\n",
      "Test Epoch 365\n",
      "-------------------------------\n",
      "loss tensor(0.7827, dtype=torch.float64)\n",
      "loss tensor(0.9206, dtype=torch.float64)\n",
      "dev loss: 1.7032340901112801\n",
      "\n",
      "Train Epoch 366\n",
      "-------------------------------\n",
      "loss: 0.544959  [  260/ 2430]\n",
      "Test Epoch 366\n",
      "-------------------------------\n",
      "loss tensor(0.8300, dtype=torch.float64)\n",
      "loss tensor(0.8969, dtype=torch.float64)\n",
      "dev loss: 1.7269120440248458\n",
      "\n",
      "Train Epoch 367\n",
      "-------------------------------\n",
      "loss: 0.581288  [  260/ 2430]\n",
      "Test Epoch 367\n",
      "-------------------------------\n",
      "loss tensor(0.8379, dtype=torch.float64)\n",
      "loss tensor(0.9388, dtype=torch.float64)\n",
      "dev loss: 1.776682186583129\n",
      "\n",
      "Train Epoch 368\n",
      "-------------------------------\n",
      "loss: 0.499354  [  260/ 2430]\n",
      "Test Epoch 368\n",
      "-------------------------------\n",
      "loss tensor(0.7949, dtype=torch.float64)\n",
      "loss tensor(0.8414, dtype=torch.float64)\n",
      "dev loss: 1.6363220548030275\n",
      "\n",
      "Train Epoch 369\n",
      "-------------------------------\n",
      "loss: 0.414662  [  260/ 2430]\n",
      "Test Epoch 369\n",
      "-------------------------------\n",
      "loss tensor(0.8955, dtype=torch.float64)\n",
      "loss tensor(0.6670, dtype=torch.float64)\n",
      "dev loss: 1.5624947034040815\n",
      "\n",
      "Train Epoch 370\n",
      "-------------------------------\n",
      "loss: 0.531915  [  260/ 2430]\n",
      "Test Epoch 370\n",
      "-------------------------------\n",
      "loss tensor(0.7884, dtype=torch.float64)\n",
      "loss tensor(0.9646, dtype=torch.float64)\n",
      "dev loss: 1.7530379374760363\n",
      "\n",
      "Train Epoch 371\n",
      "-------------------------------\n",
      "loss: 0.642115  [  260/ 2430]\n",
      "Test Epoch 371\n",
      "-------------------------------\n",
      "loss tensor(0.8053, dtype=torch.float64)\n",
      "loss tensor(0.7568, dtype=torch.float64)\n",
      "dev loss: 1.5621358280153128\n",
      "\n",
      "Train Epoch 372\n",
      "-------------------------------\n",
      "loss: 0.515879  [  260/ 2430]\n",
      "Test Epoch 372\n",
      "-------------------------------\n",
      "loss tensor(0.8258, dtype=torch.float64)\n",
      "loss tensor(1.0786, dtype=torch.float64)\n",
      "dev loss: 1.904465113649271\n",
      "\n",
      "Train Epoch 373\n",
      "-------------------------------\n",
      "loss: 0.583782  [  260/ 2430]\n",
      "Test Epoch 373\n",
      "-------------------------------\n",
      "loss tensor(0.8001, dtype=torch.float64)\n",
      "loss tensor(0.8602, dtype=torch.float64)\n",
      "dev loss: 1.6603536236515264\n",
      "\n",
      "Train Epoch 374\n",
      "-------------------------------\n",
      "loss: 0.491866  [  260/ 2430]\n",
      "Test Epoch 374\n",
      "-------------------------------\n",
      "loss tensor(0.8990, dtype=torch.float64)\n",
      "loss tensor(0.8008, dtype=torch.float64)\n",
      "dev loss: 1.6997795537804963\n",
      "\n",
      "Train Epoch 375\n",
      "-------------------------------\n",
      "loss: 0.621387  [  260/ 2430]\n",
      "Test Epoch 375\n",
      "-------------------------------\n",
      "loss tensor(0.8315, dtype=torch.float64)\n",
      "loss tensor(1.0249, dtype=torch.float64)\n",
      "dev loss: 1.8564690216010984\n",
      "\n",
      "Train Epoch 376\n",
      "-------------------------------\n",
      "loss: 0.622392  [  260/ 2430]\n",
      "Test Epoch 376\n",
      "-------------------------------\n",
      "loss tensor(0.7891, dtype=torch.float64)\n",
      "loss tensor(0.8897, dtype=torch.float64)\n",
      "dev loss: 1.6788475107571554\n",
      "\n",
      "Train Epoch 377\n",
      "-------------------------------\n",
      "loss: 0.604706  [  260/ 2430]\n",
      "Test Epoch 377\n",
      "-------------------------------\n",
      "loss tensor(0.8450, dtype=torch.float64)\n",
      "loss tensor(0.9109, dtype=torch.float64)\n",
      "dev loss: 1.755857950561797\n",
      "\n",
      "Train Epoch 378\n",
      "-------------------------------\n",
      "loss: 0.505433  [  260/ 2430]\n",
      "Test Epoch 378\n",
      "-------------------------------\n",
      "loss tensor(0.7813, dtype=torch.float64)\n",
      "loss tensor(0.7644, dtype=torch.float64)\n",
      "dev loss: 1.5456429203707291\n",
      "\n",
      "Train Epoch 379\n",
      "-------------------------------\n",
      "loss: 0.563675  [  260/ 2430]\n",
      "Test Epoch 379\n",
      "-------------------------------\n",
      "loss tensor(0.7701, dtype=torch.float64)\n",
      "loss tensor(0.9638, dtype=torch.float64)\n",
      "dev loss: 1.7338591829787893\n",
      "\n",
      "Train Epoch 380\n",
      "-------------------------------\n",
      "loss: 0.563663  [  260/ 2430]\n",
      "Test Epoch 380\n",
      "-------------------------------\n",
      "loss tensor(0.7734, dtype=torch.float64)\n",
      "loss tensor(0.7683, dtype=torch.float64)\n",
      "dev loss: 1.5416993795324392\n",
      "\n",
      "Train Epoch 381\n",
      "-------------------------------\n",
      "loss: 0.570852  [  260/ 2430]\n",
      "Test Epoch 381\n",
      "-------------------------------\n",
      "loss tensor(0.8033, dtype=torch.float64)\n",
      "loss tensor(0.8560, dtype=torch.float64)\n",
      "dev loss: 1.6592653142580391\n",
      "\n",
      "Train Epoch 382\n",
      "-------------------------------\n",
      "loss: 0.588684  [  260/ 2430]\n",
      "Test Epoch 382\n",
      "-------------------------------\n",
      "loss tensor(0.8035, dtype=torch.float64)\n",
      "loss tensor(0.8134, dtype=torch.float64)\n",
      "dev loss: 1.6169196876569303\n",
      "\n",
      "Train Epoch 383\n",
      "-------------------------------\n",
      "loss: 0.528535  [  260/ 2430]\n",
      "Test Epoch 383\n",
      "-------------------------------\n",
      "loss tensor(0.7931, dtype=torch.float64)\n",
      "loss tensor(0.9252, dtype=torch.float64)\n",
      "dev loss: 1.7183187929282855\n",
      "\n",
      "Train Epoch 384\n",
      "-------------------------------\n",
      "loss: 0.439620  [  260/ 2430]\n",
      "Test Epoch 384\n",
      "-------------------------------\n",
      "loss tensor(0.8433, dtype=torch.float64)\n",
      "loss tensor(0.9130, dtype=torch.float64)\n",
      "dev loss: 1.7562512548477125\n",
      "\n",
      "Train Epoch 385\n",
      "-------------------------------\n",
      "loss: 0.522450  [  260/ 2430]\n",
      "Test Epoch 385\n",
      "-------------------------------\n",
      "loss tensor(0.8320, dtype=torch.float64)\n",
      "loss tensor(0.7270, dtype=torch.float64)\n",
      "dev loss: 1.5590744129467913\n",
      "\n",
      "Train Epoch 386\n",
      "-------------------------------\n",
      "loss: 0.652404  [  260/ 2430]\n",
      "Test Epoch 386\n",
      "-------------------------------\n",
      "loss tensor(0.8560, dtype=torch.float64)\n",
      "loss tensor(0.9275, dtype=torch.float64)\n",
      "dev loss: 1.783476008416712\n",
      "\n",
      "Train Epoch 387\n",
      "-------------------------------\n",
      "loss: 0.496693  [  260/ 2430]\n",
      "Test Epoch 387\n",
      "-------------------------------\n",
      "loss tensor(0.8133, dtype=torch.float64)\n",
      "loss tensor(0.7707, dtype=torch.float64)\n",
      "dev loss: 1.5840354565199948\n",
      "\n",
      "Train Epoch 388\n",
      "-------------------------------\n",
      "loss: 0.523511  [  260/ 2430]\n",
      "Test Epoch 388\n",
      "-------------------------------\n",
      "loss tensor(0.8062, dtype=torch.float64)\n",
      "loss tensor(0.8785, dtype=torch.float64)\n",
      "dev loss: 1.6847225409681972\n",
      "\n",
      "Train Epoch 389\n",
      "-------------------------------\n",
      "loss: 0.484310  [  260/ 2430]\n",
      "Test Epoch 389\n",
      "-------------------------------\n",
      "loss tensor(0.8179, dtype=torch.float64)\n",
      "loss tensor(0.8946, dtype=torch.float64)\n",
      "dev loss: 1.7125244278591607\n",
      "\n",
      "Train Epoch 390\n",
      "-------------------------------\n",
      "loss: 0.586173  [  260/ 2430]\n",
      "Test Epoch 390\n",
      "-------------------------------\n",
      "loss tensor(0.7714, dtype=torch.float64)\n",
      "loss tensor(0.8217, dtype=torch.float64)\n",
      "dev loss: 1.5931031088848913\n",
      "\n",
      "Train Epoch 391\n",
      "-------------------------------\n",
      "loss: 0.674680  [  260/ 2430]\n",
      "Test Epoch 391\n",
      "-------------------------------\n",
      "loss tensor(0.7868, dtype=torch.float64)\n",
      "loss tensor(0.8941, dtype=torch.float64)\n",
      "dev loss: 1.6808826616042905\n",
      "\n",
      "Train Epoch 392\n",
      "-------------------------------\n",
      "loss: 0.584398  [  260/ 2430]\n",
      "Test Epoch 392\n",
      "-------------------------------\n",
      "loss tensor(0.8014, dtype=torch.float64)\n",
      "loss tensor(0.8901, dtype=torch.float64)\n",
      "dev loss: 1.6915209881002131\n",
      "\n",
      "Train Epoch 393\n",
      "-------------------------------\n",
      "loss: 0.468770  [  260/ 2430]\n",
      "Test Epoch 393\n",
      "-------------------------------\n",
      "loss tensor(0.8225, dtype=torch.float64)\n",
      "loss tensor(0.9243, dtype=torch.float64)\n",
      "dev loss: 1.7468129383387514\n",
      "\n",
      "Train Epoch 394\n",
      "-------------------------------\n",
      "loss: 0.466892  [  260/ 2430]\n",
      "Test Epoch 394\n",
      "-------------------------------\n",
      "loss tensor(0.8074, dtype=torch.float64)\n",
      "loss tensor(0.9783, dtype=torch.float64)\n",
      "dev loss: 1.7857127532588992\n",
      "\n",
      "Train Epoch 395\n",
      "-------------------------------\n",
      "loss: 0.446827  [  260/ 2430]\n",
      "Test Epoch 395\n",
      "-------------------------------\n",
      "loss tensor(0.7741, dtype=torch.float64)\n",
      "loss tensor(0.8558, dtype=torch.float64)\n",
      "dev loss: 1.6299196702802778\n",
      "\n",
      "Train Epoch 396\n",
      "-------------------------------\n",
      "loss: 0.497161  [  260/ 2430]\n",
      "Test Epoch 396\n",
      "-------------------------------\n",
      "loss tensor(0.8065, dtype=torch.float64)\n",
      "loss tensor(0.9409, dtype=torch.float64)\n",
      "dev loss: 1.7474618965335111\n",
      "\n",
      "Train Epoch 397\n",
      "-------------------------------\n",
      "loss: 0.606507  [  260/ 2430]\n",
      "Test Epoch 397\n",
      "-------------------------------\n",
      "loss tensor(0.7951, dtype=torch.float64)\n",
      "loss tensor(0.8454, dtype=torch.float64)\n",
      "dev loss: 1.6404704868693467\n",
      "\n",
      "Train Epoch 398\n",
      "-------------------------------\n",
      "loss: 0.545472  [  260/ 2430]\n",
      "Test Epoch 398\n",
      "-------------------------------\n",
      "loss tensor(0.7834, dtype=torch.float64)\n",
      "loss tensor(0.8896, dtype=torch.float64)\n",
      "dev loss: 1.672941498468051\n",
      "\n",
      "Train Epoch 399\n",
      "-------------------------------\n",
      "loss: 0.589680  [  260/ 2430]\n",
      "Test Epoch 399\n",
      "-------------------------------\n",
      "loss tensor(0.7905, dtype=torch.float64)\n",
      "loss tensor(0.9872, dtype=torch.float64)\n",
      "dev loss: 1.7777022908065359\n",
      "\n",
      "Train Epoch 400\n",
      "-------------------------------\n",
      "loss: 0.401652  [  260/ 2430]\n",
      "Test Epoch 400\n",
      "-------------------------------\n",
      "loss tensor(0.8234, dtype=torch.float64)\n",
      "loss tensor(0.8221, dtype=torch.float64)\n",
      "dev loss: 1.6454356882153138\n",
      "\n",
      "Train Epoch 401\n",
      "-------------------------------\n",
      "loss: 0.542093  [  260/ 2430]\n",
      "Test Epoch 401\n",
      "-------------------------------\n",
      "loss tensor(0.8060, dtype=torch.float64)\n",
      "loss tensor(0.9676, dtype=torch.float64)\n",
      "dev loss: 1.7736174686973705\n",
      "\n",
      "Train Epoch 402\n",
      "-------------------------------\n",
      "loss: 0.594450  [  260/ 2430]\n",
      "Test Epoch 402\n",
      "-------------------------------\n",
      "loss tensor(0.8098, dtype=torch.float64)\n",
      "loss tensor(0.9993, dtype=torch.float64)\n",
      "dev loss: 1.809120340344652\n",
      "\n",
      "Train Epoch 403\n",
      "-------------------------------\n",
      "loss: 0.629382  [  260/ 2430]\n",
      "Test Epoch 403\n",
      "-------------------------------\n",
      "loss tensor(0.8248, dtype=torch.float64)\n",
      "loss tensor(0.8609, dtype=torch.float64)\n",
      "dev loss: 1.6857204665183105\n",
      "\n",
      "Train Epoch 404\n",
      "-------------------------------\n",
      "loss: 0.489592  [  260/ 2430]\n",
      "Test Epoch 404\n",
      "-------------------------------\n",
      "loss tensor(0.8231, dtype=torch.float64)\n",
      "loss tensor(0.8156, dtype=torch.float64)\n",
      "dev loss: 1.638639860770253\n",
      "\n",
      "Train Epoch 405\n",
      "-------------------------------\n",
      "loss: 0.521835  [  260/ 2430]\n",
      "Test Epoch 405\n",
      "-------------------------------\n",
      "loss tensor(0.8197, dtype=torch.float64)\n",
      "loss tensor(0.9496, dtype=torch.float64)\n",
      "dev loss: 1.769381388683576\n",
      "\n",
      "Train Epoch 406\n",
      "-------------------------------\n",
      "loss: 0.513032  [  260/ 2430]\n",
      "Test Epoch 406\n",
      "-------------------------------\n",
      "loss tensor(0.8708, dtype=torch.float64)\n",
      "loss tensor(0.7638, dtype=torch.float64)\n",
      "dev loss: 1.6345416878875634\n",
      "\n",
      "Train Epoch 407\n",
      "-------------------------------\n",
      "loss: 0.684510  [  260/ 2430]\n",
      "Test Epoch 407\n",
      "-------------------------------\n",
      "loss tensor(0.7982, dtype=torch.float64)\n",
      "loss tensor(0.8990, dtype=torch.float64)\n",
      "dev loss: 1.6971381237637937\n",
      "\n",
      "Train Epoch 408\n",
      "-------------------------------\n",
      "loss: 0.514354  [  260/ 2430]\n",
      "Test Epoch 408\n",
      "-------------------------------\n",
      "loss tensor(0.7931, dtype=torch.float64)\n",
      "loss tensor(0.8632, dtype=torch.float64)\n",
      "dev loss: 1.6563445048080412\n",
      "\n",
      "Train Epoch 409\n",
      "-------------------------------\n",
      "loss: 0.573284  [  260/ 2430]\n",
      "Test Epoch 409\n",
      "-------------------------------\n",
      "loss tensor(0.7659, dtype=torch.float64)\n",
      "loss tensor(0.9232, dtype=torch.float64)\n",
      "dev loss: 1.6891151589363862\n",
      "\n",
      "Train Epoch 410\n",
      "-------------------------------\n",
      "loss: 0.594903  [  260/ 2430]\n",
      "Test Epoch 410\n",
      "-------------------------------\n",
      "loss tensor(0.7901, dtype=torch.float64)\n",
      "loss tensor(0.8124, dtype=torch.float64)\n",
      "dev loss: 1.602469308896993\n",
      "\n",
      "Train Epoch 411\n",
      "-------------------------------\n",
      "loss: 0.508779  [  260/ 2430]\n",
      "Test Epoch 411\n",
      "-------------------------------\n",
      "loss tensor(0.8091, dtype=torch.float64)\n",
      "loss tensor(0.8162, dtype=torch.float64)\n",
      "dev loss: 1.6252822389029244\n",
      "\n",
      "Train Epoch 412\n",
      "-------------------------------\n",
      "loss: 0.574328  [  260/ 2430]\n",
      "Test Epoch 412\n",
      "-------------------------------\n",
      "loss tensor(0.7697, dtype=torch.float64)\n",
      "loss tensor(0.9677, dtype=torch.float64)\n",
      "dev loss: 1.7373359553584669\n",
      "\n",
      "Train Epoch 413\n",
      "-------------------------------\n",
      "loss: 0.635238  [  260/ 2430]\n",
      "Test Epoch 413\n",
      "-------------------------------\n",
      "loss tensor(0.7864, dtype=torch.float64)\n",
      "loss tensor(0.9136, dtype=torch.float64)\n",
      "dev loss: 1.6999230343033638\n",
      "\n",
      "Train Epoch 414\n",
      "-------------------------------\n",
      "loss: 0.600541  [  260/ 2430]\n",
      "Test Epoch 414\n",
      "-------------------------------\n",
      "loss tensor(0.7933, dtype=torch.float64)\n",
      "loss tensor(0.8305, dtype=torch.float64)\n",
      "dev loss: 1.6237997112592577\n",
      "\n",
      "Train Epoch 415\n",
      "-------------------------------\n",
      "loss: 0.424183  [  260/ 2430]\n",
      "Test Epoch 415\n",
      "-------------------------------\n",
      "loss tensor(0.8170, dtype=torch.float64)\n",
      "loss tensor(0.9907, dtype=torch.float64)\n",
      "dev loss: 1.8076742093283331\n",
      "\n",
      "Train Epoch 416\n",
      "-------------------------------\n",
      "loss: 0.461935  [  260/ 2430]\n",
      "Test Epoch 416\n",
      "-------------------------------\n",
      "loss tensor(0.7754, dtype=torch.float64)\n",
      "loss tensor(0.8804, dtype=torch.float64)\n",
      "dev loss: 1.6558836165786726\n",
      "\n",
      "Train Epoch 417\n",
      "-------------------------------\n",
      "loss: 0.465866  [  260/ 2430]\n",
      "Test Epoch 417\n",
      "-------------------------------\n",
      "loss tensor(0.7923, dtype=torch.float64)\n",
      "loss tensor(1.0064, dtype=torch.float64)\n",
      "dev loss: 1.7986618535106862\n",
      "\n",
      "Train Epoch 418\n",
      "-------------------------------\n",
      "loss: 0.509201  [  260/ 2430]\n",
      "Test Epoch 418\n",
      "-------------------------------\n",
      "loss tensor(0.7763, dtype=torch.float64)\n",
      "loss tensor(0.7642, dtype=torch.float64)\n",
      "dev loss: 1.5405082346465664\n",
      "\n",
      "Train Epoch 419\n",
      "-------------------------------\n",
      "loss: 0.414363  [  260/ 2430]\n",
      "Test Epoch 419\n",
      "-------------------------------\n",
      "loss tensor(0.8357, dtype=torch.float64)\n",
      "loss tensor(1.0249, dtype=torch.float64)\n",
      "dev loss: 1.8605693804551435\n",
      "\n",
      "Train Epoch 420\n",
      "-------------------------------\n",
      "loss: 0.574089  [  260/ 2430]\n",
      "Test Epoch 420\n",
      "-------------------------------\n",
      "loss tensor(0.8243, dtype=torch.float64)\n",
      "loss tensor(0.7751, dtype=torch.float64)\n",
      "dev loss: 1.5993197500786134\n",
      "\n",
      "Train Epoch 421\n",
      "-------------------------------\n",
      "loss: 0.539231  [  260/ 2430]\n",
      "Test Epoch 421\n",
      "-------------------------------\n",
      "loss tensor(0.7941, dtype=torch.float64)\n",
      "loss tensor(0.9847, dtype=torch.float64)\n",
      "dev loss: 1.7787869026602028\n",
      "\n",
      "Train Epoch 422\n",
      "-------------------------------\n",
      "loss: 0.520016  [  260/ 2430]\n",
      "Test Epoch 422\n",
      "-------------------------------\n",
      "loss tensor(0.7779, dtype=torch.float64)\n",
      "loss tensor(0.8143, dtype=torch.float64)\n",
      "dev loss: 1.5921909412264672\n",
      "\n",
      "Train Epoch 423\n",
      "-------------------------------\n",
      "loss: 0.531982  [  260/ 2430]\n",
      "Test Epoch 423\n",
      "-------------------------------\n",
      "loss tensor(0.8111, dtype=torch.float64)\n",
      "loss tensor(0.7480, dtype=torch.float64)\n",
      "dev loss: 1.5591551173628186\n",
      "\n",
      "Train Epoch 424\n",
      "-------------------------------\n",
      "loss: 0.493854  [  260/ 2430]\n",
      "Test Epoch 424\n",
      "-------------------------------\n",
      "loss tensor(0.7864, dtype=torch.float64)\n",
      "loss tensor(1.0801, dtype=torch.float64)\n",
      "dev loss: 1.8665425634289334\n",
      "\n",
      "Train Epoch 425\n",
      "-------------------------------\n",
      "loss: 0.464443  [  260/ 2430]\n",
      "Test Epoch 425\n",
      "-------------------------------\n",
      "loss tensor(0.8163, dtype=torch.float64)\n",
      "loss tensor(0.7322, dtype=torch.float64)\n",
      "dev loss: 1.5485337327526598\n",
      "\n",
      "Train Epoch 426\n",
      "-------------------------------\n",
      "loss: 0.496991  [  260/ 2430]\n",
      "Test Epoch 426\n",
      "-------------------------------\n",
      "loss tensor(0.8284, dtype=torch.float64)\n",
      "loss tensor(1.2250, dtype=torch.float64)\n",
      "dev loss: 2.053317528448916\n",
      "\n",
      "Train Epoch 427\n",
      "-------------------------------\n",
      "loss: 0.497295  [  260/ 2430]\n",
      "Test Epoch 427\n",
      "-------------------------------\n",
      "loss tensor(0.8071, dtype=torch.float64)\n",
      "loss tensor(0.9534, dtype=torch.float64)\n",
      "dev loss: 1.7604740976883066\n",
      "\n",
      "Train Epoch 428\n",
      "-------------------------------\n",
      "loss: 0.585427  [  260/ 2430]\n",
      "Test Epoch 428\n",
      "-------------------------------\n",
      "loss tensor(0.8429, dtype=torch.float64)\n",
      "loss tensor(0.7932, dtype=torch.float64)\n",
      "dev loss: 1.6361660071751323\n",
      "\n",
      "Train Epoch 429\n",
      "-------------------------------\n",
      "loss: 0.567315  [  260/ 2430]\n",
      "Test Epoch 429\n",
      "-------------------------------\n",
      "loss tensor(0.8457, dtype=torch.float64)\n",
      "loss tensor(1.0325, dtype=torch.float64)\n",
      "dev loss: 1.8782757797767906\n",
      "\n",
      "Train Epoch 430\n",
      "-------------------------------\n",
      "loss: 0.585103  [  260/ 2430]\n",
      "Test Epoch 430\n",
      "-------------------------------\n",
      "loss tensor(0.8445, dtype=torch.float64)\n",
      "loss tensor(0.7681, dtype=torch.float64)\n",
      "dev loss: 1.6126401614520538\n",
      "\n",
      "Train Epoch 431\n",
      "-------------------------------\n",
      "loss: 0.694064  [  260/ 2430]\n",
      "Test Epoch 431\n",
      "-------------------------------\n",
      "loss tensor(0.7903, dtype=torch.float64)\n",
      "loss tensor(1.0754, dtype=torch.float64)\n",
      "dev loss: 1.8656341985821836\n",
      "\n",
      "Train Epoch 432\n",
      "-------------------------------\n",
      "loss: 0.534395  [  260/ 2430]\n",
      "Test Epoch 432\n",
      "-------------------------------\n",
      "loss tensor(0.8164, dtype=torch.float64)\n",
      "loss tensor(0.7658, dtype=torch.float64)\n",
      "dev loss: 1.5821835307083094\n",
      "\n",
      "Train Epoch 433\n",
      "-------------------------------\n",
      "loss: 0.551029  [  260/ 2430]\n",
      "Test Epoch 433\n",
      "-------------------------------\n",
      "loss tensor(0.8046, dtype=torch.float64)\n",
      "loss tensor(0.8146, dtype=torch.float64)\n",
      "dev loss: 1.6192506302883667\n",
      "\n",
      "Train Epoch 434\n",
      "-------------------------------\n",
      "loss: 0.504988  [  260/ 2430]\n",
      "Test Epoch 434\n",
      "-------------------------------\n",
      "loss tensor(0.7627, dtype=torch.float64)\n",
      "loss tensor(0.9157, dtype=torch.float64)\n",
      "dev loss: 1.678406698859237\n",
      "\n",
      "Train Epoch 435\n",
      "-------------------------------\n",
      "loss: 0.585717  [  260/ 2430]\n",
      "Test Epoch 435\n",
      "-------------------------------\n",
      "loss tensor(0.8070, dtype=torch.float64)\n",
      "loss tensor(0.7953, dtype=torch.float64)\n",
      "dev loss: 1.6022620744866067\n",
      "\n",
      "Train Epoch 436\n",
      "-------------------------------\n",
      "loss: 0.565475  [  260/ 2430]\n",
      "Test Epoch 436\n",
      "-------------------------------\n",
      "loss tensor(0.8090, dtype=torch.float64)\n",
      "loss tensor(1.1217, dtype=torch.float64)\n",
      "dev loss: 1.930717039162956\n",
      "\n",
      "Train Epoch 437\n",
      "-------------------------------\n",
      "loss: 0.492742  [  260/ 2430]\n",
      "Test Epoch 437\n",
      "-------------------------------\n",
      "loss tensor(0.8551, dtype=torch.float64)\n",
      "loss tensor(0.7209, dtype=torch.float64)\n",
      "dev loss: 1.575971958146965\n",
      "\n",
      "Train Epoch 438\n",
      "-------------------------------\n",
      "loss: 0.562815  [  260/ 2430]\n",
      "Test Epoch 438\n",
      "-------------------------------\n",
      "loss tensor(0.8792, dtype=torch.float64)\n",
      "loss tensor(1.0870, dtype=torch.float64)\n",
      "dev loss: 1.9661974678189487\n",
      "\n",
      "Train Epoch 439\n",
      "-------------------------------\n",
      "loss: 0.554097  [  260/ 2430]\n",
      "Test Epoch 439\n",
      "-------------------------------\n",
      "loss tensor(0.8245, dtype=torch.float64)\n",
      "loss tensor(0.9521, dtype=torch.float64)\n",
      "dev loss: 1.7766319501749397\n",
      "\n",
      "Train Epoch 440\n",
      "-------------------------------\n",
      "loss: 0.582191  [  260/ 2430]\n",
      "Test Epoch 440\n",
      "-------------------------------\n",
      "loss tensor(0.8253, dtype=torch.float64)\n",
      "loss tensor(0.8980, dtype=torch.float64)\n",
      "dev loss: 1.723285668843389\n",
      "\n",
      "Train Epoch 441\n",
      "-------------------------------\n",
      "loss: 0.514636  [  260/ 2430]\n",
      "Test Epoch 441\n",
      "-------------------------------\n",
      "loss tensor(0.7998, dtype=torch.float64)\n",
      "loss tensor(0.9841, dtype=torch.float64)\n",
      "dev loss: 1.7839153630052096\n",
      "\n",
      "Train Epoch 442\n",
      "-------------------------------\n",
      "loss: 0.518118  [  260/ 2430]\n",
      "Test Epoch 442\n",
      "-------------------------------\n",
      "loss tensor(0.8184, dtype=torch.float64)\n",
      "loss tensor(0.8145, dtype=torch.float64)\n",
      "dev loss: 1.6328907166800382\n",
      "\n",
      "Train Epoch 443\n",
      "-------------------------------\n",
      "loss: 0.577889  [  260/ 2430]\n",
      "Test Epoch 443\n",
      "-------------------------------\n",
      "loss tensor(0.8009, dtype=torch.float64)\n",
      "loss tensor(0.9599, dtype=torch.float64)\n",
      "dev loss: 1.7608239364357603\n",
      "\n",
      "Train Epoch 444\n",
      "-------------------------------\n",
      "loss: 0.528354  [  260/ 2430]\n",
      "Test Epoch 444\n",
      "-------------------------------\n",
      "loss tensor(0.8615, dtype=torch.float64)\n",
      "loss tensor(0.9080, dtype=torch.float64)\n",
      "dev loss: 1.7695153306860916\n",
      "\n",
      "Train Epoch 445\n",
      "-------------------------------\n",
      "loss: 0.466340  [  260/ 2430]\n",
      "Test Epoch 445\n",
      "-------------------------------\n",
      "loss tensor(0.8735, dtype=torch.float64)\n",
      "loss tensor(0.8014, dtype=torch.float64)\n",
      "dev loss: 1.674913781140698\n",
      "\n",
      "Train Epoch 446\n",
      "-------------------------------\n",
      "loss: 0.597928  [  260/ 2430]\n",
      "Test Epoch 446\n",
      "-------------------------------\n",
      "loss tensor(0.8871, dtype=torch.float64)\n",
      "loss tensor(0.8692, dtype=torch.float64)\n",
      "dev loss: 1.7562652240576715\n",
      "\n",
      "Train Epoch 447\n",
      "-------------------------------\n",
      "loss: 0.505530  [  260/ 2430]\n",
      "Test Epoch 447\n",
      "-------------------------------\n",
      "loss tensor(0.8367, dtype=torch.float64)\n",
      "loss tensor(0.7698, dtype=torch.float64)\n",
      "dev loss: 1.6064560808755481\n",
      "\n",
      "Train Epoch 448\n",
      "-------------------------------\n",
      "loss: 0.591100  [  260/ 2430]\n",
      "Test Epoch 448\n",
      "-------------------------------\n",
      "loss tensor(0.8236, dtype=torch.float64)\n",
      "loss tensor(0.8657, dtype=torch.float64)\n",
      "dev loss: 1.6892333469020548\n",
      "\n",
      "Train Epoch 449\n",
      "-------------------------------\n",
      "loss: 0.529927  [  260/ 2430]\n",
      "Test Epoch 449\n",
      "-------------------------------\n",
      "loss tensor(0.7696, dtype=torch.float64)\n",
      "loss tensor(0.8879, dtype=torch.float64)\n",
      "dev loss: 1.6574962884129503\n",
      "\n",
      "Train Epoch 450\n",
      "-------------------------------\n",
      "loss: 0.549105  [  260/ 2430]\n",
      "Test Epoch 450\n",
      "-------------------------------\n",
      "loss tensor(0.8392, dtype=torch.float64)\n",
      "loss tensor(0.8376, dtype=torch.float64)\n",
      "dev loss: 1.6768417999237928\n",
      "\n",
      "Train Epoch 451\n",
      "-------------------------------\n",
      "loss: 0.534846  [  260/ 2430]\n",
      "Test Epoch 451\n",
      "-------------------------------\n",
      "loss tensor(0.8774, dtype=torch.float64)\n",
      "loss tensor(1.0352, dtype=torch.float64)\n",
      "dev loss: 1.9125722377880168\n",
      "\n",
      "Train Epoch 452\n",
      "-------------------------------\n",
      "loss: 0.726019  [  260/ 2430]\n",
      "Test Epoch 452\n",
      "-------------------------------\n",
      "loss tensor(0.7808, dtype=torch.float64)\n",
      "loss tensor(0.8803, dtype=torch.float64)\n",
      "dev loss: 1.6610563723228235\n",
      "\n",
      "Train Epoch 453\n",
      "-------------------------------\n",
      "loss: 0.471585  [  260/ 2430]\n",
      "Test Epoch 453\n",
      "-------------------------------\n",
      "loss tensor(0.7941, dtype=torch.float64)\n",
      "loss tensor(0.9054, dtype=torch.float64)\n",
      "dev loss: 1.699454745798445\n",
      "\n",
      "Train Epoch 454\n",
      "-------------------------------\n",
      "loss: 0.505928  [  260/ 2430]\n",
      "Test Epoch 454\n",
      "-------------------------------\n",
      "loss tensor(0.8082, dtype=torch.float64)\n",
      "loss tensor(0.8920, dtype=torch.float64)\n",
      "dev loss: 1.7001924297998174\n",
      "\n",
      "Train Epoch 455\n",
      "-------------------------------\n",
      "loss: 0.485795  [  260/ 2430]\n",
      "Test Epoch 455\n",
      "-------------------------------\n",
      "loss tensor(0.8087, dtype=torch.float64)\n",
      "loss tensor(0.8398, dtype=torch.float64)\n",
      "dev loss: 1.6484761850819294\n",
      "\n",
      "Train Epoch 456\n",
      "-------------------------------\n",
      "loss: 0.526089  [  260/ 2430]\n",
      "Test Epoch 456\n",
      "-------------------------------\n",
      "loss tensor(0.8271, dtype=torch.float64)\n",
      "loss tensor(0.9839, dtype=torch.float64)\n",
      "dev loss: 1.811014680044547\n",
      "\n",
      "Train Epoch 457\n",
      "-------------------------------\n",
      "loss: 0.521160  [  260/ 2430]\n",
      "Test Epoch 457\n",
      "-------------------------------\n",
      "loss tensor(0.8331, dtype=torch.float64)\n",
      "loss tensor(0.7723, dtype=torch.float64)\n",
      "dev loss: 1.6053346173022018\n",
      "\n",
      "Train Epoch 458\n",
      "-------------------------------\n",
      "loss: 0.537791  [  260/ 2430]\n",
      "Test Epoch 458\n",
      "-------------------------------\n",
      "loss tensor(0.9190, dtype=torch.float64)\n",
      "loss tensor(1.1869, dtype=torch.float64)\n",
      "dev loss: 2.105881510640195\n",
      "\n",
      "Train Epoch 459\n",
      "-------------------------------\n",
      "loss: 0.559123  [  260/ 2430]\n",
      "Test Epoch 459\n",
      "-------------------------------\n",
      "loss tensor(0.8029, dtype=torch.float64)\n",
      "loss tensor(0.7804, dtype=torch.float64)\n",
      "dev loss: 1.583295922208434\n",
      "\n",
      "Train Epoch 460\n",
      "-------------------------------\n",
      "loss: 0.536138  [  260/ 2430]\n",
      "Test Epoch 460\n",
      "-------------------------------\n",
      "loss tensor(0.8227, dtype=torch.float64)\n",
      "loss tensor(0.8934, dtype=torch.float64)\n",
      "dev loss: 1.7160518365464532\n",
      "\n",
      "Train Epoch 461\n",
      "-------------------------------\n",
      "loss: 0.521814  [  260/ 2430]\n",
      "Test Epoch 461\n",
      "-------------------------------\n",
      "loss tensor(0.8350, dtype=torch.float64)\n",
      "loss tensor(1.0627, dtype=torch.float64)\n",
      "dev loss: 1.89764739695597\n",
      "\n",
      "Train Epoch 462\n",
      "-------------------------------\n",
      "loss: 0.567443  [  260/ 2430]\n",
      "Test Epoch 462\n",
      "-------------------------------\n",
      "loss tensor(0.8085, dtype=torch.float64)\n",
      "loss tensor(0.9750, dtype=torch.float64)\n",
      "dev loss: 1.7835636109965636\n",
      "\n",
      "Train Epoch 463\n",
      "-------------------------------\n",
      "loss: 0.454628  [  260/ 2430]\n",
      "Test Epoch 463\n",
      "-------------------------------\n",
      "loss tensor(0.8668, dtype=torch.float64)\n",
      "loss tensor(0.8534, dtype=torch.float64)\n",
      "dev loss: 1.720187386333042\n",
      "\n",
      "Train Epoch 464\n",
      "-------------------------------\n",
      "loss: 0.616216  [  260/ 2430]\n",
      "Test Epoch 464\n",
      "-------------------------------\n",
      "loss tensor(0.8219, dtype=torch.float64)\n",
      "loss tensor(0.9389, dtype=torch.float64)\n",
      "dev loss: 1.7608054791705678\n",
      "\n",
      "Train Epoch 465\n",
      "-------------------------------\n",
      "loss: 0.507468  [  260/ 2430]\n",
      "Test Epoch 465\n",
      "-------------------------------\n",
      "loss tensor(0.8823, dtype=torch.float64)\n",
      "loss tensor(1.0303, dtype=torch.float64)\n",
      "dev loss: 1.9126251681598065\n",
      "\n",
      "Train Epoch 466\n",
      "-------------------------------\n",
      "loss: 0.621183  [  260/ 2430]\n",
      "Test Epoch 466\n",
      "-------------------------------\n",
      "loss tensor(0.8530, dtype=torch.float64)\n",
      "loss tensor(1.0344, dtype=torch.float64)\n",
      "dev loss: 1.8874654305010232\n",
      "\n",
      "Train Epoch 467\n",
      "-------------------------------\n",
      "loss: 0.588969  [  260/ 2430]\n",
      "Test Epoch 467\n",
      "-------------------------------\n",
      "loss tensor(0.8484, dtype=torch.float64)\n",
      "loss tensor(0.8743, dtype=torch.float64)\n",
      "dev loss: 1.7226896157793994\n",
      "\n",
      "Train Epoch 468\n",
      "-------------------------------\n",
      "loss: 0.530600  [  260/ 2430]\n",
      "Test Epoch 468\n",
      "-------------------------------\n",
      "loss tensor(0.8182, dtype=torch.float64)\n",
      "loss tensor(0.8415, dtype=torch.float64)\n",
      "dev loss: 1.659676860889478\n",
      "\n",
      "Train Epoch 469\n",
      "-------------------------------\n",
      "loss: 0.516567  [  260/ 2430]\n",
      "Test Epoch 469\n",
      "-------------------------------\n",
      "loss tensor(0.8058, dtype=torch.float64)\n",
      "loss tensor(0.9129, dtype=torch.float64)\n",
      "dev loss: 1.7186770398456894\n",
      "\n",
      "Train Epoch 470\n",
      "-------------------------------\n",
      "loss: 0.574593  [  260/ 2430]\n",
      "Test Epoch 470\n",
      "-------------------------------\n",
      "loss tensor(0.7898, dtype=torch.float64)\n",
      "loss tensor(0.9351, dtype=torch.float64)\n",
      "dev loss: 1.7248407158815873\n",
      "\n",
      "Train Epoch 471\n",
      "-------------------------------\n",
      "loss: 0.465795  [  260/ 2430]\n",
      "Test Epoch 471\n",
      "-------------------------------\n",
      "loss tensor(0.8232, dtype=torch.float64)\n",
      "loss tensor(0.9704, dtype=torch.float64)\n",
      "dev loss: 1.7935878304525081\n",
      "\n",
      "Train Epoch 472\n",
      "-------------------------------\n",
      "loss: 0.536423  [  260/ 2430]\n",
      "Test Epoch 472\n",
      "-------------------------------\n",
      "loss tensor(0.7931, dtype=torch.float64)\n",
      "loss tensor(0.8749, dtype=torch.float64)\n",
      "dev loss: 1.668026528901863\n",
      "\n",
      "Train Epoch 473\n",
      "-------------------------------\n",
      "loss: 0.475820  [  260/ 2430]\n",
      "Test Epoch 473\n",
      "-------------------------------\n",
      "loss tensor(0.8073, dtype=torch.float64)\n",
      "loss tensor(0.9926, dtype=torch.float64)\n",
      "dev loss: 1.7998714160367568\n",
      "\n",
      "Train Epoch 474\n",
      "-------------------------------\n",
      "loss: 0.475585  [  260/ 2430]\n",
      "Test Epoch 474\n",
      "-------------------------------\n",
      "loss tensor(0.8771, dtype=torch.float64)\n",
      "loss tensor(0.8218, dtype=torch.float64)\n",
      "dev loss: 1.6988547416602542\n",
      "\n",
      "Train Epoch 475\n",
      "-------------------------------\n",
      "loss: 0.671503  [  260/ 2430]\n",
      "Test Epoch 475\n",
      "-------------------------------\n",
      "loss tensor(0.8396, dtype=torch.float64)\n",
      "loss tensor(1.1502, dtype=torch.float64)\n",
      "dev loss: 1.989857646522545\n",
      "\n",
      "Train Epoch 476\n",
      "-------------------------------\n",
      "loss: 0.519210  [  260/ 2430]\n",
      "Test Epoch 476\n",
      "-------------------------------\n",
      "loss tensor(1.0692, dtype=torch.float64)\n",
      "loss tensor(0.7887, dtype=torch.float64)\n",
      "dev loss: 1.8579046464993345\n",
      "\n",
      "Train Epoch 477\n",
      "-------------------------------\n",
      "loss: 0.799896  [  260/ 2430]\n",
      "Test Epoch 477\n",
      "-------------------------------\n",
      "loss tensor(0.8690, dtype=torch.float64)\n",
      "loss tensor(1.1563, dtype=torch.float64)\n",
      "dev loss: 2.0253815276089484\n",
      "\n",
      "Train Epoch 478\n",
      "-------------------------------\n",
      "loss: 0.555417  [  260/ 2430]\n",
      "Test Epoch 478\n",
      "-------------------------------\n",
      "loss tensor(0.8896, dtype=torch.float64)\n",
      "loss tensor(0.8363, dtype=torch.float64)\n",
      "dev loss: 1.7259206256874409\n",
      "\n",
      "Train Epoch 479\n",
      "-------------------------------\n",
      "loss: 0.527864  [  260/ 2430]\n",
      "Test Epoch 479\n",
      "-------------------------------\n",
      "loss tensor(0.7943, dtype=torch.float64)\n",
      "loss tensor(1.0808, dtype=torch.float64)\n",
      "dev loss: 1.8751073361418513\n",
      "\n",
      "Train Epoch 480\n",
      "-------------------------------\n",
      "loss: 0.490137  [  260/ 2430]\n",
      "Test Epoch 480\n",
      "-------------------------------\n",
      "loss tensor(0.8613, dtype=torch.float64)\n",
      "loss tensor(1.0834, dtype=torch.float64)\n",
      "dev loss: 1.9447130647795223\n",
      "\n",
      "Train Epoch 481\n",
      "-------------------------------\n",
      "loss: 0.570732  [  260/ 2430]\n",
      "Test Epoch 481\n",
      "-------------------------------\n",
      "loss tensor(0.8105, dtype=torch.float64)\n",
      "loss tensor(0.8073, dtype=torch.float64)\n",
      "dev loss: 1.6178027350464315\n",
      "\n",
      "Train Epoch 482\n",
      "-------------------------------\n",
      "loss: 0.465706  [  260/ 2430]\n",
      "Test Epoch 482\n",
      "-------------------------------\n",
      "loss tensor(0.8078, dtype=torch.float64)\n",
      "loss tensor(0.8869, dtype=torch.float64)\n",
      "dev loss: 1.6947783353403971\n",
      "\n",
      "Train Epoch 483\n",
      "-------------------------------\n",
      "loss: 0.543738  [  260/ 2430]\n",
      "Test Epoch 483\n",
      "-------------------------------\n",
      "loss tensor(0.8615, dtype=torch.float64)\n",
      "loss tensor(0.9178, dtype=torch.float64)\n",
      "dev loss: 1.7793491489104345\n",
      "\n",
      "Train Epoch 484\n",
      "-------------------------------\n",
      "loss: 0.541288  [  260/ 2430]\n",
      "Test Epoch 484\n",
      "-------------------------------\n",
      "loss tensor(0.8140, dtype=torch.float64)\n",
      "loss tensor(0.9750, dtype=torch.float64)\n",
      "dev loss: 1.7889950587807353\n",
      "\n",
      "Train Epoch 485\n",
      "-------------------------------\n",
      "loss: 0.444471  [  260/ 2430]\n",
      "Test Epoch 485\n",
      "-------------------------------\n",
      "loss tensor(0.9259, dtype=torch.float64)\n",
      "loss tensor(0.9460, dtype=torch.float64)\n",
      "dev loss: 1.871830020598257\n",
      "\n",
      "Train Epoch 486\n",
      "-------------------------------\n",
      "loss: 0.600427  [  260/ 2430]\n",
      "Test Epoch 486\n",
      "-------------------------------\n",
      "loss tensor(0.8066, dtype=torch.float64)\n",
      "loss tensor(1.0024, dtype=torch.float64)\n",
      "dev loss: 1.8090142836570915\n",
      "\n",
      "Train Epoch 487\n",
      "-------------------------------\n",
      "loss: 0.493251  [  260/ 2430]\n",
      "Test Epoch 487\n",
      "-------------------------------\n",
      "loss tensor(0.8428, dtype=torch.float64)\n",
      "loss tensor(0.7937, dtype=torch.float64)\n",
      "dev loss: 1.636506522283638\n",
      "\n",
      "Train Epoch 488\n",
      "-------------------------------\n",
      "loss: 0.532521  [  260/ 2430]\n",
      "Test Epoch 488\n",
      "-------------------------------\n",
      "loss tensor(0.8004, dtype=torch.float64)\n",
      "loss tensor(0.8361, dtype=torch.float64)\n",
      "dev loss: 1.6365037399998754\n",
      "\n",
      "Train Epoch 489\n",
      "-------------------------------\n",
      "loss: 0.502535  [  260/ 2430]\n",
      "Test Epoch 489\n",
      "-------------------------------\n",
      "loss tensor(0.7833, dtype=torch.float64)\n",
      "loss tensor(0.8855, dtype=torch.float64)\n",
      "dev loss: 1.668766931932557\n",
      "\n",
      "Train Epoch 490\n",
      "-------------------------------\n",
      "loss: 0.433727  [  260/ 2430]\n",
      "Test Epoch 490\n",
      "-------------------------------\n",
      "loss tensor(0.7681, dtype=torch.float64)\n",
      "loss tensor(1.0573, dtype=torch.float64)\n",
      "dev loss: 1.8254626013163238\n",
      "\n",
      "Train Epoch 491\n",
      "-------------------------------\n",
      "loss: 0.539472  [  260/ 2430]\n",
      "Test Epoch 491\n",
      "-------------------------------\n",
      "loss tensor(0.8022, dtype=torch.float64)\n",
      "loss tensor(0.9045, dtype=torch.float64)\n",
      "dev loss: 1.7067149169239013\n",
      "\n",
      "Train Epoch 492\n",
      "-------------------------------\n",
      "loss: 0.459380  [  260/ 2430]\n",
      "Test Epoch 492\n",
      "-------------------------------\n",
      "loss tensor(0.7849, dtype=torch.float64)\n",
      "loss tensor(0.8566, dtype=torch.float64)\n",
      "dev loss: 1.6414635063660807\n",
      "\n",
      "Train Epoch 493\n",
      "-------------------------------\n",
      "loss: 0.487552  [  260/ 2430]\n",
      "Test Epoch 493\n",
      "-------------------------------\n",
      "loss tensor(0.7918, dtype=torch.float64)\n",
      "loss tensor(0.9121, dtype=torch.float64)\n",
      "dev loss: 1.7039164081692326\n",
      "\n",
      "Train Epoch 494\n",
      "-------------------------------\n",
      "loss: 0.474840  [  260/ 2430]\n",
      "Test Epoch 494\n",
      "-------------------------------\n",
      "loss tensor(0.8236, dtype=torch.float64)\n",
      "loss tensor(0.8973, dtype=torch.float64)\n",
      "dev loss: 1.720906997952737\n",
      "\n",
      "Train Epoch 495\n",
      "-------------------------------\n",
      "loss: 0.510723  [  260/ 2430]\n",
      "Test Epoch 495\n",
      "-------------------------------\n",
      "loss tensor(0.8175, dtype=torch.float64)\n",
      "loss tensor(1.0922, dtype=torch.float64)\n",
      "dev loss: 1.9096870219780393\n",
      "\n",
      "Train Epoch 496\n",
      "-------------------------------\n",
      "loss: 0.446255  [  260/ 2430]\n",
      "Test Epoch 496\n",
      "-------------------------------\n",
      "loss tensor(0.8262, dtype=torch.float64)\n",
      "loss tensor(0.8726, dtype=torch.float64)\n",
      "dev loss: 1.698816984942575\n",
      "\n",
      "Train Epoch 497\n",
      "-------------------------------\n",
      "loss: 0.462726  [  260/ 2430]\n",
      "Test Epoch 497\n",
      "-------------------------------\n",
      "loss tensor(0.7972, dtype=torch.float64)\n",
      "loss tensor(0.8401, dtype=torch.float64)\n",
      "dev loss: 1.637205782370582\n",
      "\n",
      "Train Epoch 498\n",
      "-------------------------------\n",
      "loss: 0.446557  [  260/ 2430]\n",
      "Test Epoch 498\n",
      "-------------------------------\n",
      "loss tensor(0.8091, dtype=torch.float64)\n",
      "loss tensor(1.0555, dtype=torch.float64)\n",
      "dev loss: 1.864579939634861\n",
      "\n",
      "Train Epoch 499\n",
      "-------------------------------\n",
      "loss: 0.623460  [  260/ 2430]\n",
      "Test Epoch 499\n",
      "-------------------------------\n",
      "loss tensor(0.7835, dtype=torch.float64)\n",
      "loss tensor(0.9745, dtype=torch.float64)\n",
      "dev loss: 1.757979527837604\n",
      "\n",
      "Train Epoch 500\n",
      "-------------------------------\n",
      "loss: 0.492301  [  260/ 2430]\n",
      "Test Epoch 500\n",
      "-------------------------------\n",
      "loss tensor(0.8285, dtype=torch.float64)\n",
      "loss tensor(0.7651, dtype=torch.float64)\n",
      "dev loss: 1.5935898162929925\n",
      "\n",
      "Train Epoch 501\n",
      "-------------------------------\n",
      "loss: 0.591796  [  260/ 2430]\n",
      "Test Epoch 501\n",
      "-------------------------------\n",
      "loss tensor(0.8082, dtype=torch.float64)\n",
      "loss tensor(1.0293, dtype=torch.float64)\n",
      "dev loss: 1.8375341958422018\n",
      "\n",
      "Train Epoch 502\n",
      "-------------------------------\n",
      "loss: 0.539624  [  260/ 2430]\n",
      "Test Epoch 502\n",
      "-------------------------------\n",
      "loss tensor(0.8010, dtype=torch.float64)\n",
      "loss tensor(0.8532, dtype=torch.float64)\n",
      "dev loss: 1.6542107278291391\n",
      "\n",
      "Train Epoch 503\n",
      "-------------------------------\n",
      "loss: 0.510380  [  260/ 2430]\n",
      "Test Epoch 503\n",
      "-------------------------------\n",
      "loss tensor(0.8215, dtype=torch.float64)\n",
      "loss tensor(0.8492, dtype=torch.float64)\n",
      "dev loss: 1.6706988095234458\n",
      "\n",
      "Train Epoch 504\n",
      "-------------------------------\n",
      "loss: 0.519144  [  260/ 2430]\n",
      "Test Epoch 504\n",
      "-------------------------------\n",
      "loss tensor(0.8608, dtype=torch.float64)\n",
      "loss tensor(0.7701, dtype=torch.float64)\n",
      "dev loss: 1.6309280496243628\n",
      "\n",
      "Train Epoch 505\n",
      "-------------------------------\n",
      "loss: 0.541116  [  260/ 2430]\n",
      "Test Epoch 505\n",
      "-------------------------------\n",
      "loss tensor(0.8218, dtype=torch.float64)\n",
      "loss tensor(1.0756, dtype=torch.float64)\n",
      "dev loss: 1.8974221111001288\n",
      "\n",
      "Train Epoch 506\n",
      "-------------------------------\n",
      "loss: 0.519655  [  260/ 2430]\n",
      "Test Epoch 506\n",
      "-------------------------------\n",
      "loss tensor(0.7924, dtype=torch.float64)\n",
      "loss tensor(0.9231, dtype=torch.float64)\n",
      "dev loss: 1.7154472397634901\n",
      "\n",
      "Train Epoch 507\n",
      "-------------------------------\n",
      "loss: 0.463352  [  260/ 2430]\n",
      "Test Epoch 507\n",
      "-------------------------------\n",
      "loss tensor(0.7919, dtype=torch.float64)\n",
      "loss tensor(0.9015, dtype=torch.float64)\n",
      "dev loss: 1.693372993377797\n",
      "\n",
      "Train Epoch 508\n",
      "-------------------------------\n",
      "loss: 0.583368  [  260/ 2430]\n",
      "Test Epoch 508\n",
      "-------------------------------\n",
      "loss tensor(0.8365, dtype=torch.float64)\n",
      "loss tensor(1.1776, dtype=torch.float64)\n",
      "dev loss: 2.0140768255808577\n",
      "\n",
      "Train Epoch 509\n",
      "-------------------------------\n",
      "loss: 0.444292  [  260/ 2430]\n",
      "Test Epoch 509\n",
      "-------------------------------\n",
      "loss tensor(0.8807, dtype=torch.float64)\n",
      "loss tensor(0.7150, dtype=torch.float64)\n",
      "dev loss: 1.5957043984163057\n",
      "\n",
      "Train Epoch 510\n",
      "-------------------------------\n",
      "loss: 0.564918  [  260/ 2430]\n",
      "Test Epoch 510\n",
      "-------------------------------\n",
      "loss tensor(0.8920, dtype=torch.float64)\n",
      "loss tensor(1.2386, dtype=torch.float64)\n",
      "dev loss: 2.1305347649958666\n",
      "\n",
      "Train Epoch 511\n",
      "-------------------------------\n",
      "loss: 0.442164  [  260/ 2430]\n",
      "Test Epoch 511\n",
      "-------------------------------\n",
      "loss tensor(0.8933, dtype=torch.float64)\n",
      "loss tensor(1.2149, dtype=torch.float64)\n",
      "dev loss: 2.108236543978099\n",
      "\n",
      "Train Epoch 512\n",
      "-------------------------------\n",
      "loss: 0.649855  [  260/ 2430]\n",
      "Test Epoch 512\n",
      "-------------------------------\n",
      "loss tensor(0.8654, dtype=torch.float64)\n",
      "loss tensor(0.7062, dtype=torch.float64)\n",
      "dev loss: 1.57160263620803\n",
      "\n",
      "Train Epoch 513\n",
      "-------------------------------\n",
      "loss: 0.435397  [  260/ 2430]\n",
      "Test Epoch 513\n",
      "-------------------------------\n",
      "loss tensor(0.8597, dtype=torch.float64)\n",
      "loss tensor(1.0683, dtype=torch.float64)\n",
      "dev loss: 1.9279148313743675\n",
      "\n",
      "Train Epoch 514\n",
      "-------------------------------\n",
      "loss: 0.554095  [  260/ 2430]\n",
      "Test Epoch 514\n",
      "-------------------------------\n",
      "loss tensor(0.8468, dtype=torch.float64)\n",
      "loss tensor(0.8629, dtype=torch.float64)\n",
      "dev loss: 1.7097279594165427\n",
      "\n",
      "Train Epoch 515\n",
      "-------------------------------\n",
      "loss: 0.572602  [  260/ 2430]\n",
      "Test Epoch 515\n",
      "-------------------------------\n",
      "loss tensor(0.8306, dtype=torch.float64)\n",
      "loss tensor(1.0801, dtype=torch.float64)\n",
      "dev loss: 1.9107123495645433\n",
      "\n",
      "Train Epoch 516\n",
      "-------------------------------\n",
      "loss: 0.519534  [  260/ 2430]\n",
      "Test Epoch 516\n",
      "-------------------------------\n",
      "loss tensor(0.7999, dtype=torch.float64)\n",
      "loss tensor(0.8805, dtype=torch.float64)\n",
      "dev loss: 1.680358507037107\n",
      "\n",
      "Train Epoch 517\n",
      "-------------------------------\n",
      "loss: 0.618840  [  260/ 2430]\n",
      "Test Epoch 517\n",
      "-------------------------------\n",
      "loss tensor(0.8273, dtype=torch.float64)\n",
      "loss tensor(1.3018, dtype=torch.float64)\n",
      "dev loss: 2.129014595386301\n",
      "\n",
      "Train Epoch 518\n",
      "-------------------------------\n",
      "loss: 0.408575  [  260/ 2430]\n",
      "Test Epoch 518\n",
      "-------------------------------\n",
      "loss tensor(0.8145, dtype=torch.float64)\n",
      "loss tensor(0.8296, dtype=torch.float64)\n",
      "dev loss: 1.6440556063317\n",
      "\n",
      "Train Epoch 519\n",
      "-------------------------------\n",
      "loss: 0.493366  [  260/ 2430]\n",
      "Test Epoch 519\n",
      "-------------------------------\n",
      "loss tensor(0.8117, dtype=torch.float64)\n",
      "loss tensor(0.9685, dtype=torch.float64)\n",
      "dev loss: 1.7801976964020327\n",
      "\n",
      "Train Epoch 520\n",
      "-------------------------------\n",
      "loss: 0.524177  [  260/ 2430]\n",
      "Test Epoch 520\n",
      "-------------------------------\n",
      "loss tensor(0.8084, dtype=torch.float64)\n",
      "loss tensor(0.8644, dtype=torch.float64)\n",
      "dev loss: 1.6727716753342574\n",
      "\n",
      "Train Epoch 521\n",
      "-------------------------------\n",
      "loss: 0.442876  [  260/ 2430]\n",
      "Test Epoch 521\n",
      "-------------------------------\n",
      "loss tensor(0.8467, dtype=torch.float64)\n",
      "loss tensor(0.8003, dtype=torch.float64)\n",
      "dev loss: 1.6469894960058586\n",
      "\n",
      "Train Epoch 522\n",
      "-------------------------------\n",
      "loss: 0.587776  [  260/ 2430]\n",
      "Test Epoch 522\n",
      "-------------------------------\n",
      "loss tensor(0.8255, dtype=torch.float64)\n",
      "loss tensor(1.0200, dtype=torch.float64)\n",
      "dev loss: 1.8454592206507063\n",
      "\n",
      "Train Epoch 523\n",
      "-------------------------------\n",
      "loss: 0.520825  [  260/ 2430]\n",
      "Test Epoch 523\n",
      "-------------------------------\n",
      "loss tensor(0.7892, dtype=torch.float64)\n",
      "loss tensor(0.8432, dtype=torch.float64)\n",
      "dev loss: 1.6324095182018343\n",
      "\n",
      "Train Epoch 524\n",
      "-------------------------------\n",
      "loss: 0.529752  [  260/ 2430]\n",
      "Test Epoch 524\n",
      "-------------------------------\n",
      "loss tensor(0.7984, dtype=torch.float64)\n",
      "loss tensor(0.9194, dtype=torch.float64)\n",
      "dev loss: 1.7178052553320606\n",
      "\n",
      "Train Epoch 525\n",
      "-------------------------------\n",
      "loss: 0.519001  [  260/ 2430]\n",
      "Test Epoch 525\n",
      "-------------------------------\n",
      "loss tensor(0.8292, dtype=torch.float64)\n",
      "loss tensor(0.7371, dtype=torch.float64)\n",
      "dev loss: 1.5662966110033008\n",
      "\n",
      "Train Epoch 526\n",
      "-------------------------------\n",
      "loss: 0.538659  [  260/ 2430]\n",
      "Test Epoch 526\n",
      "-------------------------------\n",
      "loss tensor(0.8127, dtype=torch.float64)\n",
      "loss tensor(1.2395, dtype=torch.float64)\n",
      "dev loss: 2.0522239751441766\n",
      "\n",
      "Train Epoch 527\n",
      "-------------------------------\n",
      "loss: 0.578096  [  260/ 2430]\n",
      "Test Epoch 527\n",
      "-------------------------------\n",
      "loss tensor(0.8201, dtype=torch.float64)\n",
      "loss tensor(0.7974, dtype=torch.float64)\n",
      "dev loss: 1.6174468265516575\n",
      "\n",
      "Train Epoch 528\n",
      "-------------------------------\n",
      "loss: 0.446014  [  260/ 2430]\n",
      "Test Epoch 528\n",
      "-------------------------------\n",
      "loss tensor(0.8380, dtype=torch.float64)\n",
      "loss tensor(1.0145, dtype=torch.float64)\n",
      "dev loss: 1.8524902061447654\n",
      "\n",
      "Train Epoch 529\n",
      "-------------------------------\n",
      "loss: 0.480927  [  260/ 2430]\n",
      "Test Epoch 529\n",
      "-------------------------------\n",
      "loss tensor(0.8285, dtype=torch.float64)\n",
      "loss tensor(0.8734, dtype=torch.float64)\n",
      "dev loss: 1.7018738574455798\n",
      "\n",
      "Train Epoch 530\n",
      "-------------------------------\n",
      "loss: 0.420209  [  260/ 2430]\n",
      "Test Epoch 530\n",
      "-------------------------------\n",
      "loss tensor(0.7988, dtype=torch.float64)\n",
      "loss tensor(0.8741, dtype=torch.float64)\n",
      "dev loss: 1.672991954301056\n",
      "\n",
      "Train Epoch 531\n",
      "-------------------------------\n",
      "loss: 0.472365  [  260/ 2430]\n",
      "Test Epoch 531\n",
      "-------------------------------\n",
      "loss tensor(0.8028, dtype=torch.float64)\n",
      "loss tensor(0.9872, dtype=torch.float64)\n",
      "dev loss: 1.7900185951858405\n",
      "\n",
      "Train Epoch 532\n",
      "-------------------------------\n",
      "loss: 0.526802  [  260/ 2430]\n",
      "Test Epoch 532\n",
      "-------------------------------\n",
      "loss tensor(0.8204, dtype=torch.float64)\n",
      "loss tensor(0.9226, dtype=torch.float64)\n",
      "dev loss: 1.7429948979030772\n",
      "\n",
      "Train Epoch 533\n",
      "-------------------------------\n",
      "loss: 0.540446  [  260/ 2430]\n",
      "Test Epoch 533\n",
      "-------------------------------\n",
      "loss tensor(0.7940, dtype=torch.float64)\n",
      "loss tensor(0.8103, dtype=torch.float64)\n",
      "dev loss: 1.6042891866062212\n",
      "\n",
      "Train Epoch 534\n",
      "-------------------------------\n",
      "loss: 0.514448  [  260/ 2430]\n",
      "Test Epoch 534\n",
      "-------------------------------\n",
      "loss tensor(0.8152, dtype=torch.float64)\n",
      "loss tensor(0.9460, dtype=torch.float64)\n",
      "dev loss: 1.761191491741096\n",
      "\n",
      "Train Epoch 535\n",
      "-------------------------------\n",
      "loss: 0.451743  [  260/ 2430]\n",
      "Test Epoch 535\n",
      "-------------------------------\n",
      "loss tensor(0.7930, dtype=torch.float64)\n",
      "loss tensor(0.7712, dtype=torch.float64)\n",
      "dev loss: 1.5642255488578787\n",
      "\n",
      "Train Epoch 536\n",
      "-------------------------------\n",
      "loss: 0.526093  [  260/ 2430]\n",
      "Test Epoch 536\n",
      "-------------------------------\n",
      "loss tensor(0.7872, dtype=torch.float64)\n",
      "loss tensor(0.9207, dtype=torch.float64)\n",
      "dev loss: 1.7078666313223607\n",
      "\n",
      "Train Epoch 537\n",
      "-------------------------------\n",
      "loss: 0.412520  [  260/ 2430]\n",
      "Test Epoch 537\n",
      "-------------------------------\n",
      "loss tensor(0.7792, dtype=torch.float64)\n",
      "loss tensor(0.8450, dtype=torch.float64)\n",
      "dev loss: 1.6241948457370872\n",
      "\n",
      "Train Epoch 538\n",
      "-------------------------------\n",
      "loss: 0.432444  [  260/ 2430]\n",
      "Test Epoch 538\n",
      "-------------------------------\n",
      "loss tensor(0.8335, dtype=torch.float64)\n",
      "loss tensor(1.1605, dtype=torch.float64)\n",
      "dev loss: 1.9940670561666431\n",
      "\n",
      "Train Epoch 539\n",
      "-------------------------------\n",
      "loss: 0.465272  [  260/ 2430]\n",
      "Test Epoch 539\n",
      "-------------------------------\n",
      "loss tensor(0.7858, dtype=torch.float64)\n",
      "loss tensor(0.8485, dtype=torch.float64)\n",
      "dev loss: 1.6342773340410917\n",
      "\n",
      "Train Epoch 540\n",
      "-------------------------------\n",
      "loss: 0.391577  [  260/ 2430]\n",
      "Test Epoch 540\n",
      "-------------------------------\n",
      "loss tensor(0.8074, dtype=torch.float64)\n",
      "loss tensor(0.9248, dtype=torch.float64)\n",
      "dev loss: 1.7322232629447503\n",
      "\n",
      "Train Epoch 541\n",
      "-------------------------------\n",
      "loss: 0.496317  [  260/ 2430]\n",
      "Test Epoch 541\n",
      "-------------------------------\n",
      "loss tensor(0.8335, dtype=torch.float64)\n",
      "loss tensor(0.9958, dtype=torch.float64)\n",
      "dev loss: 1.8293088542967775\n",
      "\n",
      "Train Epoch 542\n",
      "-------------------------------\n",
      "loss: 0.452915  [  260/ 2430]\n",
      "Test Epoch 542\n",
      "-------------------------------\n",
      "loss tensor(0.8459, dtype=torch.float64)\n",
      "loss tensor(0.7581, dtype=torch.float64)\n",
      "dev loss: 1.6039537818644287\n",
      "\n",
      "Train Epoch 543\n",
      "-------------------------------\n",
      "loss: 0.624712  [  260/ 2430]\n",
      "Test Epoch 543\n",
      "-------------------------------\n",
      "loss tensor(0.7810, dtype=torch.float64)\n",
      "loss tensor(0.8201, dtype=torch.float64)\n",
      "dev loss: 1.6011387243118624\n",
      "\n",
      "Train Epoch 544\n",
      "-------------------------------\n",
      "loss: 0.548388  [  260/ 2430]\n",
      "Test Epoch 544\n",
      "-------------------------------\n",
      "loss tensor(0.8191, dtype=torch.float64)\n",
      "loss tensor(0.9833, dtype=torch.float64)\n",
      "dev loss: 1.8024212654777219\n",
      "\n",
      "Train Epoch 545\n",
      "-------------------------------\n",
      "loss: 0.508073  [  260/ 2430]\n",
      "Test Epoch 545\n",
      "-------------------------------\n",
      "loss tensor(0.8211, dtype=torch.float64)\n",
      "loss tensor(0.9865, dtype=torch.float64)\n",
      "dev loss: 1.8075839965664073\n",
      "\n",
      "Train Epoch 546\n",
      "-------------------------------\n",
      "loss: 0.518547  [  260/ 2430]\n",
      "Test Epoch 546\n",
      "-------------------------------\n",
      "loss tensor(0.7896, dtype=torch.float64)\n",
      "loss tensor(0.8890, dtype=torch.float64)\n",
      "dev loss: 1.678621844909947\n",
      "\n",
      "Train Epoch 547\n",
      "-------------------------------\n",
      "loss: 0.507724  [  260/ 2430]\n",
      "Test Epoch 547\n",
      "-------------------------------\n",
      "loss tensor(0.8664, dtype=torch.float64)\n",
      "loss tensor(1.3004, dtype=torch.float64)\n",
      "dev loss: 2.1668152242563616\n",
      "\n",
      "Train Epoch 548\n",
      "-------------------------------\n",
      "loss: 0.491013  [  260/ 2430]\n",
      "Test Epoch 548\n",
      "-------------------------------\n",
      "loss tensor(0.8215, dtype=torch.float64)\n",
      "loss tensor(1.0572, dtype=torch.float64)\n",
      "dev loss: 1.8787017323482025\n",
      "\n",
      "Train Epoch 549\n",
      "-------------------------------\n",
      "loss: 0.431402  [  260/ 2430]\n",
      "Test Epoch 549\n",
      "-------------------------------\n",
      "loss tensor(0.8623, dtype=torch.float64)\n",
      "loss tensor(0.6887, dtype=torch.float64)\n",
      "dev loss: 1.5509783657605867\n",
      "\n",
      "Train Epoch 550\n",
      "-------------------------------\n",
      "loss: 0.486420  [  260/ 2430]\n",
      "Test Epoch 550\n",
      "-------------------------------\n",
      "loss tensor(0.8047, dtype=torch.float64)\n",
      "loss tensor(1.0577, dtype=torch.float64)\n",
      "dev loss: 1.8623997303401214\n",
      "\n",
      "Train Epoch 551\n",
      "-------------------------------\n",
      "loss: 0.383693  [  260/ 2430]\n",
      "Test Epoch 551\n",
      "-------------------------------\n",
      "loss tensor(0.8138, dtype=torch.float64)\n",
      "loss tensor(0.8133, dtype=torch.float64)\n",
      "dev loss: 1.6271256626779307\n",
      "\n",
      "Train Epoch 552\n",
      "-------------------------------\n",
      "loss: 0.487963  [  260/ 2430]\n",
      "Test Epoch 552\n",
      "-------------------------------\n",
      "loss tensor(0.7952, dtype=torch.float64)\n",
      "loss tensor(1.0433, dtype=torch.float64)\n",
      "dev loss: 1.838525431205667\n",
      "\n",
      "Train Epoch 553\n",
      "-------------------------------\n",
      "loss: 0.391507  [  260/ 2430]\n",
      "Test Epoch 553\n",
      "-------------------------------\n",
      "loss tensor(0.7895, dtype=torch.float64)\n",
      "loss tensor(0.8295, dtype=torch.float64)\n",
      "dev loss: 1.6190075100162158\n",
      "\n",
      "Train Epoch 554\n",
      "-------------------------------\n",
      "loss: 0.468622  [  260/ 2430]\n",
      "Test Epoch 554\n",
      "-------------------------------\n",
      "loss tensor(0.8322, dtype=torch.float64)\n",
      "loss tensor(0.9750, dtype=torch.float64)\n",
      "dev loss: 1.8071238127372533\n",
      "\n",
      "Train Epoch 555\n",
      "-------------------------------\n",
      "loss: 0.484489  [  260/ 2430]\n",
      "Test Epoch 555\n",
      "-------------------------------\n",
      "loss tensor(0.7818, dtype=torch.float64)\n",
      "loss tensor(0.8023, dtype=torch.float64)\n",
      "dev loss: 1.5840652730342115\n",
      "\n",
      "Train Epoch 556\n",
      "-------------------------------\n",
      "loss: 0.449710  [  260/ 2430]\n",
      "Test Epoch 556\n",
      "-------------------------------\n",
      "loss tensor(0.8146, dtype=torch.float64)\n",
      "loss tensor(0.8030, dtype=torch.float64)\n",
      "dev loss: 1.6176187639191482\n",
      "\n",
      "Train Epoch 557\n",
      "-------------------------------\n",
      "loss: 0.520506  [  260/ 2430]\n",
      "Test Epoch 557\n",
      "-------------------------------\n",
      "loss tensor(0.7925, dtype=torch.float64)\n",
      "loss tensor(0.8625, dtype=torch.float64)\n",
      "dev loss: 1.6550539803358117\n",
      "\n",
      "Train Epoch 558\n",
      "-------------------------------\n",
      "loss: 0.528003  [  260/ 2430]\n",
      "Test Epoch 558\n",
      "-------------------------------\n",
      "loss tensor(0.8147, dtype=torch.float64)\n",
      "loss tensor(1.0842, dtype=torch.float64)\n",
      "dev loss: 1.898980254881184\n",
      "\n",
      "Train Epoch 559\n",
      "-------------------------------\n",
      "loss: 0.507740  [  260/ 2430]\n",
      "Test Epoch 559\n",
      "-------------------------------\n",
      "loss tensor(0.8002, dtype=torch.float64)\n",
      "loss tensor(0.7947, dtype=torch.float64)\n",
      "dev loss: 1.5948508272327473\n",
      "\n",
      "Train Epoch 560\n",
      "-------------------------------\n",
      "loss: 0.479696  [  260/ 2430]\n",
      "Test Epoch 560\n",
      "-------------------------------\n",
      "loss tensor(0.8030, dtype=torch.float64)\n",
      "loss tensor(0.8260, dtype=torch.float64)\n",
      "dev loss: 1.6290230990465588\n",
      "\n",
      "Train Epoch 561\n",
      "-------------------------------\n",
      "loss: 0.548186  [  260/ 2430]\n",
      "Test Epoch 561\n",
      "-------------------------------\n",
      "loss tensor(0.9272, dtype=torch.float64)\n",
      "loss tensor(1.7317, dtype=torch.float64)\n",
      "dev loss: 2.658821008511408\n",
      "\n",
      "Train Epoch 562\n",
      "-------------------------------\n",
      "loss: 0.699321  [  260/ 2430]\n",
      "Test Epoch 562\n",
      "-------------------------------\n",
      "loss tensor(0.8120, dtype=torch.float64)\n",
      "loss tensor(0.7826, dtype=torch.float64)\n",
      "dev loss: 1.59464286293337\n",
      "\n",
      "Train Epoch 563\n",
      "-------------------------------\n",
      "loss: 0.499206  [  260/ 2430]\n",
      "Test Epoch 563\n",
      "-------------------------------\n",
      "loss tensor(0.8486, dtype=torch.float64)\n",
      "loss tensor(0.8840, dtype=torch.float64)\n",
      "dev loss: 1.7326547242173878\n",
      "\n",
      "Train Epoch 564\n",
      "-------------------------------\n",
      "loss: 0.491190  [  260/ 2430]\n",
      "Test Epoch 564\n",
      "-------------------------------\n",
      "loss tensor(0.8490, dtype=torch.float64)\n",
      "loss tensor(1.0665, dtype=torch.float64)\n",
      "dev loss: 1.915572360869227\n",
      "\n",
      "Train Epoch 565\n",
      "-------------------------------\n",
      "loss: 0.608090  [  260/ 2430]\n",
      "Test Epoch 565\n",
      "-------------------------------\n",
      "loss tensor(0.8152, dtype=torch.float64)\n",
      "loss tensor(0.7824, dtype=torch.float64)\n",
      "dev loss: 1.5975526180250008\n",
      "\n",
      "Train Epoch 566\n",
      "-------------------------------\n",
      "loss: 0.447825  [  260/ 2430]\n",
      "Test Epoch 566\n",
      "-------------------------------\n",
      "loss tensor(0.7907, dtype=torch.float64)\n",
      "loss tensor(0.8262, dtype=torch.float64)\n",
      "dev loss: 1.61696672594848\n",
      "\n",
      "Train Epoch 567\n",
      "-------------------------------\n",
      "loss: 0.381386  [  260/ 2430]\n",
      "Test Epoch 567\n",
      "-------------------------------\n",
      "loss tensor(0.8035, dtype=torch.float64)\n",
      "loss tensor(0.9585, dtype=torch.float64)\n",
      "dev loss: 1.7619602424129786\n",
      "\n",
      "Train Epoch 568\n",
      "-------------------------------\n",
      "loss: 0.396022  [  260/ 2430]\n",
      "Test Epoch 568\n",
      "-------------------------------\n",
      "loss tensor(0.7983, dtype=torch.float64)\n",
      "loss tensor(0.8031, dtype=torch.float64)\n",
      "dev loss: 1.6014360204522569\n",
      "\n",
      "Train Epoch 569\n",
      "-------------------------------\n",
      "loss: 0.440749  [  260/ 2430]\n",
      "Test Epoch 569\n",
      "-------------------------------\n",
      "loss tensor(0.8586, dtype=torch.float64)\n",
      "loss tensor(1.4290, dtype=torch.float64)\n",
      "dev loss: 2.2875579286776615\n",
      "\n",
      "Train Epoch 570\n",
      "-------------------------------\n",
      "loss: 0.619060  [  260/ 2430]\n",
      "Test Epoch 570\n",
      "-------------------------------\n",
      "loss tensor(0.9054, dtype=torch.float64)\n",
      "loss tensor(0.7311, dtype=torch.float64)\n",
      "dev loss: 1.6364798789859107\n",
      "\n",
      "Train Epoch 571\n",
      "-------------------------------\n",
      "loss: 0.537221  [  260/ 2430]\n",
      "Test Epoch 571\n",
      "-------------------------------\n",
      "loss tensor(0.8465, dtype=torch.float64)\n",
      "loss tensor(1.1728, dtype=torch.float64)\n",
      "dev loss: 2.019330816353988\n",
      "\n",
      "Train Epoch 572\n",
      "-------------------------------\n",
      "loss: 0.558797  [  260/ 2430]\n",
      "Test Epoch 572\n",
      "-------------------------------\n",
      "loss tensor(0.8902, dtype=torch.float64)\n",
      "loss tensor(0.8652, dtype=torch.float64)\n",
      "dev loss: 1.755459164059737\n",
      "\n",
      "Train Epoch 573\n",
      "-------------------------------\n",
      "loss: 0.536392  [  260/ 2430]\n",
      "Test Epoch 573\n",
      "-------------------------------\n",
      "loss tensor(0.7954, dtype=torch.float64)\n",
      "loss tensor(0.8443, dtype=torch.float64)\n",
      "dev loss: 1.6397333361006723\n",
      "\n",
      "Train Epoch 574\n",
      "-------------------------------\n",
      "loss: 0.520590  [  260/ 2430]\n",
      "Test Epoch 574\n",
      "-------------------------------\n",
      "loss tensor(0.8177, dtype=torch.float64)\n",
      "loss tensor(0.8875, dtype=torch.float64)\n",
      "dev loss: 1.7051399428398595\n",
      "\n",
      "Train Epoch 575\n",
      "-------------------------------\n",
      "loss: 0.470244  [  260/ 2430]\n",
      "Test Epoch 575\n",
      "-------------------------------\n",
      "loss tensor(0.8027, dtype=torch.float64)\n",
      "loss tensor(0.7696, dtype=torch.float64)\n",
      "dev loss: 1.5722984988864341\n",
      "\n",
      "Train Epoch 576\n",
      "-------------------------------\n",
      "loss: 0.523146  [  260/ 2430]\n",
      "Test Epoch 576\n",
      "-------------------------------\n",
      "loss tensor(0.8391, dtype=torch.float64)\n",
      "loss tensor(0.9980, dtype=torch.float64)\n",
      "dev loss: 1.8371007263947043\n",
      "\n",
      "Train Epoch 577\n",
      "-------------------------------\n",
      "loss: 0.532994  [  260/ 2430]\n",
      "Test Epoch 577\n",
      "-------------------------------\n",
      "loss tensor(0.8400, dtype=torch.float64)\n",
      "loss tensor(1.0340, dtype=torch.float64)\n",
      "dev loss: 1.873954926356602\n",
      "\n",
      "Train Epoch 578\n",
      "-------------------------------\n",
      "loss: 0.442900  [  260/ 2430]\n",
      "Test Epoch 578\n",
      "-------------------------------\n",
      "loss tensor(0.8361, dtype=torch.float64)\n",
      "loss tensor(1.1163, dtype=torch.float64)\n",
      "dev loss: 1.9523778210552976\n",
      "\n",
      "Train Epoch 579\n",
      "-------------------------------\n",
      "loss: 0.592927  [  260/ 2430]\n",
      "Test Epoch 579\n",
      "-------------------------------\n",
      "loss tensor(0.8392, dtype=torch.float64)\n",
      "loss tensor(0.8633, dtype=torch.float64)\n",
      "dev loss: 1.702484142769595\n",
      "\n",
      "Train Epoch 580\n",
      "-------------------------------\n",
      "loss: 0.439113  [  260/ 2430]\n",
      "Test Epoch 580\n",
      "-------------------------------\n",
      "loss tensor(0.7872, dtype=torch.float64)\n",
      "loss tensor(1.0463, dtype=torch.float64)\n",
      "dev loss: 1.8334582867394094\n",
      "\n",
      "Train Epoch 581\n",
      "-------------------------------\n",
      "loss: 0.420269  [  260/ 2430]\n",
      "Test Epoch 581\n",
      "-------------------------------\n",
      "loss tensor(0.8058, dtype=torch.float64)\n",
      "loss tensor(0.8882, dtype=torch.float64)\n",
      "dev loss: 1.6939871070370964\n",
      "\n",
      "Train Epoch 582\n",
      "-------------------------------\n",
      "loss: 0.428785  [  260/ 2430]\n",
      "Test Epoch 582\n",
      "-------------------------------\n",
      "loss tensor(0.7890, dtype=torch.float64)\n",
      "loss tensor(0.9860, dtype=torch.float64)\n",
      "dev loss: 1.7750137523179155\n",
      "\n",
      "Train Epoch 583\n",
      "-------------------------------\n",
      "loss: 0.541740  [  260/ 2430]\n",
      "Test Epoch 583\n",
      "-------------------------------\n",
      "loss tensor(0.8172, dtype=torch.float64)\n",
      "loss tensor(0.7777, dtype=torch.float64)\n",
      "dev loss: 1.5948393585382088\n",
      "\n",
      "Train Epoch 584\n",
      "-------------------------------\n",
      "loss: 0.463569  [  260/ 2430]\n",
      "Test Epoch 584\n",
      "-------------------------------\n",
      "loss tensor(0.8903, dtype=torch.float64)\n",
      "loss tensor(1.2181, dtype=torch.float64)\n",
      "dev loss: 2.108438637323675\n",
      "\n",
      "Train Epoch 585\n",
      "-------------------------------\n",
      "loss: 0.507829  [  260/ 2430]\n",
      "Test Epoch 585\n",
      "-------------------------------\n",
      "loss tensor(0.8343, dtype=torch.float64)\n",
      "loss tensor(0.9136, dtype=torch.float64)\n",
      "dev loss: 1.7478553202378422\n",
      "\n",
      "Train Epoch 586\n",
      "-------------------------------\n",
      "loss: 0.459716  [  260/ 2430]\n",
      "Test Epoch 586\n",
      "-------------------------------\n",
      "loss tensor(0.8510, dtype=torch.float64)\n",
      "loss tensor(0.7795, dtype=torch.float64)\n",
      "dev loss: 1.6305005645267956\n",
      "\n",
      "Train Epoch 587\n",
      "-------------------------------\n",
      "loss: 0.510315  [  260/ 2430]\n",
      "Test Epoch 587\n",
      "-------------------------------\n",
      "loss tensor(0.8125, dtype=torch.float64)\n",
      "loss tensor(0.9258, dtype=torch.float64)\n",
      "dev loss: 1.7382246042471712\n",
      "\n",
      "Train Epoch 588\n",
      "-------------------------------\n",
      "loss: 0.432494  [  260/ 2430]\n",
      "Test Epoch 588\n",
      "-------------------------------\n",
      "loss tensor(0.8023, dtype=torch.float64)\n",
      "loss tensor(0.7581, dtype=torch.float64)\n",
      "dev loss: 1.5604023684399855\n",
      "\n",
      "Train Epoch 589\n",
      "-------------------------------\n",
      "loss: 0.485970  [  260/ 2430]\n",
      "Test Epoch 589\n",
      "-------------------------------\n",
      "loss tensor(0.8096, dtype=torch.float64)\n",
      "loss tensor(1.0235, dtype=torch.float64)\n",
      "dev loss: 1.8331380150093997\n",
      "\n",
      "Train Epoch 590\n",
      "-------------------------------\n",
      "loss: 0.394219  [  260/ 2430]\n",
      "Test Epoch 590\n",
      "-------------------------------\n",
      "loss tensor(0.8204, dtype=torch.float64)\n",
      "loss tensor(0.7724, dtype=torch.float64)\n",
      "dev loss: 1.5927886102178097\n",
      "\n",
      "Train Epoch 591\n",
      "-------------------------------\n",
      "loss: 0.544106  [  260/ 2430]\n",
      "Test Epoch 591\n",
      "-------------------------------\n",
      "loss tensor(0.8241, dtype=torch.float64)\n",
      "loss tensor(1.0707, dtype=torch.float64)\n",
      "dev loss: 1.8948209080621568\n",
      "\n",
      "Train Epoch 592\n",
      "-------------------------------\n",
      "loss: 0.398287  [  260/ 2430]\n",
      "Test Epoch 592\n",
      "-------------------------------\n",
      "loss tensor(0.8439, dtype=torch.float64)\n",
      "loss tensor(0.8468, dtype=torch.float64)\n",
      "dev loss: 1.6906988170809023\n",
      "\n",
      "Train Epoch 593\n",
      "-------------------------------\n",
      "loss: 0.589478  [  260/ 2430]\n",
      "Test Epoch 593\n",
      "-------------------------------\n",
      "loss tensor(0.8478, dtype=torch.float64)\n",
      "loss tensor(0.9911, dtype=torch.float64)\n",
      "dev loss: 1.8388516461601037\n",
      "\n",
      "Train Epoch 594\n",
      "-------------------------------\n",
      "loss: 0.476063  [  260/ 2430]\n",
      "Test Epoch 594\n",
      "-------------------------------\n",
      "loss tensor(0.8366, dtype=torch.float64)\n",
      "loss tensor(0.9986, dtype=torch.float64)\n",
      "dev loss: 1.8351928539847306\n",
      "\n",
      "Train Epoch 595\n",
      "-------------------------------\n",
      "loss: 0.443900  [  260/ 2430]\n",
      "Test Epoch 595\n",
      "-------------------------------\n",
      "loss tensor(0.8830, dtype=torch.float64)\n",
      "loss tensor(0.8593, dtype=torch.float64)\n",
      "dev loss: 1.7423094127065606\n",
      "\n",
      "Train Epoch 596\n",
      "-------------------------------\n",
      "loss: 0.547387  [  260/ 2430]\n",
      "Test Epoch 596\n",
      "-------------------------------\n",
      "loss tensor(0.8079, dtype=torch.float64)\n",
      "loss tensor(0.9317, dtype=torch.float64)\n",
      "dev loss: 1.7395699822288293\n",
      "\n",
      "Train Epoch 597\n",
      "-------------------------------\n",
      "loss: 0.496268  [  260/ 2430]\n",
      "Test Epoch 597\n",
      "-------------------------------\n",
      "loss tensor(0.8218, dtype=torch.float64)\n",
      "loss tensor(0.9575, dtype=torch.float64)\n",
      "dev loss: 1.7793673225052449\n",
      "\n",
      "Train Epoch 598\n",
      "-------------------------------\n",
      "loss: 0.411343  [  260/ 2430]\n",
      "Test Epoch 598\n",
      "-------------------------------\n",
      "loss tensor(0.8053, dtype=torch.float64)\n",
      "loss tensor(0.9411, dtype=torch.float64)\n",
      "dev loss: 1.746346962308475\n",
      "\n",
      "Train Epoch 599\n",
      "-------------------------------\n",
      "loss: 0.510418  [  260/ 2430]\n",
      "Test Epoch 599\n",
      "-------------------------------\n",
      "loss tensor(0.8771, dtype=torch.float64)\n",
      "loss tensor(0.7630, dtype=torch.float64)\n",
      "dev loss: 1.6401382991479305\n",
      "\n",
      "Train Epoch 600\n",
      "-------------------------------\n",
      "loss: 0.484676  [  260/ 2430]\n",
      "Test Epoch 600\n",
      "-------------------------------\n",
      "loss tensor(0.8058, dtype=torch.float64)\n",
      "loss tensor(1.1253, dtype=torch.float64)\n",
      "dev loss: 1.9311144709583004\n",
      "\n",
      "Train Epoch 601\n",
      "-------------------------------\n",
      "loss: 0.508798  [  260/ 2430]\n",
      "Test Epoch 601\n",
      "-------------------------------\n",
      "loss tensor(0.8165, dtype=torch.float64)\n",
      "loss tensor(0.8492, dtype=torch.float64)\n",
      "dev loss: 1.6656698025545442\n",
      "\n",
      "Train Epoch 602\n",
      "-------------------------------\n",
      "loss: 0.485071  [  260/ 2430]\n",
      "Test Epoch 602\n",
      "-------------------------------\n",
      "loss tensor(0.7755, dtype=torch.float64)\n",
      "loss tensor(0.7374, dtype=torch.float64)\n",
      "dev loss: 1.5128212711590772\n",
      "\n",
      "Train Epoch 603\n",
      "-------------------------------\n",
      "loss: 0.468931  [  260/ 2430]\n",
      "Test Epoch 603\n",
      "-------------------------------\n",
      "loss tensor(0.8128, dtype=torch.float64)\n",
      "loss tensor(1.4131, dtype=torch.float64)\n",
      "dev loss: 2.225973942205054\n",
      "\n",
      "Train Epoch 604\n",
      "-------------------------------\n",
      "loss: 0.465070  [  260/ 2430]\n",
      "Test Epoch 604\n",
      "-------------------------------\n",
      "loss tensor(0.8120, dtype=torch.float64)\n",
      "loss tensor(0.8300, dtype=torch.float64)\n",
      "dev loss: 1.6419372935511818\n",
      "\n",
      "Train Epoch 605\n",
      "-------------------------------\n",
      "loss: 0.430804  [  260/ 2430]\n",
      "Test Epoch 605\n",
      "-------------------------------\n",
      "loss tensor(0.7762, dtype=torch.float64)\n",
      "loss tensor(0.9540, dtype=torch.float64)\n",
      "dev loss: 1.730225247254844\n",
      "\n",
      "Train Epoch 606\n",
      "-------------------------------\n",
      "loss: 0.420713  [  260/ 2430]\n",
      "Test Epoch 606\n",
      "-------------------------------\n",
      "loss tensor(0.8217, dtype=torch.float64)\n",
      "loss tensor(0.9366, dtype=torch.float64)\n",
      "dev loss: 1.7583374467713\n",
      "\n",
      "Train Epoch 607\n",
      "-------------------------------\n",
      "loss: 0.510037  [  260/ 2430]\n",
      "Test Epoch 607\n",
      "-------------------------------\n",
      "loss tensor(0.7750, dtype=torch.float64)\n",
      "loss tensor(0.9292, dtype=torch.float64)\n",
      "dev loss: 1.704180948972109\n",
      "\n",
      "Train Epoch 608\n",
      "-------------------------------\n",
      "loss: 0.542549  [  260/ 2430]\n",
      "Test Epoch 608\n",
      "-------------------------------\n",
      "loss tensor(0.8544, dtype=torch.float64)\n",
      "loss tensor(1.0219, dtype=torch.float64)\n",
      "dev loss: 1.876295246281313\n",
      "\n",
      "Train Epoch 609\n",
      "-------------------------------\n",
      "loss: 0.457642  [  260/ 2430]\n",
      "Test Epoch 609\n",
      "-------------------------------\n",
      "loss tensor(0.8448, dtype=torch.float64)\n",
      "loss tensor(0.9811, dtype=torch.float64)\n",
      "dev loss: 1.825887496589084\n",
      "\n",
      "Train Epoch 610\n",
      "-------------------------------\n",
      "loss: 0.486791  [  260/ 2430]\n",
      "Test Epoch 610\n",
      "-------------------------------\n",
      "loss tensor(0.8222, dtype=torch.float64)\n",
      "loss tensor(1.1491, dtype=torch.float64)\n",
      "dev loss: 1.9712458468656648\n",
      "\n",
      "Train Epoch 611\n",
      "-------------------------------\n",
      "loss: 0.569510  [  260/ 2430]\n",
      "Test Epoch 611\n",
      "-------------------------------\n",
      "loss tensor(0.7881, dtype=torch.float64)\n",
      "loss tensor(0.9469, dtype=torch.float64)\n",
      "dev loss: 1.7349681422554157\n",
      "\n",
      "Train Epoch 612\n",
      "-------------------------------\n",
      "loss: 0.429748  [  260/ 2430]\n",
      "Test Epoch 612\n",
      "-------------------------------\n",
      "loss tensor(0.7621, dtype=torch.float64)\n",
      "loss tensor(0.8084, dtype=torch.float64)\n",
      "dev loss: 1.5704176537156953\n",
      "\n",
      "Train Epoch 613\n",
      "-------------------------------\n",
      "loss: 0.479908  [  260/ 2430]\n",
      "Test Epoch 613\n",
      "-------------------------------\n",
      "loss tensor(0.8390, dtype=torch.float64)\n",
      "loss tensor(1.0378, dtype=torch.float64)\n",
      "dev loss: 1.8767550792611671\n",
      "\n",
      "Train Epoch 614\n",
      "-------------------------------\n",
      "loss: 0.501324  [  260/ 2430]\n",
      "Test Epoch 614\n",
      "-------------------------------\n",
      "loss tensor(0.8897, dtype=torch.float64)\n",
      "loss tensor(1.0086, dtype=torch.float64)\n",
      "dev loss: 1.8983238152402735\n",
      "\n",
      "Train Epoch 615\n",
      "-------------------------------\n",
      "loss: 0.418831  [  260/ 2430]\n",
      "Test Epoch 615\n",
      "-------------------------------\n",
      "loss tensor(0.8709, dtype=torch.float64)\n",
      "loss tensor(1.0287, dtype=torch.float64)\n",
      "dev loss: 1.8996167047262045\n",
      "\n",
      "Train Epoch 616\n",
      "-------------------------------\n",
      "loss: 0.605726  [  260/ 2430]\n",
      "Test Epoch 616\n",
      "-------------------------------\n",
      "loss tensor(0.8705, dtype=torch.float64)\n",
      "loss tensor(1.3139, dtype=torch.float64)\n",
      "dev loss: 2.1844197429658054\n",
      "\n",
      "Train Epoch 617\n",
      "-------------------------------\n",
      "loss: 0.467551  [  260/ 2430]\n",
      "Test Epoch 617\n",
      "-------------------------------\n",
      "loss tensor(0.8409, dtype=torch.float64)\n",
      "loss tensor(0.7804, dtype=torch.float64)\n",
      "dev loss: 1.6212346449907589\n",
      "\n",
      "Train Epoch 618\n",
      "-------------------------------\n",
      "loss: 0.409228  [  260/ 2430]\n",
      "Test Epoch 618\n",
      "-------------------------------\n",
      "loss tensor(0.8243, dtype=torch.float64)\n",
      "loss tensor(1.1306, dtype=torch.float64)\n",
      "dev loss: 1.9549488180056391\n",
      "\n",
      "Train Epoch 619\n",
      "-------------------------------\n",
      "loss: 0.453078  [  260/ 2430]\n",
      "Test Epoch 619\n",
      "-------------------------------\n",
      "loss tensor(0.8603, dtype=torch.float64)\n",
      "loss tensor(0.9491, dtype=torch.float64)\n",
      "dev loss: 1.8094076799105532\n",
      "\n",
      "Train Epoch 620\n",
      "-------------------------------\n",
      "loss: 0.514398  [  260/ 2430]\n",
      "Test Epoch 620\n",
      "-------------------------------\n",
      "loss tensor(0.8204, dtype=torch.float64)\n",
      "loss tensor(0.8758, dtype=torch.float64)\n",
      "dev loss: 1.6961629539269256\n",
      "\n",
      "Train Epoch 621\n",
      "-------------------------------\n",
      "loss: 0.379322  [  260/ 2430]\n",
      "Test Epoch 621\n",
      "-------------------------------\n",
      "loss tensor(0.8310, dtype=torch.float64)\n",
      "loss tensor(1.0263, dtype=torch.float64)\n",
      "dev loss: 1.857365956023443\n",
      "\n",
      "Train Epoch 622\n",
      "-------------------------------\n",
      "loss: 0.457862  [  260/ 2430]\n",
      "Test Epoch 622\n",
      "-------------------------------\n",
      "loss tensor(0.8722, dtype=torch.float64)\n",
      "loss tensor(0.9872, dtype=torch.float64)\n",
      "dev loss: 1.859365661497446\n",
      "\n",
      "Train Epoch 623\n",
      "-------------------------------\n",
      "loss: 0.474607  [  260/ 2430]\n",
      "Test Epoch 623\n",
      "-------------------------------\n",
      "loss tensor(0.7886, dtype=torch.float64)\n",
      "loss tensor(0.9636, dtype=torch.float64)\n",
      "dev loss: 1.7522699406331688\n",
      "\n",
      "Train Epoch 624\n",
      "-------------------------------\n",
      "loss: 0.484369  [  260/ 2430]\n",
      "Test Epoch 624\n",
      "-------------------------------\n",
      "loss tensor(0.8142, dtype=torch.float64)\n",
      "loss tensor(0.8610, dtype=torch.float64)\n",
      "dev loss: 1.675244801768233\n",
      "\n",
      "Train Epoch 625\n",
      "-------------------------------\n",
      "loss: 0.391203  [  260/ 2430]\n",
      "Test Epoch 625\n",
      "-------------------------------\n",
      "loss tensor(0.9321, dtype=torch.float64)\n",
      "loss tensor(1.0006, dtype=torch.float64)\n",
      "dev loss: 1.9326156423097285\n",
      "\n",
      "Train Epoch 626\n",
      "-------------------------------\n",
      "loss: 0.525101  [  260/ 2430]\n",
      "Test Epoch 626\n",
      "-------------------------------\n",
      "loss tensor(0.7944, dtype=torch.float64)\n",
      "loss tensor(1.0174, dtype=torch.float64)\n",
      "dev loss: 1.81179566960537\n",
      "\n",
      "Train Epoch 627\n",
      "-------------------------------\n",
      "loss: 0.434200  [  260/ 2430]\n",
      "Test Epoch 627\n",
      "-------------------------------\n",
      "loss tensor(0.8405, dtype=torch.float64)\n",
      "loss tensor(1.1084, dtype=torch.float64)\n",
      "dev loss: 1.9488607501628177\n",
      "\n",
      "Train Epoch 628\n",
      "-------------------------------\n",
      "loss: 0.493440  [  260/ 2430]\n",
      "Test Epoch 628\n",
      "-------------------------------\n",
      "loss tensor(0.7894, dtype=torch.float64)\n",
      "loss tensor(0.8955, dtype=torch.float64)\n",
      "dev loss: 1.6848980990400162\n",
      "\n",
      "Train Epoch 629\n",
      "-------------------------------\n",
      "loss: 0.465641  [  260/ 2430]\n",
      "Test Epoch 629\n",
      "-------------------------------\n",
      "loss tensor(0.8464, dtype=torch.float64)\n",
      "loss tensor(1.0552, dtype=torch.float64)\n",
      "dev loss: 1.9015847550889728\n",
      "\n",
      "Train Epoch 630\n",
      "-------------------------------\n",
      "loss: 0.402037  [  260/ 2430]\n",
      "Test Epoch 630\n",
      "-------------------------------\n",
      "loss tensor(0.9014, dtype=torch.float64)\n",
      "loss tensor(0.7725, dtype=torch.float64)\n",
      "dev loss: 1.6739206374302031\n",
      "\n",
      "Train Epoch 631\n",
      "-------------------------------\n",
      "loss: 0.551021  [  260/ 2430]\n",
      "Test Epoch 631\n",
      "-------------------------------\n",
      "loss tensor(0.7955, dtype=torch.float64)\n",
      "loss tensor(1.0317, dtype=torch.float64)\n",
      "dev loss: 1.8272007125389171\n",
      "\n",
      "Train Epoch 632\n",
      "-------------------------------\n",
      "loss: 0.504569  [  260/ 2430]\n",
      "Test Epoch 632\n",
      "-------------------------------\n",
      "loss tensor(0.7942, dtype=torch.float64)\n",
      "loss tensor(0.8281, dtype=torch.float64)\n",
      "dev loss: 1.6222985765325015\n",
      "\n",
      "Train Epoch 633\n",
      "-------------------------------\n",
      "loss: 0.413951  [  260/ 2430]\n",
      "Test Epoch 633\n",
      "-------------------------------\n",
      "loss tensor(0.8771, dtype=torch.float64)\n",
      "loss tensor(0.8330, dtype=torch.float64)\n",
      "dev loss: 1.7100966063755563\n",
      "\n",
      "Train Epoch 634\n",
      "-------------------------------\n",
      "loss: 0.476768  [  260/ 2430]\n",
      "Test Epoch 634\n",
      "-------------------------------\n",
      "loss tensor(0.8067, dtype=torch.float64)\n",
      "loss tensor(1.0278, dtype=torch.float64)\n",
      "dev loss: 1.8345058444127536\n",
      "\n",
      "Train Epoch 635\n",
      "-------------------------------\n",
      "loss: 0.432255  [  260/ 2430]\n",
      "Test Epoch 635\n",
      "-------------------------------\n",
      "loss tensor(0.7907, dtype=torch.float64)\n",
      "loss tensor(0.9838, dtype=torch.float64)\n",
      "dev loss: 1.7744242481177226\n",
      "\n",
      "Train Epoch 636\n",
      "-------------------------------\n",
      "loss: 0.447652  [  260/ 2430]\n",
      "Test Epoch 636\n",
      "-------------------------------\n",
      "loss tensor(0.8615, dtype=torch.float64)\n",
      "loss tensor(0.9881, dtype=torch.float64)\n",
      "dev loss: 1.8495359268191267\n",
      "\n",
      "Train Epoch 637\n",
      "-------------------------------\n",
      "loss: 0.585527  [  260/ 2430]\n",
      "Test Epoch 637\n",
      "-------------------------------\n",
      "loss tensor(0.8030, dtype=torch.float64)\n",
      "loss tensor(0.7742, dtype=torch.float64)\n",
      "dev loss: 1.5772513812453761\n",
      "\n",
      "Train Epoch 638\n",
      "-------------------------------\n",
      "loss: 0.489305  [  260/ 2430]\n",
      "Test Epoch 638\n",
      "-------------------------------\n",
      "loss tensor(0.7902, dtype=torch.float64)\n",
      "loss tensor(0.8985, dtype=torch.float64)\n",
      "dev loss: 1.688706377336664\n",
      "\n",
      "Train Epoch 639\n",
      "-------------------------------\n",
      "loss: 0.441312  [  260/ 2430]\n",
      "Test Epoch 639\n",
      "-------------------------------\n",
      "loss tensor(0.7819, dtype=torch.float64)\n",
      "loss tensor(1.0007, dtype=torch.float64)\n",
      "dev loss: 1.782614530712303\n",
      "\n",
      "Train Epoch 640\n",
      "-------------------------------\n",
      "loss: 0.536306  [  260/ 2430]\n",
      "Test Epoch 640\n",
      "-------------------------------\n",
      "loss tensor(0.7980, dtype=torch.float64)\n",
      "loss tensor(0.9256, dtype=torch.float64)\n",
      "dev loss: 1.7236481519757876\n",
      "\n",
      "Train Epoch 641\n",
      "-------------------------------\n",
      "loss: 0.491270  [  260/ 2430]\n",
      "Test Epoch 641\n",
      "-------------------------------\n",
      "loss tensor(0.8034, dtype=torch.float64)\n",
      "loss tensor(0.8901, dtype=torch.float64)\n",
      "dev loss: 1.693501213858812\n",
      "\n",
      "Train Epoch 642\n",
      "-------------------------------\n",
      "loss: 0.399780  [  260/ 2430]\n",
      "Test Epoch 642\n",
      "-------------------------------\n",
      "loss tensor(0.8569, dtype=torch.float64)\n",
      "loss tensor(0.8885, dtype=torch.float64)\n",
      "dev loss: 1.7454731825490406\n",
      "\n",
      "Train Epoch 643\n",
      "-------------------------------\n",
      "loss: 0.390355  [  260/ 2430]\n",
      "Test Epoch 643\n",
      "-------------------------------\n",
      "loss tensor(0.8832, dtype=torch.float64)\n",
      "loss tensor(0.9559, dtype=torch.float64)\n",
      "dev loss: 1.8391125580722654\n",
      "\n",
      "Train Epoch 644\n",
      "-------------------------------\n",
      "loss: 0.437793  [  260/ 2430]\n",
      "Test Epoch 644\n",
      "-------------------------------\n",
      "loss tensor(0.8515, dtype=torch.float64)\n",
      "loss tensor(0.8370, dtype=torch.float64)\n",
      "dev loss: 1.6884900405587628\n",
      "\n",
      "Train Epoch 645\n",
      "-------------------------------\n",
      "loss: 0.486930  [  260/ 2430]\n",
      "Test Epoch 645\n",
      "-------------------------------\n",
      "loss tensor(0.8240, dtype=torch.float64)\n",
      "loss tensor(1.2448, dtype=torch.float64)\n",
      "dev loss: 2.0687406877299237\n",
      "\n",
      "Train Epoch 646\n",
      "-------------------------------\n",
      "loss: 0.451459  [  260/ 2430]\n",
      "Test Epoch 646\n",
      "-------------------------------\n",
      "loss tensor(0.8140, dtype=torch.float64)\n",
      "loss tensor(0.8863, dtype=torch.float64)\n",
      "dev loss: 1.70030189831496\n",
      "\n",
      "Train Epoch 647\n",
      "-------------------------------\n",
      "loss: 0.412685  [  260/ 2430]\n",
      "Test Epoch 647\n",
      "-------------------------------\n",
      "loss tensor(0.7908, dtype=torch.float64)\n",
      "loss tensor(0.9513, dtype=torch.float64)\n",
      "dev loss: 1.742093600691057\n",
      "\n",
      "Train Epoch 648\n",
      "-------------------------------\n",
      "loss: 0.455626  [  260/ 2430]\n",
      "Test Epoch 648\n",
      "-------------------------------\n",
      "loss tensor(0.7908, dtype=torch.float64)\n",
      "loss tensor(0.9917, dtype=torch.float64)\n",
      "dev loss: 1.7824970237762257\n",
      "\n",
      "Train Epoch 649\n",
      "-------------------------------\n",
      "loss: 0.415576  [  260/ 2430]\n",
      "Test Epoch 649\n",
      "-------------------------------\n",
      "loss tensor(0.8104, dtype=torch.float64)\n",
      "loss tensor(1.1788, dtype=torch.float64)\n",
      "dev loss: 1.9892220387197863\n",
      "\n",
      "Train Epoch 650\n",
      "-------------------------------\n",
      "loss: 0.417431  [  260/ 2430]\n",
      "Test Epoch 650\n",
      "-------------------------------\n",
      "loss tensor(0.8215, dtype=torch.float64)\n",
      "loss tensor(0.8238, dtype=torch.float64)\n",
      "dev loss: 1.645326876011954\n",
      "\n",
      "Train Epoch 651\n",
      "-------------------------------\n",
      "loss: 0.344221  [  260/ 2430]\n",
      "Test Epoch 651\n",
      "-------------------------------\n",
      "loss tensor(0.8215, dtype=torch.float64)\n",
      "loss tensor(0.9843, dtype=torch.float64)\n",
      "dev loss: 1.8057705627381462\n",
      "\n",
      "Train Epoch 652\n",
      "-------------------------------\n",
      "loss: 0.539660  [  260/ 2430]\n",
      "Test Epoch 652\n",
      "-------------------------------\n",
      "loss tensor(0.8459, dtype=torch.float64)\n",
      "loss tensor(1.0662, dtype=torch.float64)\n",
      "dev loss: 1.9121298195815957\n",
      "\n",
      "Train Epoch 653\n",
      "-------------------------------\n",
      "loss: 0.457203  [  260/ 2430]\n",
      "Test Epoch 653\n",
      "-------------------------------\n",
      "loss tensor(0.8112, dtype=torch.float64)\n",
      "loss tensor(0.7885, dtype=torch.float64)\n",
      "dev loss: 1.5997008290446302\n",
      "\n",
      "Train Epoch 654\n",
      "-------------------------------\n",
      "loss: 0.416131  [  260/ 2430]\n",
      "Test Epoch 654\n",
      "-------------------------------\n",
      "loss tensor(0.7725, dtype=torch.float64)\n",
      "loss tensor(1.0160, dtype=torch.float64)\n",
      "dev loss: 1.7884994213897156\n",
      "\n",
      "Train Epoch 655\n",
      "-------------------------------\n",
      "loss: 0.388418  [  260/ 2430]\n",
      "Test Epoch 655\n",
      "-------------------------------\n",
      "loss tensor(0.8140, dtype=torch.float64)\n",
      "loss tensor(0.7957, dtype=torch.float64)\n",
      "dev loss: 1.6096546497096544\n",
      "\n",
      "Train Epoch 656\n",
      "-------------------------------\n",
      "loss: 0.454974  [  260/ 2430]\n",
      "Test Epoch 656\n",
      "-------------------------------\n",
      "loss tensor(0.8255, dtype=torch.float64)\n",
      "loss tensor(1.0909, dtype=torch.float64)\n",
      "dev loss: 1.9163699200715465\n",
      "\n",
      "Train Epoch 657\n",
      "-------------------------------\n",
      "loss: 0.452983  [  260/ 2430]\n",
      "Test Epoch 657\n",
      "-------------------------------\n",
      "loss tensor(0.8056, dtype=torch.float64)\n",
      "loss tensor(0.7945, dtype=torch.float64)\n",
      "dev loss: 1.6001495296979111\n",
      "\n",
      "Train Epoch 658\n",
      "-------------------------------\n",
      "loss: 0.446598  [  260/ 2430]\n",
      "Test Epoch 658\n",
      "-------------------------------\n",
      "loss tensor(0.7851, dtype=torch.float64)\n",
      "loss tensor(0.9518, dtype=torch.float64)\n",
      "dev loss: 1.736866848850935\n",
      "\n",
      "Train Epoch 659\n",
      "-------------------------------\n",
      "loss: 0.425859  [  260/ 2430]\n",
      "Test Epoch 659\n",
      "-------------------------------\n",
      "loss tensor(0.8544, dtype=torch.float64)\n",
      "loss tensor(0.7623, dtype=torch.float64)\n",
      "dev loss: 1.6166837776095275\n",
      "\n",
      "Train Epoch 660\n",
      "-------------------------------\n",
      "loss: 0.396853  [  260/ 2430]\n",
      "Test Epoch 660\n",
      "-------------------------------\n",
      "loss tensor(0.8226, dtype=torch.float64)\n",
      "loss tensor(1.0307, dtype=torch.float64)\n",
      "dev loss: 1.853304234597564\n",
      "\n",
      "Train Epoch 661\n",
      "-------------------------------\n",
      "loss: 0.467648  [  260/ 2430]\n",
      "Test Epoch 661\n",
      "-------------------------------\n",
      "loss tensor(0.8004, dtype=torch.float64)\n",
      "loss tensor(0.9351, dtype=torch.float64)\n",
      "dev loss: 1.735470619853669\n",
      "\n",
      "Train Epoch 662\n",
      "-------------------------------\n",
      "loss: 0.466555  [  260/ 2430]\n",
      "Test Epoch 662\n",
      "-------------------------------\n",
      "loss tensor(0.7953, dtype=torch.float64)\n",
      "loss tensor(0.8559, dtype=torch.float64)\n",
      "dev loss: 1.6511859379944283\n",
      "\n",
      "Train Epoch 663\n",
      "-------------------------------\n",
      "loss: 0.382551  [  260/ 2430]\n",
      "Test Epoch 663\n",
      "-------------------------------\n",
      "loss tensor(0.8294, dtype=torch.float64)\n",
      "loss tensor(0.8606, dtype=torch.float64)\n",
      "dev loss: 1.6899912424716494\n",
      "\n",
      "Train Epoch 664\n",
      "-------------------------------\n",
      "loss: 0.502329  [  260/ 2430]\n",
      "Test Epoch 664\n",
      "-------------------------------\n",
      "loss tensor(0.8321, dtype=torch.float64)\n",
      "loss tensor(1.0248, dtype=torch.float64)\n",
      "dev loss: 1.8569214401351504\n",
      "\n",
      "Train Epoch 665\n",
      "-------------------------------\n",
      "loss: 0.433994  [  260/ 2430]\n",
      "Test Epoch 665\n",
      "-------------------------------\n",
      "loss tensor(0.7786, dtype=torch.float64)\n",
      "loss tensor(0.9083, dtype=torch.float64)\n",
      "dev loss: 1.686919228549765\n",
      "\n",
      "Train Epoch 666\n",
      "-------------------------------\n",
      "loss: 0.435242  [  260/ 2430]\n",
      "Test Epoch 666\n",
      "-------------------------------\n",
      "loss tensor(0.8616, dtype=torch.float64)\n",
      "loss tensor(1.0108, dtype=torch.float64)\n",
      "dev loss: 1.8724240319240333\n",
      "\n",
      "Train Epoch 667\n",
      "-------------------------------\n",
      "loss: 0.360718  [  260/ 2430]\n",
      "Test Epoch 667\n",
      "-------------------------------\n",
      "loss tensor(0.8323, dtype=torch.float64)\n",
      "loss tensor(1.0789, dtype=torch.float64)\n",
      "dev loss: 1.9111811028808088\n",
      "\n",
      "Train Epoch 668\n",
      "-------------------------------\n",
      "loss: 0.414698  [  260/ 2430]\n",
      "Test Epoch 668\n",
      "-------------------------------\n",
      "loss tensor(0.8100, dtype=torch.float64)\n",
      "loss tensor(0.6819, dtype=torch.float64)\n",
      "dev loss: 1.4918701466288091\n",
      "\n",
      "Train Epoch 669\n",
      "-------------------------------\n",
      "loss: 0.420920  [  260/ 2430]\n",
      "Test Epoch 669\n",
      "-------------------------------\n",
      "loss tensor(0.7978, dtype=torch.float64)\n",
      "loss tensor(0.9816, dtype=torch.float64)\n",
      "dev loss: 1.7793462779615918\n",
      "\n",
      "Train Epoch 670\n",
      "-------------------------------\n",
      "loss: 0.422593  [  260/ 2430]\n",
      "Test Epoch 670\n",
      "-------------------------------\n",
      "loss tensor(0.8478, dtype=torch.float64)\n",
      "loss tensor(0.8462, dtype=torch.float64)\n",
      "dev loss: 1.6939414612485826\n",
      "\n",
      "Train Epoch 671\n",
      "-------------------------------\n",
      "loss: 0.427964  [  260/ 2430]\n",
      "Test Epoch 671\n",
      "-------------------------------\n",
      "loss tensor(0.8688, dtype=torch.float64)\n",
      "loss tensor(0.9785, dtype=torch.float64)\n",
      "dev loss: 1.8472601311373211\n",
      "\n",
      "Train Epoch 672\n",
      "-------------------------------\n",
      "loss: 0.557404  [  260/ 2430]\n",
      "Test Epoch 672\n",
      "-------------------------------\n",
      "loss tensor(0.8313, dtype=torch.float64)\n",
      "loss tensor(1.0274, dtype=torch.float64)\n",
      "dev loss: 1.858636098225781\n",
      "\n",
      "Train Epoch 673\n",
      "-------------------------------\n",
      "loss: 0.536995  [  260/ 2430]\n",
      "Test Epoch 673\n",
      "-------------------------------\n",
      "loss tensor(0.8128, dtype=torch.float64)\n",
      "loss tensor(1.0383, dtype=torch.float64)\n",
      "dev loss: 1.8511912290962633\n",
      "\n",
      "Train Epoch 674\n",
      "-------------------------------\n",
      "loss: 0.367323  [  260/ 2430]\n",
      "Test Epoch 674\n",
      "-------------------------------\n",
      "loss tensor(0.7963, dtype=torch.float64)\n",
      "loss tensor(0.8279, dtype=torch.float64)\n",
      "dev loss: 1.6242020365299257\n",
      "\n",
      "Train Epoch 675\n",
      "-------------------------------\n",
      "loss: 0.514437  [  260/ 2430]\n",
      "Test Epoch 675\n",
      "-------------------------------\n",
      "loss tensor(0.7897, dtype=torch.float64)\n",
      "loss tensor(1.4104, dtype=torch.float64)\n",
      "dev loss: 2.200071356355001\n",
      "\n",
      "Train Epoch 676\n",
      "-------------------------------\n",
      "loss: 0.454379  [  260/ 2430]\n",
      "Test Epoch 676\n",
      "-------------------------------\n",
      "loss tensor(0.8282, dtype=torch.float64)\n",
      "loss tensor(0.7762, dtype=torch.float64)\n",
      "dev loss: 1.604355056098913\n",
      "\n",
      "Train Epoch 677\n",
      "-------------------------------\n",
      "loss: 0.438816  [  260/ 2430]\n",
      "Test Epoch 677\n",
      "-------------------------------\n",
      "loss tensor(0.8043, dtype=torch.float64)\n",
      "loss tensor(1.2257, dtype=torch.float64)\n",
      "dev loss: 2.0299567636599782\n",
      "\n",
      "Train Epoch 678\n",
      "-------------------------------\n",
      "loss: 0.370932  [  260/ 2430]\n",
      "Test Epoch 678\n",
      "-------------------------------\n",
      "loss tensor(0.8088, dtype=torch.float64)\n",
      "loss tensor(1.0033, dtype=torch.float64)\n",
      "dev loss: 1.8121215897135592\n",
      "\n",
      "Train Epoch 679\n",
      "-------------------------------\n",
      "loss: 0.408338  [  260/ 2430]\n",
      "Test Epoch 679\n",
      "-------------------------------\n",
      "loss tensor(0.9148, dtype=torch.float64)\n",
      "loss tensor(0.8203, dtype=torch.float64)\n",
      "dev loss: 1.7350957862919727\n",
      "\n",
      "Train Epoch 680\n",
      "-------------------------------\n",
      "loss: 0.486166  [  260/ 2430]\n",
      "Test Epoch 680\n",
      "-------------------------------\n",
      "loss tensor(0.8483, dtype=torch.float64)\n",
      "loss tensor(1.2884, dtype=torch.float64)\n",
      "dev loss: 2.13670029249934\n",
      "\n",
      "Train Epoch 681\n",
      "-------------------------------\n",
      "loss: 0.464991  [  260/ 2430]\n",
      "Test Epoch 681\n",
      "-------------------------------\n",
      "loss tensor(0.8143, dtype=torch.float64)\n",
      "loss tensor(0.8919, dtype=torch.float64)\n",
      "dev loss: 1.7062435181557518\n",
      "\n",
      "Train Epoch 682\n",
      "-------------------------------\n",
      "loss: 0.455090  [  260/ 2430]\n",
      "Test Epoch 682\n",
      "-------------------------------\n",
      "loss tensor(0.7757, dtype=torch.float64)\n",
      "loss tensor(0.6777, dtype=torch.float64)\n",
      "dev loss: 1.4533887243543013\n",
      "\n",
      "Train Epoch 683\n",
      "-------------------------------\n",
      "loss: 0.379667  [  260/ 2430]\n",
      "Test Epoch 683\n",
      "-------------------------------\n",
      "loss tensor(0.8005, dtype=torch.float64)\n",
      "loss tensor(1.0701, dtype=torch.float64)\n",
      "dev loss: 1.8706094624247376\n",
      "\n",
      "Train Epoch 684\n",
      "-------------------------------\n",
      "loss: 0.474224  [  260/ 2430]\n",
      "Test Epoch 684\n",
      "-------------------------------\n",
      "loss tensor(0.8115, dtype=torch.float64)\n",
      "loss tensor(0.7506, dtype=torch.float64)\n",
      "dev loss: 1.5620762690095937\n",
      "\n",
      "Train Epoch 685\n",
      "-------------------------------\n",
      "loss: 0.367696  [  260/ 2430]\n",
      "Test Epoch 685\n",
      "-------------------------------\n",
      "loss tensor(0.8780, dtype=torch.float64)\n",
      "loss tensor(1.0403, dtype=torch.float64)\n",
      "dev loss: 1.9182998173303933\n",
      "\n",
      "Train Epoch 686\n",
      "-------------------------------\n",
      "loss: 0.501032  [  260/ 2430]\n",
      "Test Epoch 686\n",
      "-------------------------------\n",
      "loss tensor(0.8743, dtype=torch.float64)\n",
      "loss tensor(0.9761, dtype=torch.float64)\n",
      "dev loss: 1.8504122159968306\n",
      "\n",
      "Train Epoch 687\n",
      "-------------------------------\n",
      "loss: 0.445692  [  260/ 2430]\n",
      "Test Epoch 687\n",
      "-------------------------------\n",
      "loss tensor(0.8552, dtype=torch.float64)\n",
      "loss tensor(0.8233, dtype=torch.float64)\n",
      "dev loss: 1.6784371518628447\n",
      "\n",
      "Train Epoch 688\n",
      "-------------------------------\n",
      "loss: 0.529602  [  260/ 2430]\n",
      "Test Epoch 688\n",
      "-------------------------------\n",
      "loss tensor(0.8390, dtype=torch.float64)\n",
      "loss tensor(0.9050, dtype=torch.float64)\n",
      "dev loss: 1.7439831254859364\n",
      "\n",
      "Train Epoch 689\n",
      "-------------------------------\n",
      "loss: 0.465122  [  260/ 2430]\n",
      "Test Epoch 689\n",
      "-------------------------------\n",
      "loss tensor(0.7684, dtype=torch.float64)\n",
      "loss tensor(0.9733, dtype=torch.float64)\n",
      "dev loss: 1.741645689702921\n",
      "\n",
      "Train Epoch 690\n",
      "-------------------------------\n",
      "loss: 0.388377  [  260/ 2430]\n",
      "Test Epoch 690\n",
      "-------------------------------\n",
      "loss tensor(0.7891, dtype=torch.float64)\n",
      "loss tensor(1.0064, dtype=torch.float64)\n",
      "dev loss: 1.795483717998314\n",
      "\n",
      "Train Epoch 691\n",
      "-------------------------------\n",
      "loss: 0.479458  [  260/ 2430]\n",
      "Test Epoch 691\n",
      "-------------------------------\n",
      "loss tensor(0.8242, dtype=torch.float64)\n",
      "loss tensor(0.7818, dtype=torch.float64)\n",
      "dev loss: 1.6060220296213947\n",
      "\n",
      "Train Epoch 692\n",
      "-------------------------------\n",
      "loss: 0.486299  [  260/ 2430]\n",
      "Test Epoch 692\n",
      "-------------------------------\n",
      "loss tensor(0.8007, dtype=torch.float64)\n",
      "loss tensor(0.8131, dtype=torch.float64)\n",
      "dev loss: 1.6137892930281166\n",
      "\n",
      "Train Epoch 693\n",
      "-------------------------------\n",
      "loss: 0.338719  [  260/ 2430]\n",
      "Test Epoch 693\n",
      "-------------------------------\n",
      "loss tensor(0.8012, dtype=torch.float64)\n",
      "loss tensor(0.9649, dtype=torch.float64)\n",
      "dev loss: 1.7660903948568896\n",
      "\n",
      "Train Epoch 694\n",
      "-------------------------------\n",
      "loss: 0.476900  [  260/ 2430]\n",
      "Test Epoch 694\n",
      "-------------------------------\n",
      "loss tensor(0.8377, dtype=torch.float64)\n",
      "loss tensor(0.7924, dtype=torch.float64)\n",
      "dev loss: 1.6300690264962796\n",
      "\n",
      "Train Epoch 695\n",
      "-------------------------------\n",
      "loss: 0.381470  [  260/ 2430]\n",
      "Test Epoch 695\n",
      "-------------------------------\n",
      "loss tensor(0.8778, dtype=torch.float64)\n",
      "loss tensor(1.3487, dtype=torch.float64)\n",
      "dev loss: 2.2265102135670936\n",
      "\n",
      "Train Epoch 696\n",
      "-------------------------------\n",
      "loss: 0.459747  [  260/ 2430]\n",
      "Test Epoch 696\n",
      "-------------------------------\n",
      "loss tensor(0.7913, dtype=torch.float64)\n",
      "loss tensor(0.8598, dtype=torch.float64)\n",
      "dev loss: 1.6511274466938883\n",
      "\n",
      "Train Epoch 697\n",
      "-------------------------------\n",
      "loss: 0.402137  [  260/ 2430]\n",
      "Test Epoch 697\n",
      "-------------------------------\n",
      "loss tensor(0.8197, dtype=torch.float64)\n",
      "loss tensor(1.0236, dtype=torch.float64)\n",
      "dev loss: 1.843282125301409\n",
      "\n",
      "Train Epoch 698\n",
      "-------------------------------\n",
      "loss: 0.362141  [  260/ 2430]\n",
      "Test Epoch 698\n",
      "-------------------------------\n",
      "loss tensor(0.8699, dtype=torch.float64)\n",
      "loss tensor(1.0482, dtype=torch.float64)\n",
      "dev loss: 1.918043624938512\n",
      "\n",
      "Train Epoch 699\n",
      "-------------------------------\n",
      "loss: 0.459680  [  260/ 2430]\n",
      "Test Epoch 699\n",
      "-------------------------------\n",
      "loss tensor(0.8195, dtype=torch.float64)\n",
      "loss tensor(0.8014, dtype=torch.float64)\n",
      "dev loss: 1.620879294073212\n",
      "\n",
      "Train Epoch 700\n",
      "-------------------------------\n",
      "loss: 0.438539  [  260/ 2430]\n",
      "Test Epoch 700\n",
      "-------------------------------\n",
      "loss tensor(0.7707, dtype=torch.float64)\n",
      "loss tensor(1.0049, dtype=torch.float64)\n",
      "dev loss: 1.775630544762608\n",
      "\n",
      "Train Epoch 701\n",
      "-------------------------------\n",
      "loss: 0.475211  [  260/ 2430]\n",
      "Test Epoch 701\n",
      "-------------------------------\n",
      "loss tensor(0.7884, dtype=torch.float64)\n",
      "loss tensor(0.8892, dtype=torch.float64)\n",
      "dev loss: 1.6776542406314952\n",
      "\n",
      "Train Epoch 702\n",
      "-------------------------------\n",
      "loss: 0.403896  [  260/ 2430]\n",
      "Test Epoch 702\n",
      "-------------------------------\n",
      "loss tensor(0.8281, dtype=torch.float64)\n",
      "loss tensor(0.8779, dtype=torch.float64)\n",
      "dev loss: 1.7060452396931873\n",
      "\n",
      "Train Epoch 703\n",
      "-------------------------------\n",
      "loss: 0.383437  [  260/ 2430]\n",
      "Test Epoch 703\n",
      "-------------------------------\n",
      "loss tensor(0.8250, dtype=torch.float64)\n",
      "loss tensor(0.9904, dtype=torch.float64)\n",
      "dev loss: 1.8153501983807963\n",
      "\n",
      "Train Epoch 704\n",
      "-------------------------------\n",
      "loss: 0.465544  [  260/ 2430]\n",
      "Test Epoch 704\n",
      "-------------------------------\n",
      "loss tensor(0.7940, dtype=torch.float64)\n",
      "loss tensor(0.8788, dtype=torch.float64)\n",
      "dev loss: 1.672814908740055\n",
      "\n",
      "Train Epoch 705\n",
      "-------------------------------\n",
      "loss: 0.392440  [  260/ 2430]\n",
      "Test Epoch 705\n",
      "-------------------------------\n",
      "loss tensor(0.7836, dtype=torch.float64)\n",
      "loss tensor(0.9775, dtype=torch.float64)\n",
      "dev loss: 1.76105445002932\n",
      "\n",
      "Train Epoch 706\n",
      "-------------------------------\n",
      "loss: 0.388661  [  260/ 2430]\n",
      "Test Epoch 706\n",
      "-------------------------------\n",
      "loss tensor(0.8497, dtype=torch.float64)\n",
      "loss tensor(1.0635, dtype=torch.float64)\n",
      "dev loss: 1.9131251706244585\n",
      "\n",
      "Train Epoch 707\n",
      "-------------------------------\n",
      "loss: 0.395950  [  260/ 2430]\n",
      "Test Epoch 707\n",
      "-------------------------------\n",
      "loss tensor(0.8323, dtype=torch.float64)\n",
      "loss tensor(1.1276, dtype=torch.float64)\n",
      "dev loss: 1.9598825287515227\n",
      "\n",
      "Train Epoch 708\n",
      "-------------------------------\n",
      "loss: 0.394222  [  260/ 2430]\n",
      "Test Epoch 708\n",
      "-------------------------------\n",
      "loss tensor(0.8496, dtype=torch.float64)\n",
      "loss tensor(0.8082, dtype=torch.float64)\n",
      "dev loss: 1.6577819314874125\n",
      "\n",
      "Train Epoch 709\n",
      "-------------------------------\n",
      "loss: 0.359512  [  260/ 2430]\n",
      "Test Epoch 709\n",
      "-------------------------------\n",
      "loss tensor(0.8678, dtype=torch.float64)\n",
      "loss tensor(1.4569, dtype=torch.float64)\n",
      "dev loss: 2.3247178928194856\n",
      "\n",
      "Train Epoch 710\n",
      "-------------------------------\n",
      "loss: 0.490503  [  260/ 2430]\n",
      "Test Epoch 710\n",
      "-------------------------------\n",
      "loss tensor(0.7914, dtype=torch.float64)\n",
      "loss tensor(0.7515, dtype=torch.float64)\n",
      "dev loss: 1.54293139008215\n",
      "\n",
      "Train Epoch 711\n",
      "-------------------------------\n",
      "loss: 0.491705  [  260/ 2430]\n",
      "Test Epoch 711\n",
      "-------------------------------\n",
      "loss tensor(0.8234, dtype=torch.float64)\n",
      "loss tensor(1.0035, dtype=torch.float64)\n",
      "dev loss: 1.8269982492477213\n",
      "\n",
      "Train Epoch 712\n",
      "-------------------------------\n",
      "loss: 0.414944  [  260/ 2430]\n",
      "Test Epoch 712\n",
      "-------------------------------\n",
      "loss tensor(0.8733, dtype=torch.float64)\n",
      "loss tensor(0.7415, dtype=torch.float64)\n",
      "dev loss: 1.6148115941265961\n",
      "\n",
      "Train Epoch 713\n",
      "-------------------------------\n",
      "loss: 0.463800  [  260/ 2430]\n",
      "Test Epoch 713\n",
      "-------------------------------\n",
      "loss tensor(0.8628, dtype=torch.float64)\n",
      "loss tensor(1.0708, dtype=torch.float64)\n",
      "dev loss: 1.933555282366011\n",
      "\n",
      "Train Epoch 714\n",
      "-------------------------------\n",
      "loss: 0.401681  [  260/ 2430]\n",
      "Test Epoch 714\n",
      "-------------------------------\n",
      "loss tensor(0.7982, dtype=torch.float64)\n",
      "loss tensor(1.0740, dtype=torch.float64)\n",
      "dev loss: 1.872247149554172\n",
      "\n",
      "Train Epoch 715\n",
      "-------------------------------\n",
      "loss: 0.404365  [  260/ 2430]\n",
      "Test Epoch 715\n",
      "-------------------------------\n",
      "loss tensor(0.9291, dtype=torch.float64)\n",
      "loss tensor(0.9400, dtype=torch.float64)\n",
      "dev loss: 1.8690531796404528\n",
      "\n",
      "Train Epoch 716\n",
      "-------------------------------\n",
      "loss: 0.611309  [  260/ 2430]\n",
      "Test Epoch 716\n",
      "-------------------------------\n",
      "loss tensor(0.8277, dtype=torch.float64)\n",
      "loss tensor(0.8385, dtype=torch.float64)\n",
      "dev loss: 1.6662338820318494\n",
      "\n",
      "Train Epoch 717\n",
      "-------------------------------\n",
      "loss: 0.438095  [  260/ 2430]\n",
      "Test Epoch 717\n",
      "-------------------------------\n",
      "loss tensor(0.8249, dtype=torch.float64)\n",
      "loss tensor(0.9048, dtype=torch.float64)\n",
      "dev loss: 1.7296846215527475\n",
      "\n",
      "Train Epoch 718\n",
      "-------------------------------\n",
      "loss: 0.499941  [  260/ 2430]\n",
      "Test Epoch 718\n",
      "-------------------------------\n",
      "loss tensor(0.7945, dtype=torch.float64)\n",
      "loss tensor(0.8331, dtype=torch.float64)\n",
      "dev loss: 1.6276293937999708\n",
      "\n",
      "Train Epoch 719\n",
      "-------------------------------\n",
      "loss: 0.421317  [  260/ 2430]\n",
      "Test Epoch 719\n",
      "-------------------------------\n",
      "loss tensor(0.8264, dtype=torch.float64)\n",
      "loss tensor(1.0787, dtype=torch.float64)\n",
      "dev loss: 1.9051239542833154\n",
      "\n",
      "Train Epoch 720\n",
      "-------------------------------\n",
      "loss: 0.468031  [  260/ 2430]\n",
      "Test Epoch 720\n",
      "-------------------------------\n",
      "loss tensor(0.7733, dtype=torch.float64)\n",
      "loss tensor(0.8201, dtype=torch.float64)\n",
      "dev loss: 1.5933665807171595\n",
      "\n",
      "Train Epoch 721\n",
      "-------------------------------\n",
      "loss: 0.365448  [  260/ 2430]\n",
      "Test Epoch 721\n",
      "-------------------------------\n",
      "loss tensor(0.8050, dtype=torch.float64)\n",
      "loss tensor(1.1261, dtype=torch.float64)\n",
      "dev loss: 1.9311749706731292\n",
      "\n",
      "Train Epoch 722\n",
      "-------------------------------\n",
      "loss: 0.433666  [  260/ 2430]\n",
      "Test Epoch 722\n",
      "-------------------------------\n",
      "loss tensor(0.8266, dtype=torch.float64)\n",
      "loss tensor(0.6790, dtype=torch.float64)\n",
      "dev loss: 1.5056280035056653\n",
      "\n",
      "Train Epoch 723\n",
      "-------------------------------\n",
      "loss: 0.416855  [  260/ 2430]\n",
      "Test Epoch 723\n",
      "-------------------------------\n",
      "loss tensor(0.7995, dtype=torch.float64)\n",
      "loss tensor(0.8514, dtype=torch.float64)\n",
      "dev loss: 1.6508962087824866\n",
      "\n",
      "Train Epoch 724\n",
      "-------------------------------\n",
      "loss: 0.449292  [  260/ 2430]\n",
      "Test Epoch 724\n",
      "-------------------------------\n",
      "loss tensor(0.8519, dtype=torch.float64)\n",
      "loss tensor(1.5007, dtype=torch.float64)\n",
      "dev loss: 2.3525780708080384\n",
      "\n",
      "Train Epoch 725\n",
      "-------------------------------\n",
      "loss: 0.437507  [  260/ 2430]\n",
      "Test Epoch 725\n",
      "-------------------------------\n",
      "loss tensor(0.9157, dtype=torch.float64)\n",
      "loss tensor(1.3931, dtype=torch.float64)\n",
      "dev loss: 2.3087922055774923\n",
      "\n",
      "Train Epoch 726\n",
      "-------------------------------\n",
      "loss: 0.514504  [  260/ 2430]\n",
      "Test Epoch 726\n",
      "-------------------------------\n",
      "loss tensor(0.8074, dtype=torch.float64)\n",
      "loss tensor(0.7928, dtype=torch.float64)\n",
      "dev loss: 1.6001439713446868\n",
      "\n",
      "Train Epoch 727\n",
      "-------------------------------\n",
      "loss: 0.401763  [  260/ 2430]\n",
      "Test Epoch 727\n",
      "-------------------------------\n",
      "loss tensor(0.8834, dtype=torch.float64)\n",
      "loss tensor(0.9827, dtype=torch.float64)\n",
      "dev loss: 1.8660617855721306\n",
      "\n",
      "Train Epoch 728\n",
      "-------------------------------\n",
      "loss: 0.390065  [  260/ 2430]\n",
      "Test Epoch 728\n",
      "-------------------------------\n",
      "loss tensor(0.8106, dtype=torch.float64)\n",
      "loss tensor(0.9121, dtype=torch.float64)\n",
      "dev loss: 1.722708697682241\n",
      "\n",
      "Train Epoch 729\n",
      "-------------------------------\n",
      "loss: 0.459361  [  260/ 2430]\n",
      "Test Epoch 729\n",
      "-------------------------------\n",
      "loss tensor(0.8683, dtype=torch.float64)\n",
      "loss tensor(1.5009, dtype=torch.float64)\n",
      "dev loss: 2.369207030951608\n",
      "\n",
      "Train Epoch 730\n",
      "-------------------------------\n",
      "loss: 0.537776  [  260/ 2430]\n",
      "Test Epoch 730\n",
      "-------------------------------\n",
      "loss tensor(0.8084, dtype=torch.float64)\n",
      "loss tensor(1.1046, dtype=torch.float64)\n",
      "dev loss: 1.9130517149260449\n",
      "\n",
      "Train Epoch 731\n",
      "-------------------------------\n",
      "loss: 0.364650  [  260/ 2430]\n",
      "Test Epoch 731\n",
      "-------------------------------\n",
      "loss tensor(0.9215, dtype=torch.float64)\n",
      "loss tensor(0.7003, dtype=torch.float64)\n",
      "dev loss: 1.621784263978253\n",
      "\n",
      "Train Epoch 732\n",
      "-------------------------------\n",
      "loss: 0.427690  [  260/ 2430]\n",
      "Test Epoch 732\n",
      "-------------------------------\n",
      "loss tensor(0.7671, dtype=torch.float64)\n",
      "loss tensor(1.0573, dtype=torch.float64)\n",
      "dev loss: 1.8244305192564556\n",
      "\n",
      "Train Epoch 733\n",
      "-------------------------------\n",
      "loss: 0.379297  [  260/ 2430]\n",
      "Test Epoch 733\n",
      "-------------------------------\n",
      "loss tensor(0.7865, dtype=torch.float64)\n",
      "loss tensor(0.8516, dtype=torch.float64)\n",
      "dev loss: 1.6381135443226227\n",
      "\n",
      "Train Epoch 734\n",
      "-------------------------------\n",
      "loss: 0.422552  [  260/ 2430]\n",
      "Test Epoch 734\n",
      "-------------------------------\n",
      "loss tensor(0.8281, dtype=torch.float64)\n",
      "loss tensor(1.1073, dtype=torch.float64)\n",
      "dev loss: 1.9354583105580767\n",
      "\n",
      "Train Epoch 735\n",
      "-------------------------------\n",
      "loss: 0.440373  [  260/ 2430]\n",
      "Test Epoch 735\n",
      "-------------------------------\n",
      "loss tensor(0.8097, dtype=torch.float64)\n",
      "loss tensor(1.1261, dtype=torch.float64)\n",
      "dev loss: 1.9358733804910133\n",
      "\n",
      "Train Epoch 736\n",
      "-------------------------------\n",
      "loss: 0.438162  [  260/ 2430]\n",
      "Test Epoch 736\n",
      "-------------------------------\n",
      "loss tensor(0.8457, dtype=torch.float64)\n",
      "loss tensor(1.2948, dtype=torch.float64)\n",
      "dev loss: 2.1404772637967575\n",
      "\n",
      "Train Epoch 737\n",
      "-------------------------------\n",
      "loss: 0.490201  [  260/ 2430]\n",
      "Test Epoch 737\n",
      "-------------------------------\n",
      "loss tensor(0.8406, dtype=torch.float64)\n",
      "loss tensor(1.2981, dtype=torch.float64)\n",
      "dev loss: 2.1387483579007744\n",
      "\n",
      "Train Epoch 738\n",
      "-------------------------------\n",
      "loss: 0.413978  [  260/ 2430]\n",
      "Test Epoch 738\n",
      "-------------------------------\n",
      "loss tensor(0.8471, dtype=torch.float64)\n",
      "loss tensor(0.8478, dtype=torch.float64)\n",
      "dev loss: 1.694977387142326\n",
      "\n",
      "Train Epoch 739\n",
      "-------------------------------\n",
      "loss: 0.445093  [  260/ 2430]\n",
      "Test Epoch 739\n",
      "-------------------------------\n",
      "loss tensor(0.8409, dtype=torch.float64)\n",
      "loss tensor(0.8277, dtype=torch.float64)\n",
      "dev loss: 1.6686870968495038\n",
      "\n",
      "Train Epoch 740\n",
      "-------------------------------\n",
      "loss: 0.357837  [  260/ 2430]\n",
      "Test Epoch 740\n",
      "-------------------------------\n",
      "loss tensor(0.7974, dtype=torch.float64)\n",
      "loss tensor(1.2160, dtype=torch.float64)\n",
      "dev loss: 2.0134130053223007\n",
      "\n",
      "Train Epoch 741\n",
      "-------------------------------\n",
      "loss: 0.385936  [  260/ 2430]\n",
      "Test Epoch 741\n",
      "-------------------------------\n",
      "loss tensor(0.8411, dtype=torch.float64)\n",
      "loss tensor(0.7619, dtype=torch.float64)\n",
      "dev loss: 1.60303975646969\n",
      "\n",
      "Train Epoch 742\n",
      "-------------------------------\n",
      "loss: 0.354705  [  260/ 2430]\n",
      "Test Epoch 742\n",
      "-------------------------------\n",
      "loss tensor(0.8033, dtype=torch.float64)\n",
      "loss tensor(1.3323, dtype=torch.float64)\n",
      "dev loss: 2.13567938882485\n",
      "\n",
      "Train Epoch 743\n",
      "-------------------------------\n",
      "loss: 0.495186  [  260/ 2430]\n",
      "Test Epoch 743\n",
      "-------------------------------\n",
      "loss tensor(0.8139, dtype=torch.float64)\n",
      "loss tensor(0.8423, dtype=torch.float64)\n",
      "dev loss: 1.6562474896200885\n",
      "\n",
      "Train Epoch 744\n",
      "-------------------------------\n",
      "loss: 0.450064  [  260/ 2430]\n",
      "Test Epoch 744\n",
      "-------------------------------\n",
      "loss tensor(0.8115, dtype=torch.float64)\n",
      "loss tensor(0.7541, dtype=torch.float64)\n",
      "dev loss: 1.565607719075818\n",
      "\n",
      "Train Epoch 745\n",
      "-------------------------------\n",
      "loss: 0.475373  [  260/ 2430]\n",
      "Test Epoch 745\n",
      "-------------------------------\n",
      "loss tensor(0.8077, dtype=torch.float64)\n",
      "loss tensor(1.0214, dtype=torch.float64)\n",
      "dev loss: 1.8290832900497573\n",
      "\n",
      "Train Epoch 746\n",
      "-------------------------------\n",
      "loss: 0.350815  [  260/ 2430]\n",
      "Test Epoch 746\n",
      "-------------------------------\n",
      "loss tensor(0.8176, dtype=torch.float64)\n",
      "loss tensor(1.6784, dtype=torch.float64)\n",
      "dev loss: 2.495990364371075\n",
      "\n",
      "Train Epoch 747\n",
      "-------------------------------\n",
      "loss: 0.420497  [  260/ 2430]\n",
      "Test Epoch 747\n",
      "-------------------------------\n",
      "loss tensor(0.8701, dtype=torch.float64)\n",
      "loss tensor(0.6377, dtype=torch.float64)\n",
      "dev loss: 1.5077669688215325\n",
      "\n",
      "Train Epoch 748\n",
      "-------------------------------\n",
      "loss: 0.408652  [  260/ 2430]\n",
      "Test Epoch 748\n",
      "-------------------------------\n",
      "loss tensor(0.8385, dtype=torch.float64)\n",
      "loss tensor(1.3503, dtype=torch.float64)\n",
      "dev loss: 2.1888224402741265\n",
      "\n",
      "Train Epoch 749\n",
      "-------------------------------\n",
      "loss: 0.462609  [  260/ 2430]\n",
      "Test Epoch 749\n",
      "-------------------------------\n",
      "loss tensor(0.9155, dtype=torch.float64)\n",
      "loss tensor(1.0427, dtype=torch.float64)\n",
      "dev loss: 1.9582694304795045\n",
      "\n",
      "Train Epoch 750\n",
      "-------------------------------\n",
      "loss: 0.470497  [  260/ 2430]\n",
      "Test Epoch 750\n",
      "-------------------------------\n",
      "loss tensor(0.8341, dtype=torch.float64)\n",
      "loss tensor(0.8986, dtype=torch.float64)\n",
      "dev loss: 1.7326869738170367\n",
      "\n",
      "Train Epoch 751\n",
      "-------------------------------\n",
      "loss: 0.340041  [  260/ 2430]\n",
      "Test Epoch 751\n",
      "-------------------------------\n",
      "loss tensor(0.7991, dtype=torch.float64)\n",
      "loss tensor(0.9205, dtype=torch.float64)\n",
      "dev loss: 1.7195954128970454\n",
      "\n",
      "Train Epoch 752\n",
      "-------------------------------\n",
      "loss: 0.457591  [  260/ 2430]\n",
      "Test Epoch 752\n",
      "-------------------------------\n",
      "loss tensor(0.8188, dtype=torch.float64)\n",
      "loss tensor(1.1954, dtype=torch.float64)\n",
      "dev loss: 2.014271032147253\n",
      "\n",
      "Train Epoch 753\n",
      "-------------------------------\n",
      "loss: 0.394070  [  260/ 2430]\n",
      "Test Epoch 753\n",
      "-------------------------------\n",
      "loss tensor(0.7947, dtype=torch.float64)\n",
      "loss tensor(1.1351, dtype=torch.float64)\n",
      "dev loss: 1.929786321085814\n",
      "\n",
      "Train Epoch 754\n",
      "-------------------------------\n",
      "loss: 0.404846  [  260/ 2430]\n",
      "Test Epoch 754\n",
      "-------------------------------\n",
      "loss tensor(0.7893, dtype=torch.float64)\n",
      "loss tensor(0.8072, dtype=torch.float64)\n",
      "dev loss: 1.596491700209559\n",
      "\n",
      "Train Epoch 755\n",
      "-------------------------------\n",
      "loss: 0.410752  [  260/ 2430]\n",
      "Test Epoch 755\n",
      "-------------------------------\n",
      "loss tensor(0.8518, dtype=torch.float64)\n",
      "loss tensor(0.9611, dtype=torch.float64)\n",
      "dev loss: 1.8129673354503448\n",
      "\n",
      "Train Epoch 756\n",
      "-------------------------------\n",
      "loss: 0.493411  [  260/ 2430]\n",
      "Test Epoch 756\n",
      "-------------------------------\n",
      "loss tensor(0.8772, dtype=torch.float64)\n",
      "loss tensor(1.0569, dtype=torch.float64)\n",
      "dev loss: 1.934061022220762\n",
      "\n",
      "Train Epoch 757\n",
      "-------------------------------\n",
      "loss: 0.504322  [  260/ 2430]\n",
      "Test Epoch 757\n",
      "-------------------------------\n",
      "loss tensor(0.8672, dtype=torch.float64)\n",
      "loss tensor(1.2764, dtype=torch.float64)\n",
      "dev loss: 2.1435883382612015\n",
      "\n",
      "Train Epoch 758\n",
      "-------------------------------\n",
      "loss: 0.326832  [  260/ 2430]\n",
      "Test Epoch 758\n",
      "-------------------------------\n",
      "loss tensor(0.8830, dtype=torch.float64)\n",
      "loss tensor(0.6726, dtype=torch.float64)\n",
      "dev loss: 1.555615264467154\n",
      "\n",
      "Train Epoch 759\n",
      "-------------------------------\n",
      "loss: 0.386307  [  260/ 2430]\n",
      "Test Epoch 759\n",
      "-------------------------------\n",
      "loss tensor(0.7839, dtype=torch.float64)\n",
      "loss tensor(0.9731, dtype=torch.float64)\n",
      "dev loss: 1.7570214572914238\n",
      "\n",
      "Train Epoch 760\n",
      "-------------------------------\n",
      "loss: 0.406044  [  260/ 2430]\n",
      "Test Epoch 760\n",
      "-------------------------------\n",
      "loss tensor(0.8264, dtype=torch.float64)\n",
      "loss tensor(1.3130, dtype=torch.float64)\n",
      "dev loss: 2.139426295711484\n",
      "\n",
      "Train Epoch 761\n",
      "-------------------------------\n",
      "loss: 0.368290  [  260/ 2430]\n",
      "Test Epoch 761\n",
      "-------------------------------\n",
      "loss tensor(0.7819, dtype=torch.float64)\n",
      "loss tensor(1.0269, dtype=torch.float64)\n",
      "dev loss: 1.8087655174930872\n",
      "\n",
      "Train Epoch 762\n",
      "-------------------------------\n",
      "loss: 0.377569  [  260/ 2430]\n",
      "Test Epoch 762\n",
      "-------------------------------\n",
      "loss tensor(0.7950, dtype=torch.float64)\n",
      "loss tensor(0.8075, dtype=torch.float64)\n",
      "dev loss: 1.6024937573179634\n",
      "\n",
      "Train Epoch 763\n",
      "-------------------------------\n",
      "loss: 0.402106  [  260/ 2430]\n",
      "Test Epoch 763\n",
      "-------------------------------\n",
      "loss tensor(0.8128, dtype=torch.float64)\n",
      "loss tensor(0.7470, dtype=torch.float64)\n",
      "dev loss: 1.5597688652185708\n",
      "\n",
      "Train Epoch 764\n",
      "-------------------------------\n",
      "loss: 0.462517  [  260/ 2430]\n",
      "Test Epoch 764\n",
      "-------------------------------\n",
      "loss tensor(0.8145, dtype=torch.float64)\n",
      "loss tensor(1.0042, dtype=torch.float64)\n",
      "dev loss: 1.8186448548398986\n",
      "\n",
      "Train Epoch 765\n",
      "-------------------------------\n",
      "loss: 0.445704  [  260/ 2430]\n",
      "Test Epoch 765\n",
      "-------------------------------\n",
      "loss tensor(0.7804, dtype=torch.float64)\n",
      "loss tensor(1.1176, dtype=torch.float64)\n",
      "dev loss: 1.8979653016557647\n",
      "\n",
      "Train Epoch 766\n",
      "-------------------------------\n",
      "loss: 0.421431  [  260/ 2430]\n",
      "Test Epoch 766\n",
      "-------------------------------\n",
      "loss tensor(0.8556, dtype=torch.float64)\n",
      "loss tensor(1.0797, dtype=torch.float64)\n",
      "dev loss: 1.935306823015008\n",
      "\n",
      "Train Epoch 767\n",
      "-------------------------------\n",
      "loss: 0.444221  [  260/ 2430]\n",
      "Test Epoch 767\n",
      "-------------------------------\n",
      "loss tensor(0.8213, dtype=torch.float64)\n",
      "loss tensor(0.8059, dtype=torch.float64)\n",
      "dev loss: 1.6271942268392996\n",
      "\n",
      "Train Epoch 768\n",
      "-------------------------------\n",
      "loss: 0.393748  [  260/ 2430]\n",
      "Test Epoch 768\n",
      "-------------------------------\n",
      "loss tensor(0.8641, dtype=torch.float64)\n",
      "loss tensor(0.9776, dtype=torch.float64)\n",
      "dev loss: 1.8417332402369007\n",
      "\n",
      "Train Epoch 769\n",
      "-------------------------------\n",
      "loss: 0.420048  [  260/ 2430]\n",
      "Test Epoch 769\n",
      "-------------------------------\n",
      "loss tensor(0.7714, dtype=torch.float64)\n",
      "loss tensor(0.7941, dtype=torch.float64)\n",
      "dev loss: 1.565470760295374\n",
      "\n",
      "Train Epoch 770\n",
      "-------------------------------\n",
      "loss: 0.410611  [  260/ 2430]\n",
      "Test Epoch 770\n",
      "-------------------------------\n",
      "loss tensor(0.8172, dtype=torch.float64)\n",
      "loss tensor(1.0941, dtype=torch.float64)\n",
      "dev loss: 1.9112913994766934\n",
      "\n",
      "Train Epoch 771\n",
      "-------------------------------\n",
      "loss: 0.394127  [  260/ 2430]\n",
      "Test Epoch 771\n",
      "-------------------------------\n",
      "loss tensor(0.8412, dtype=torch.float64)\n",
      "loss tensor(1.2356, dtype=torch.float64)\n",
      "dev loss: 2.0768278415005823\n",
      "\n",
      "Train Epoch 772\n",
      "-------------------------------\n",
      "loss: 0.465437  [  260/ 2430]\n",
      "Test Epoch 772\n",
      "-------------------------------\n",
      "loss tensor(0.8194, dtype=torch.float64)\n",
      "loss tensor(0.9864, dtype=torch.float64)\n",
      "dev loss: 1.8057787143765407\n",
      "\n",
      "Train Epoch 773\n",
      "-------------------------------\n",
      "loss: 0.411051  [  260/ 2430]\n",
      "Test Epoch 773\n",
      "-------------------------------\n",
      "loss tensor(0.8250, dtype=torch.float64)\n",
      "loss tensor(0.8667, dtype=torch.float64)\n",
      "dev loss: 1.6917548798854214\n",
      "\n",
      "Train Epoch 774\n",
      "-------------------------------\n",
      "loss: 0.436110  [  260/ 2430]\n",
      "Test Epoch 774\n",
      "-------------------------------\n",
      "loss tensor(0.8103, dtype=torch.float64)\n",
      "loss tensor(0.9322, dtype=torch.float64)\n",
      "dev loss: 1.7424631629349938\n",
      "\n",
      "Train Epoch 775\n",
      "-------------------------------\n",
      "loss: 0.421063  [  260/ 2430]\n",
      "Test Epoch 775\n",
      "-------------------------------\n",
      "loss tensor(0.7896, dtype=torch.float64)\n",
      "loss tensor(0.8004, dtype=torch.float64)\n",
      "dev loss: 1.5900371143174734\n",
      "\n",
      "Train Epoch 776\n",
      "-------------------------------\n",
      "loss: 0.376825  [  260/ 2430]\n",
      "Test Epoch 776\n",
      "-------------------------------\n",
      "loss tensor(0.8049, dtype=torch.float64)\n",
      "loss tensor(1.1788, dtype=torch.float64)\n",
      "dev loss: 1.983693510445455\n",
      "\n",
      "Train Epoch 777\n",
      "-------------------------------\n",
      "loss: 0.369617  [  260/ 2430]\n",
      "Test Epoch 777\n",
      "-------------------------------\n",
      "loss tensor(0.9146, dtype=torch.float64)\n",
      "loss tensor(1.2061, dtype=torch.float64)\n",
      "dev loss: 2.1207554094535785\n",
      "\n",
      "Train Epoch 778\n",
      "-------------------------------\n",
      "loss: 0.487313  [  260/ 2430]\n",
      "Test Epoch 778\n",
      "-------------------------------\n",
      "loss tensor(0.8530, dtype=torch.float64)\n",
      "loss tensor(0.7752, dtype=torch.float64)\n",
      "dev loss: 1.628252433724887\n",
      "\n",
      "Train Epoch 779\n",
      "-------------------------------\n",
      "loss: 0.400378  [  260/ 2430]\n",
      "Test Epoch 779\n",
      "-------------------------------\n",
      "loss tensor(0.7782, dtype=torch.float64)\n",
      "loss tensor(1.2030, dtype=torch.float64)\n",
      "dev loss: 1.9812004579700768\n",
      "\n",
      "Train Epoch 780\n",
      "-------------------------------\n",
      "loss: 0.434015  [  260/ 2430]\n",
      "Test Epoch 780\n",
      "-------------------------------\n",
      "loss tensor(0.7680, dtype=torch.float64)\n",
      "loss tensor(0.8400, dtype=torch.float64)\n",
      "dev loss: 1.607961295374506\n",
      "\n",
      "Train Epoch 781\n",
      "-------------------------------\n",
      "loss: 0.437682  [  260/ 2430]\n",
      "Test Epoch 781\n",
      "-------------------------------\n",
      "loss tensor(0.8892, dtype=torch.float64)\n",
      "loss tensor(1.0001, dtype=torch.float64)\n",
      "dev loss: 1.8892818945162144\n",
      "\n",
      "Train Epoch 782\n",
      "-------------------------------\n",
      "loss: 0.477787  [  260/ 2430]\n",
      "Test Epoch 782\n",
      "-------------------------------\n",
      "loss tensor(0.8197, dtype=torch.float64)\n",
      "loss tensor(1.1223, dtype=torch.float64)\n",
      "dev loss: 1.942037701436674\n",
      "\n",
      "Train Epoch 783\n",
      "-------------------------------\n",
      "loss: 0.468723  [  260/ 2430]\n",
      "Test Epoch 783\n",
      "-------------------------------\n",
      "loss tensor(0.8106, dtype=torch.float64)\n",
      "loss tensor(0.7478, dtype=torch.float64)\n",
      "dev loss: 1.5583963074378215\n",
      "\n",
      "Train Epoch 784\n",
      "-------------------------------\n",
      "loss: 0.374305  [  260/ 2430]\n",
      "Test Epoch 784\n",
      "-------------------------------\n",
      "loss tensor(0.8381, dtype=torch.float64)\n",
      "loss tensor(1.1032, dtype=torch.float64)\n",
      "dev loss: 1.9413077609268627\n",
      "\n",
      "Train Epoch 785\n",
      "-------------------------------\n",
      "loss: 0.426861  [  260/ 2430]\n",
      "Test Epoch 785\n",
      "-------------------------------\n",
      "loss tensor(0.8030, dtype=torch.float64)\n",
      "loss tensor(0.8396, dtype=torch.float64)\n",
      "dev loss: 1.6425459874064001\n",
      "\n",
      "Train Epoch 786\n",
      "-------------------------------\n",
      "loss: 0.465535  [  260/ 2430]\n",
      "Test Epoch 786\n",
      "-------------------------------\n",
      "loss tensor(0.8066, dtype=torch.float64)\n",
      "loss tensor(0.8221, dtype=torch.float64)\n",
      "dev loss: 1.6287826357932735\n",
      "\n",
      "Train Epoch 787\n",
      "-------------------------------\n",
      "loss: 0.411071  [  260/ 2430]\n",
      "Test Epoch 787\n",
      "-------------------------------\n",
      "loss tensor(0.7866, dtype=torch.float64)\n",
      "loss tensor(0.8225, dtype=torch.float64)\n",
      "dev loss: 1.609180312404463\n",
      "\n",
      "Train Epoch 788\n",
      "-------------------------------\n",
      "loss: 0.422207  [  260/ 2430]\n",
      "Test Epoch 788\n",
      "-------------------------------\n",
      "loss tensor(0.8589, dtype=torch.float64)\n",
      "loss tensor(1.0782, dtype=torch.float64)\n",
      "dev loss: 1.9371134408244912\n",
      "\n",
      "Train Epoch 789\n",
      "-------------------------------\n",
      "loss: 0.435656  [  260/ 2430]\n",
      "Test Epoch 789\n",
      "-------------------------------\n",
      "loss tensor(0.8216, dtype=torch.float64)\n",
      "loss tensor(1.1893, dtype=torch.float64)\n",
      "dev loss: 2.0109153432723312\n",
      "\n",
      "Train Epoch 790\n",
      "-------------------------------\n",
      "loss: 0.375256  [  260/ 2430]\n",
      "Test Epoch 790\n",
      "-------------------------------\n",
      "loss tensor(0.9282, dtype=torch.float64)\n",
      "loss tensor(0.7251, dtype=torch.float64)\n",
      "dev loss: 1.6532786659481238\n",
      "\n",
      "Train Epoch 791\n",
      "-------------------------------\n",
      "loss: 0.532422  [  260/ 2430]\n",
      "Test Epoch 791\n",
      "-------------------------------\n",
      "loss tensor(0.8032, dtype=torch.float64)\n",
      "loss tensor(1.0572, dtype=torch.float64)\n",
      "dev loss: 1.8603385842409097\n",
      "\n",
      "Train Epoch 792\n",
      "-------------------------------\n",
      "loss: 0.308529  [  260/ 2430]\n",
      "Test Epoch 792\n",
      "-------------------------------\n",
      "loss tensor(0.8229, dtype=torch.float64)\n",
      "loss tensor(1.1894, dtype=torch.float64)\n",
      "dev loss: 2.0122727689429336\n",
      "\n",
      "Train Epoch 793\n",
      "-------------------------------\n",
      "loss: 0.423829  [  260/ 2430]\n",
      "Test Epoch 793\n",
      "-------------------------------\n",
      "loss tensor(0.8680, dtype=torch.float64)\n",
      "loss tensor(1.2705, dtype=torch.float64)\n",
      "dev loss: 2.138533993464458\n",
      "\n",
      "Train Epoch 794\n",
      "-------------------------------\n",
      "loss: 0.403343  [  260/ 2430]\n",
      "Test Epoch 794\n",
      "-------------------------------\n",
      "loss tensor(0.9003, dtype=torch.float64)\n",
      "loss tensor(0.8210, dtype=torch.float64)\n",
      "dev loss: 1.7212778029999187\n",
      "\n",
      "Train Epoch 795\n",
      "-------------------------------\n",
      "loss: 0.437989  [  260/ 2430]\n",
      "Test Epoch 795\n",
      "-------------------------------\n",
      "loss tensor(0.7783, dtype=torch.float64)\n",
      "loss tensor(1.2223, dtype=torch.float64)\n",
      "dev loss: 2.000603647309008\n",
      "\n",
      "Train Epoch 796\n",
      "-------------------------------\n",
      "loss: 0.385176  [  260/ 2430]\n",
      "Test Epoch 796\n",
      "-------------------------------\n",
      "loss tensor(0.7881, dtype=torch.float64)\n",
      "loss tensor(0.9984, dtype=torch.float64)\n",
      "dev loss: 1.7865066502210523\n",
      "\n",
      "Train Epoch 797\n",
      "-------------------------------\n",
      "loss: 0.368747  [  260/ 2430]\n",
      "Test Epoch 797\n",
      "-------------------------------\n",
      "loss tensor(0.7578, dtype=torch.float64)\n",
      "loss tensor(0.9495, dtype=torch.float64)\n",
      "dev loss: 1.7072965204230528\n",
      "\n",
      "Train Epoch 798\n",
      "-------------------------------\n",
      "loss: 0.401312  [  260/ 2430]\n",
      "Test Epoch 798\n",
      "-------------------------------\n",
      "loss tensor(0.8018, dtype=torch.float64)\n",
      "loss tensor(1.0004, dtype=torch.float64)\n",
      "dev loss: 1.8022310168999491\n",
      "\n",
      "Train Epoch 799\n",
      "-------------------------------\n",
      "loss: 0.433259  [  260/ 2430]\n",
      "Test Epoch 799\n",
      "-------------------------------\n",
      "loss tensor(0.8414, dtype=torch.float64)\n",
      "loss tensor(1.1924, dtype=torch.float64)\n",
      "dev loss: 2.033874251615631\n",
      "\n",
      "Train Epoch 800\n",
      "-------------------------------\n",
      "loss: 0.393850  [  260/ 2430]\n",
      "Test Epoch 800\n",
      "-------------------------------\n",
      "loss tensor(0.7993, dtype=torch.float64)\n",
      "loss tensor(0.9309, dtype=torch.float64)\n",
      "dev loss: 1.7301946757730695\n",
      "\n",
      "Train Epoch 801\n",
      "-------------------------------\n",
      "loss: 0.402587  [  260/ 2430]\n",
      "Test Epoch 801\n",
      "-------------------------------\n",
      "loss tensor(0.8026, dtype=torch.float64)\n",
      "loss tensor(1.1759, dtype=torch.float64)\n",
      "dev loss: 1.978448349861913\n",
      "\n",
      "Train Epoch 802\n",
      "-------------------------------\n",
      "loss: 0.345237  [  260/ 2430]\n",
      "Test Epoch 802\n",
      "-------------------------------\n",
      "loss tensor(0.8268, dtype=torch.float64)\n",
      "loss tensor(0.9793, dtype=torch.float64)\n",
      "dev loss: 1.8060866456565265\n",
      "\n",
      "Train Epoch 803\n",
      "-------------------------------\n",
      "loss: 0.437574  [  260/ 2430]\n",
      "Test Epoch 803\n",
      "-------------------------------\n",
      "loss tensor(0.8845, dtype=torch.float64)\n",
      "loss tensor(1.4129, dtype=torch.float64)\n",
      "dev loss: 2.2973082794481736\n",
      "\n",
      "Train Epoch 804\n",
      "-------------------------------\n",
      "loss: 0.517231  [  260/ 2430]\n",
      "Test Epoch 804\n",
      "-------------------------------\n",
      "loss tensor(0.8436, dtype=torch.float64)\n",
      "loss tensor(1.1022, dtype=torch.float64)\n",
      "dev loss: 1.9457694271229509\n",
      "\n",
      "Train Epoch 805\n",
      "-------------------------------\n",
      "loss: 0.417310  [  260/ 2430]\n",
      "Test Epoch 805\n",
      "-------------------------------\n",
      "loss tensor(0.8688, dtype=torch.float64)\n",
      "loss tensor(0.9585, dtype=torch.float64)\n",
      "dev loss: 1.8273373125439178\n",
      "\n",
      "Train Epoch 806\n",
      "-------------------------------\n",
      "loss: 0.402245  [  260/ 2430]\n",
      "Test Epoch 806\n",
      "-------------------------------\n",
      "loss tensor(0.8334, dtype=torch.float64)\n",
      "loss tensor(1.0752, dtype=torch.float64)\n",
      "dev loss: 1.9086021953183272\n",
      "\n",
      "Train Epoch 807\n",
      "-------------------------------\n",
      "loss: 0.481520  [  260/ 2430]\n",
      "Test Epoch 807\n",
      "-------------------------------\n",
      "loss tensor(0.8892, dtype=torch.float64)\n",
      "loss tensor(1.0324, dtype=torch.float64)\n",
      "dev loss: 1.921546471561289\n",
      "\n",
      "Train Epoch 808\n",
      "-------------------------------\n",
      "loss: 0.387413  [  260/ 2430]\n",
      "Test Epoch 808\n",
      "-------------------------------\n",
      "loss tensor(0.7981, dtype=torch.float64)\n",
      "loss tensor(0.9252, dtype=torch.float64)\n",
      "dev loss: 1.7232986388002671\n",
      "\n",
      "Train Epoch 809\n",
      "-------------------------------\n",
      "loss: 0.309237  [  260/ 2430]\n",
      "Test Epoch 809\n",
      "-------------------------------\n",
      "loss tensor(0.8249, dtype=torch.float64)\n",
      "loss tensor(0.9422, dtype=torch.float64)\n",
      "dev loss: 1.7671821069733946\n",
      "\n",
      "Train Epoch 810\n",
      "-------------------------------\n",
      "loss: 0.397156  [  260/ 2430]\n",
      "Test Epoch 810\n",
      "-------------------------------\n",
      "loss tensor(0.7728, dtype=torch.float64)\n",
      "loss tensor(0.9924, dtype=torch.float64)\n",
      "dev loss: 1.765270384399647\n",
      "\n",
      "Train Epoch 811\n",
      "-------------------------------\n",
      "loss: 0.397756  [  260/ 2430]\n",
      "Test Epoch 811\n",
      "-------------------------------\n",
      "loss tensor(0.8063, dtype=torch.float64)\n",
      "loss tensor(0.9422, dtype=torch.float64)\n",
      "dev loss: 1.7484962672848305\n",
      "\n",
      "Train Epoch 812\n",
      "-------------------------------\n",
      "loss: 0.402219  [  260/ 2430]\n",
      "Test Epoch 812\n",
      "-------------------------------\n",
      "loss tensor(0.8375, dtype=torch.float64)\n",
      "loss tensor(1.0576, dtype=torch.float64)\n",
      "dev loss: 1.8951033794557368\n",
      "\n",
      "Train Epoch 813\n",
      "-------------------------------\n",
      "loss: 0.464963  [  260/ 2430]\n",
      "Test Epoch 813\n",
      "-------------------------------\n",
      "loss tensor(0.7770, dtype=torch.float64)\n",
      "loss tensor(1.1242, dtype=torch.float64)\n",
      "dev loss: 1.9012014600351472\n",
      "\n",
      "Train Epoch 814\n",
      "-------------------------------\n",
      "loss: 0.443416  [  260/ 2430]\n",
      "Test Epoch 814\n",
      "-------------------------------\n",
      "loss tensor(0.8157, dtype=torch.float64)\n",
      "loss tensor(0.7687, dtype=torch.float64)\n",
      "dev loss: 1.584457232272647\n",
      "\n",
      "Train Epoch 815\n",
      "-------------------------------\n",
      "loss: 0.353046  [  260/ 2430]\n",
      "Test Epoch 815\n",
      "-------------------------------\n",
      "loss tensor(0.8652, dtype=torch.float64)\n",
      "loss tensor(1.1967, dtype=torch.float64)\n",
      "dev loss: 2.0618793951768843\n",
      "\n",
      "Train Epoch 816\n",
      "-------------------------------\n",
      "loss: 0.504401  [  260/ 2430]\n",
      "Test Epoch 816\n",
      "-------------------------------\n",
      "loss tensor(0.9225, dtype=torch.float64)\n",
      "loss tensor(0.7436, dtype=torch.float64)\n",
      "dev loss: 1.6660554892288815\n",
      "\n",
      "Train Epoch 817\n",
      "-------------------------------\n",
      "loss: 0.435195  [  260/ 2430]\n",
      "Test Epoch 817\n",
      "-------------------------------\n",
      "loss tensor(0.8249, dtype=torch.float64)\n",
      "loss tensor(1.2240, dtype=torch.float64)\n",
      "dev loss: 2.0488431871411006\n",
      "\n",
      "Train Epoch 818\n",
      "-------------------------------\n",
      "loss: 0.443900  [  260/ 2430]\n",
      "Test Epoch 818\n",
      "-------------------------------\n",
      "loss tensor(0.9463, dtype=torch.float64)\n",
      "loss tensor(1.8391, dtype=torch.float64)\n",
      "dev loss: 2.785375533120819\n",
      "\n",
      "Train Epoch 819\n",
      "-------------------------------\n",
      "loss: 0.595287  [  260/ 2430]\n",
      "Test Epoch 819\n",
      "-------------------------------\n",
      "loss tensor(0.8258, dtype=torch.float64)\n",
      "loss tensor(1.0693, dtype=torch.float64)\n",
      "dev loss: 1.8950843994630802\n",
      "\n",
      "Train Epoch 820\n",
      "-------------------------------\n",
      "loss: 0.353569  [  260/ 2430]\n",
      "Test Epoch 820\n",
      "-------------------------------\n",
      "loss tensor(0.8979, dtype=torch.float64)\n",
      "loss tensor(0.7834, dtype=torch.float64)\n",
      "dev loss: 1.6812845382184656\n",
      "\n",
      "Train Epoch 821\n",
      "-------------------------------\n",
      "loss: 0.445119  [  260/ 2430]\n",
      "Test Epoch 821\n",
      "-------------------------------\n",
      "loss tensor(0.7669, dtype=torch.float64)\n",
      "loss tensor(0.7703, dtype=torch.float64)\n",
      "dev loss: 1.5371991691055054\n",
      "\n",
      "Train Epoch 822\n",
      "-------------------------------\n",
      "loss: 0.424519  [  260/ 2430]\n",
      "Test Epoch 822\n",
      "-------------------------------\n",
      "loss tensor(0.8261, dtype=torch.float64)\n",
      "loss tensor(1.2932, dtype=torch.float64)\n",
      "dev loss: 2.1192941188907692\n",
      "\n",
      "Train Epoch 823\n",
      "-------------------------------\n",
      "loss: 0.342614  [  260/ 2430]\n",
      "Test Epoch 823\n",
      "-------------------------------\n",
      "loss tensor(0.8145, dtype=torch.float64)\n",
      "loss tensor(0.7817, dtype=torch.float64)\n",
      "dev loss: 1.5961309617070605\n",
      "\n",
      "Train Epoch 824\n",
      "-------------------------------\n",
      "loss: 0.423783  [  260/ 2430]\n",
      "Test Epoch 824\n",
      "-------------------------------\n",
      "loss tensor(0.8745, dtype=torch.float64)\n",
      "loss tensor(0.7748, dtype=torch.float64)\n",
      "dev loss: 1.6493038692171376\n",
      "\n",
      "Train Epoch 825\n",
      "-------------------------------\n",
      "loss: 0.465457  [  260/ 2430]\n",
      "Test Epoch 825\n",
      "-------------------------------\n",
      "loss tensor(0.8009, dtype=torch.float64)\n",
      "loss tensor(1.1128, dtype=torch.float64)\n",
      "dev loss: 1.9137321772397098\n",
      "\n",
      "Train Epoch 826\n",
      "-------------------------------\n",
      "loss: 0.398402  [  260/ 2430]\n",
      "Test Epoch 826\n",
      "-------------------------------\n",
      "loss tensor(0.8018, dtype=torch.float64)\n",
      "loss tensor(1.1298, dtype=torch.float64)\n",
      "dev loss: 1.931607601179115\n",
      "\n",
      "Train Epoch 827\n",
      "-------------------------------\n",
      "loss: 0.354208  [  260/ 2430]\n",
      "Test Epoch 827\n",
      "-------------------------------\n",
      "loss tensor(0.8591, dtype=torch.float64)\n",
      "loss tensor(1.0934, dtype=torch.float64)\n",
      "dev loss: 1.9524749005082835\n",
      "\n",
      "Train Epoch 828\n",
      "-------------------------------\n",
      "loss: 0.416565  [  260/ 2430]\n",
      "Test Epoch 828\n",
      "-------------------------------\n",
      "loss tensor(0.8724, dtype=torch.float64)\n",
      "loss tensor(0.8712, dtype=torch.float64)\n",
      "dev loss: 1.7435249043533387\n",
      "\n",
      "Train Epoch 829\n",
      "-------------------------------\n",
      "loss: 0.401197  [  260/ 2430]\n",
      "Test Epoch 829\n",
      "-------------------------------\n",
      "loss tensor(0.8544, dtype=torch.float64)\n",
      "loss tensor(0.6744, dtype=torch.float64)\n",
      "dev loss: 1.5287659543089505\n",
      "\n",
      "Train Epoch 830\n",
      "-------------------------------\n",
      "loss: 0.395317  [  260/ 2430]\n",
      "Test Epoch 830\n",
      "-------------------------------\n",
      "loss tensor(0.8215, dtype=torch.float64)\n",
      "loss tensor(1.1820, dtype=torch.float64)\n",
      "dev loss: 2.0034893378009615\n",
      "\n",
      "Train Epoch 831\n",
      "-------------------------------\n",
      "loss: 0.344388  [  260/ 2430]\n",
      "Test Epoch 831\n",
      "-------------------------------\n",
      "loss tensor(0.8617, dtype=torch.float64)\n",
      "loss tensor(1.2868, dtype=torch.float64)\n",
      "dev loss: 2.1485477583855888\n",
      "\n",
      "Train Epoch 832\n",
      "-------------------------------\n",
      "loss: 0.450425  [  260/ 2430]\n",
      "Test Epoch 832\n",
      "-------------------------------\n",
      "loss tensor(0.8097, dtype=torch.float64)\n",
      "loss tensor(1.3350, dtype=torch.float64)\n",
      "dev loss: 2.1446703232001947\n",
      "\n",
      "Train Epoch 833\n",
      "-------------------------------\n",
      "loss: 0.358081  [  260/ 2430]\n",
      "Test Epoch 833\n",
      "-------------------------------\n",
      "loss tensor(0.8958, dtype=torch.float64)\n",
      "loss tensor(0.7884, dtype=torch.float64)\n",
      "dev loss: 1.6841938299437107\n",
      "\n",
      "Train Epoch 834\n",
      "-------------------------------\n",
      "loss: 0.449369  [  260/ 2430]\n",
      "Test Epoch 834\n",
      "-------------------------------\n",
      "loss tensor(0.9553, dtype=torch.float64)\n",
      "loss tensor(0.7584, dtype=torch.float64)\n",
      "dev loss: 1.7136893282599932\n",
      "\n",
      "Train Epoch 835\n",
      "-------------------------------\n",
      "loss: 0.381567  [  260/ 2430]\n",
      "Test Epoch 835\n",
      "-------------------------------\n",
      "loss tensor(0.8275, dtype=torch.float64)\n",
      "loss tensor(0.7712, dtype=torch.float64)\n",
      "dev loss: 1.598686002467215\n",
      "\n",
      "Train Epoch 836\n",
      "-------------------------------\n",
      "loss: 0.413631  [  260/ 2430]\n",
      "Test Epoch 836\n",
      "-------------------------------\n",
      "loss tensor(0.8435, dtype=torch.float64)\n",
      "loss tensor(1.3662, dtype=torch.float64)\n",
      "dev loss: 2.209675761184905\n",
      "\n",
      "Train Epoch 837\n",
      "-------------------------------\n",
      "loss: 0.464850  [  260/ 2430]\n",
      "Test Epoch 837\n",
      "-------------------------------\n",
      "loss tensor(0.7755, dtype=torch.float64)\n",
      "loss tensor(0.8902, dtype=torch.float64)\n",
      "dev loss: 1.665714172581069\n",
      "\n",
      "Train Epoch 838\n",
      "-------------------------------\n",
      "loss: 0.354780  [  260/ 2430]\n",
      "Test Epoch 838\n",
      "-------------------------------\n",
      "loss tensor(0.7938, dtype=torch.float64)\n",
      "loss tensor(0.8051, dtype=torch.float64)\n",
      "dev loss: 1.5988927663150023\n",
      "\n",
      "Train Epoch 839\n",
      "-------------------------------\n",
      "loss: 0.429006  [  260/ 2430]\n",
      "Test Epoch 839\n",
      "-------------------------------\n",
      "loss tensor(0.8802, dtype=torch.float64)\n",
      "loss tensor(1.1413, dtype=torch.float64)\n",
      "dev loss: 2.021463595948451\n",
      "\n",
      "Train Epoch 840\n",
      "-------------------------------\n",
      "loss: 0.352816  [  260/ 2430]\n",
      "Test Epoch 840\n",
      "-------------------------------\n",
      "loss tensor(0.8068, dtype=torch.float64)\n",
      "loss tensor(0.9442, dtype=torch.float64)\n",
      "dev loss: 1.7509184961663693\n",
      "\n",
      "Train Epoch 841\n",
      "-------------------------------\n",
      "loss: 0.440332  [  260/ 2430]\n",
      "Test Epoch 841\n",
      "-------------------------------\n",
      "loss tensor(0.8103, dtype=torch.float64)\n",
      "loss tensor(0.9791, dtype=torch.float64)\n",
      "dev loss: 1.7894016475848544\n",
      "\n",
      "Train Epoch 842\n",
      "-------------------------------\n",
      "loss: 0.386305  [  260/ 2430]\n",
      "Test Epoch 842\n",
      "-------------------------------\n",
      "loss tensor(0.8176, dtype=torch.float64)\n",
      "loss tensor(0.7982, dtype=torch.float64)\n",
      "dev loss: 1.6158875701635385\n",
      "\n",
      "Train Epoch 843\n",
      "-------------------------------\n",
      "loss: 0.349603  [  260/ 2430]\n",
      "Test Epoch 843\n",
      "-------------------------------\n",
      "loss tensor(0.7811, dtype=torch.float64)\n",
      "loss tensor(0.9948, dtype=torch.float64)\n",
      "dev loss: 1.7759387026937092\n",
      "\n",
      "Train Epoch 844\n",
      "-------------------------------\n",
      "loss: 0.365052  [  260/ 2430]\n",
      "Test Epoch 844\n",
      "-------------------------------\n",
      "loss tensor(0.7956, dtype=torch.float64)\n",
      "loss tensor(0.9411, dtype=torch.float64)\n",
      "dev loss: 1.7366824214775471\n",
      "\n",
      "Train Epoch 845\n",
      "-------------------------------\n",
      "loss: 0.388373  [  260/ 2430]\n",
      "Test Epoch 845\n",
      "-------------------------------\n",
      "loss tensor(0.7770, dtype=torch.float64)\n",
      "loss tensor(0.7766, dtype=torch.float64)\n",
      "dev loss: 1.553565529624742\n",
      "\n",
      "Train Epoch 846\n",
      "-------------------------------\n",
      "loss: 0.370384  [  260/ 2430]\n",
      "Test Epoch 846\n",
      "-------------------------------\n",
      "loss tensor(0.8309, dtype=torch.float64)\n",
      "loss tensor(1.1344, dtype=torch.float64)\n",
      "dev loss: 1.9652848318790848\n",
      "\n",
      "Train Epoch 847\n",
      "-------------------------------\n",
      "loss: 0.406916  [  260/ 2430]\n",
      "Test Epoch 847\n",
      "-------------------------------\n",
      "loss tensor(0.8620, dtype=torch.float64)\n",
      "loss tensor(1.2813, dtype=torch.float64)\n",
      "dev loss: 2.1432814033703758\n",
      "\n",
      "Train Epoch 848\n",
      "-------------------------------\n",
      "loss: 0.452106  [  260/ 2430]\n",
      "Test Epoch 848\n",
      "-------------------------------\n",
      "loss tensor(0.8643, dtype=torch.float64)\n",
      "loss tensor(1.4827, dtype=torch.float64)\n",
      "dev loss: 2.3470469239484015\n",
      "\n",
      "Train Epoch 849\n",
      "-------------------------------\n",
      "loss: 0.462550  [  260/ 2430]\n",
      "Test Epoch 849\n",
      "-------------------------------\n",
      "loss tensor(0.7844, dtype=torch.float64)\n",
      "loss tensor(0.8533, dtype=torch.float64)\n",
      "dev loss: 1.6377174613538346\n",
      "\n",
      "Train Epoch 850\n",
      "-------------------------------\n",
      "loss: 0.420802  [  260/ 2430]\n",
      "Test Epoch 850\n",
      "-------------------------------\n",
      "loss tensor(0.8461, dtype=torch.float64)\n",
      "loss tensor(0.8947, dtype=torch.float64)\n",
      "dev loss: 1.7407968684207784\n",
      "\n",
      "Train Epoch 851\n",
      "-------------------------------\n",
      "loss: 0.372810  [  260/ 2430]\n",
      "Test Epoch 851\n",
      "-------------------------------\n",
      "loss tensor(0.8138, dtype=torch.float64)\n",
      "loss tensor(0.8549, dtype=torch.float64)\n",
      "dev loss: 1.668773386390197\n",
      "\n",
      "Train Epoch 852\n",
      "-------------------------------\n",
      "loss: 0.366455  [  260/ 2430]\n",
      "Test Epoch 852\n",
      "-------------------------------\n",
      "loss tensor(0.8612, dtype=torch.float64)\n",
      "loss tensor(0.8764, dtype=torch.float64)\n",
      "dev loss: 1.7376242922294916\n",
      "\n",
      "Train Epoch 853\n",
      "-------------------------------\n",
      "loss: 0.425684  [  260/ 2430]\n",
      "Test Epoch 853\n",
      "-------------------------------\n",
      "loss tensor(0.8694, dtype=torch.float64)\n",
      "loss tensor(1.1020, dtype=torch.float64)\n",
      "dev loss: 1.9713897368591526\n",
      "\n",
      "Train Epoch 854\n",
      "-------------------------------\n",
      "loss: 0.558983  [  260/ 2430]\n",
      "Test Epoch 854\n",
      "-------------------------------\n",
      "loss tensor(0.7937, dtype=torch.float64)\n",
      "loss tensor(1.2504, dtype=torch.float64)\n",
      "dev loss: 2.044074309849546\n",
      "\n",
      "Train Epoch 855\n",
      "-------------------------------\n",
      "loss: 0.362411  [  260/ 2430]\n",
      "Test Epoch 855\n",
      "-------------------------------\n",
      "loss tensor(0.8265, dtype=torch.float64)\n",
      "loss tensor(0.8786, dtype=torch.float64)\n",
      "dev loss: 1.7050820654618044\n",
      "\n",
      "Train Epoch 856\n",
      "-------------------------------\n",
      "loss: 0.413546  [  260/ 2430]\n",
      "Test Epoch 856\n",
      "-------------------------------\n",
      "loss tensor(0.8103, dtype=torch.float64)\n",
      "loss tensor(0.6836, dtype=torch.float64)\n",
      "dev loss: 1.493863266381414\n",
      "\n",
      "Train Epoch 857\n",
      "-------------------------------\n",
      "loss: 0.311879  [  260/ 2430]\n",
      "Test Epoch 857\n",
      "-------------------------------\n",
      "loss tensor(0.7849, dtype=torch.float64)\n",
      "loss tensor(1.1829, dtype=torch.float64)\n",
      "dev loss: 1.9678056426233224\n",
      "\n",
      "Train Epoch 858\n",
      "-------------------------------\n",
      "loss: 0.460188  [  260/ 2430]\n",
      "Test Epoch 858\n",
      "-------------------------------\n",
      "loss tensor(0.7843, dtype=torch.float64)\n",
      "loss tensor(0.7944, dtype=torch.float64)\n",
      "dev loss: 1.5787170830903772\n",
      "\n",
      "Train Epoch 859\n",
      "-------------------------------\n",
      "loss: 0.389068  [  260/ 2430]\n",
      "Test Epoch 859\n",
      "-------------------------------\n",
      "loss tensor(0.8409, dtype=torch.float64)\n",
      "loss tensor(1.0637, dtype=torch.float64)\n",
      "dev loss: 1.9045995866669112\n",
      "\n",
      "Train Epoch 860\n",
      "-------------------------------\n",
      "loss: 0.414394  [  260/ 2430]\n",
      "Test Epoch 860\n",
      "-------------------------------\n",
      "loss tensor(0.9006, dtype=torch.float64)\n",
      "loss tensor(1.1194, dtype=torch.float64)\n",
      "dev loss: 2.0200082621384703\n",
      "\n",
      "Train Epoch 861\n",
      "-------------------------------\n",
      "loss: 0.382700  [  260/ 2430]\n",
      "Test Epoch 861\n",
      "-------------------------------\n",
      "loss tensor(0.8197, dtype=torch.float64)\n",
      "loss tensor(0.8284, dtype=torch.float64)\n",
      "dev loss: 1.6480295810127021\n",
      "\n",
      "Train Epoch 862\n",
      "-------------------------------\n",
      "loss: 0.365426  [  260/ 2430]\n",
      "Test Epoch 862\n",
      "-------------------------------\n",
      "loss tensor(0.8148, dtype=torch.float64)\n",
      "loss tensor(0.7219, dtype=torch.float64)\n",
      "dev loss: 1.5366375340011833\n",
      "\n",
      "Train Epoch 863\n",
      "-------------------------------\n",
      "loss: 0.429297  [  260/ 2430]\n",
      "Test Epoch 863\n",
      "-------------------------------\n",
      "loss tensor(0.8255, dtype=torch.float64)\n",
      "loss tensor(1.6048, dtype=torch.float64)\n",
      "dev loss: 2.430282605782307\n",
      "\n",
      "Train Epoch 864\n",
      "-------------------------------\n",
      "loss: 0.392549  [  260/ 2430]\n",
      "Test Epoch 864\n",
      "-------------------------------\n",
      "loss tensor(0.8308, dtype=torch.float64)\n",
      "loss tensor(0.9845, dtype=torch.float64)\n",
      "dev loss: 1.8152480064943863\n",
      "\n",
      "Train Epoch 865\n",
      "-------------------------------\n",
      "loss: 0.377604  [  260/ 2430]\n",
      "Test Epoch 865\n",
      "-------------------------------\n",
      "loss tensor(0.8366, dtype=torch.float64)\n",
      "loss tensor(0.8713, dtype=torch.float64)\n",
      "dev loss: 1.707895640297147\n",
      "\n",
      "Train Epoch 866\n",
      "-------------------------------\n",
      "loss: 0.296004  [  260/ 2430]\n",
      "Test Epoch 866\n",
      "-------------------------------\n",
      "loss tensor(0.8467, dtype=torch.float64)\n",
      "loss tensor(0.9112, dtype=torch.float64)\n",
      "dev loss: 1.7578082894400895\n",
      "\n",
      "Train Epoch 867\n",
      "-------------------------------\n",
      "loss: 0.387211  [  260/ 2430]\n",
      "Test Epoch 867\n",
      "-------------------------------\n",
      "loss tensor(0.8040, dtype=torch.float64)\n",
      "loss tensor(0.8441, dtype=torch.float64)\n",
      "dev loss: 1.6480638549204516\n",
      "\n",
      "Train Epoch 868\n",
      "-------------------------------\n",
      "loss: 0.342205  [  260/ 2430]\n",
      "Test Epoch 868\n",
      "-------------------------------\n",
      "loss tensor(0.8430, dtype=torch.float64)\n",
      "loss tensor(1.3095, dtype=torch.float64)\n",
      "dev loss: 2.1525206411230475\n",
      "\n",
      "Train Epoch 869\n",
      "-------------------------------\n",
      "loss: 0.467169  [  260/ 2430]\n",
      "Test Epoch 869\n",
      "-------------------------------\n",
      "loss tensor(0.8716, dtype=torch.float64)\n",
      "loss tensor(1.3401, dtype=torch.float64)\n",
      "dev loss: 2.2117674885909673\n",
      "\n",
      "Train Epoch 870\n",
      "-------------------------------\n",
      "loss: 0.401873  [  260/ 2430]\n",
      "Test Epoch 870\n",
      "-------------------------------\n",
      "loss tensor(0.8095, dtype=torch.float64)\n",
      "loss tensor(1.0068, dtype=torch.float64)\n",
      "dev loss: 1.8163617779369123\n",
      "\n",
      "Train Epoch 871\n",
      "-------------------------------\n",
      "loss: 0.428543  [  260/ 2430]\n",
      "Test Epoch 871\n",
      "-------------------------------\n",
      "loss tensor(0.8700, dtype=torch.float64)\n",
      "loss tensor(1.0081, dtype=torch.float64)\n",
      "dev loss: 1.8780174016988855\n",
      "\n",
      "Train Epoch 872\n",
      "-------------------------------\n",
      "loss: 0.466244  [  260/ 2430]\n",
      "Test Epoch 872\n",
      "-------------------------------\n",
      "loss tensor(0.8669, dtype=torch.float64)\n",
      "loss tensor(0.9460, dtype=torch.float64)\n",
      "dev loss: 1.8129326060612612\n",
      "\n",
      "Train Epoch 873\n",
      "-------------------------------\n",
      "loss: 0.389104  [  260/ 2430]\n",
      "Test Epoch 873\n",
      "-------------------------------\n",
      "loss tensor(0.8494, dtype=torch.float64)\n",
      "loss tensor(1.3691, dtype=torch.float64)\n",
      "dev loss: 2.218433882872399\n",
      "\n",
      "Train Epoch 874\n",
      "-------------------------------\n",
      "loss: 0.431078  [  260/ 2430]\n",
      "Test Epoch 874\n",
      "-------------------------------\n",
      "loss tensor(0.8143, dtype=torch.float64)\n",
      "loss tensor(0.9202, dtype=torch.float64)\n",
      "dev loss: 1.7345009594096106\n",
      "\n",
      "Train Epoch 875\n",
      "-------------------------------\n",
      "loss: 0.382797  [  260/ 2430]\n",
      "Test Epoch 875\n",
      "-------------------------------\n",
      "loss tensor(0.8529, dtype=torch.float64)\n",
      "loss tensor(0.7873, dtype=torch.float64)\n",
      "dev loss: 1.6401492390381058\n",
      "\n",
      "Train Epoch 876\n",
      "-------------------------------\n",
      "loss: 0.478800  [  260/ 2430]\n",
      "Test Epoch 876\n",
      "-------------------------------\n",
      "loss tensor(0.8429, dtype=torch.float64)\n",
      "loss tensor(0.9274, dtype=torch.float64)\n",
      "dev loss: 1.7702679809776696\n",
      "\n",
      "Train Epoch 877\n",
      "-------------------------------\n",
      "loss: 0.362714  [  260/ 2430]\n",
      "Test Epoch 877\n",
      "-------------------------------\n",
      "loss tensor(0.8722, dtype=torch.float64)\n",
      "loss tensor(1.1528, dtype=torch.float64)\n",
      "dev loss: 2.0250064319249805\n",
      "\n",
      "Train Epoch 878\n",
      "-------------------------------\n",
      "loss: 0.382151  [  260/ 2430]\n",
      "Test Epoch 878\n",
      "-------------------------------\n",
      "loss tensor(0.8179, dtype=torch.float64)\n",
      "loss tensor(1.0136, dtype=torch.float64)\n",
      "dev loss: 1.831482497688039\n",
      "\n",
      "Train Epoch 879\n",
      "-------------------------------\n",
      "loss: 0.383522  [  260/ 2430]\n",
      "Test Epoch 879\n",
      "-------------------------------\n",
      "loss tensor(0.8029, dtype=torch.float64)\n",
      "loss tensor(0.9155, dtype=torch.float64)\n",
      "dev loss: 1.7184017016070063\n",
      "\n",
      "Train Epoch 880\n",
      "-------------------------------\n",
      "loss: 0.352224  [  260/ 2430]\n",
      "Test Epoch 880\n",
      "-------------------------------\n",
      "loss tensor(0.8428, dtype=torch.float64)\n",
      "loss tensor(0.8039, dtype=torch.float64)\n",
      "dev loss: 1.6466614345785922\n",
      "\n",
      "Train Epoch 881\n",
      "-------------------------------\n",
      "loss: 0.322945  [  260/ 2430]\n",
      "Test Epoch 881\n",
      "-------------------------------\n",
      "loss tensor(0.8835, dtype=torch.float64)\n",
      "loss tensor(1.0404, dtype=torch.float64)\n",
      "dev loss: 1.9238618283336857\n",
      "\n",
      "Train Epoch 882\n",
      "-------------------------------\n",
      "loss: 0.412777  [  260/ 2430]\n",
      "Test Epoch 882\n",
      "-------------------------------\n",
      "loss tensor(0.8875, dtype=torch.float64)\n",
      "loss tensor(0.9036, dtype=torch.float64)\n",
      "dev loss: 1.791143054404582\n",
      "\n",
      "Train Epoch 883\n",
      "-------------------------------\n",
      "loss: 0.470608  [  260/ 2430]\n",
      "Test Epoch 883\n",
      "-------------------------------\n",
      "loss tensor(0.8171, dtype=torch.float64)\n",
      "loss tensor(1.3126, dtype=torch.float64)\n",
      "dev loss: 2.1296931203409386\n",
      "\n",
      "Train Epoch 884\n",
      "-------------------------------\n",
      "loss: 0.393022  [  260/ 2430]\n",
      "Test Epoch 884\n",
      "-------------------------------\n",
      "loss tensor(0.7871, dtype=torch.float64)\n",
      "loss tensor(0.8572, dtype=torch.float64)\n",
      "dev loss: 1.6443186867845938\n",
      "\n",
      "Train Epoch 885\n",
      "-------------------------------\n",
      "loss: 0.406004  [  260/ 2430]\n",
      "Test Epoch 885\n",
      "-------------------------------\n",
      "loss tensor(0.8133, dtype=torch.float64)\n",
      "loss tensor(1.1510, dtype=torch.float64)\n",
      "dev loss: 1.9642954673631314\n",
      "\n",
      "Train Epoch 886\n",
      "-------------------------------\n",
      "loss: 0.281007  [  260/ 2430]\n",
      "Test Epoch 886\n",
      "-------------------------------\n",
      "loss tensor(0.7866, dtype=torch.float64)\n",
      "loss tensor(0.9411, dtype=torch.float64)\n",
      "dev loss: 1.7277567661838968\n",
      "\n",
      "Train Epoch 887\n",
      "-------------------------------\n",
      "loss: 0.387800  [  260/ 2430]\n",
      "Test Epoch 887\n",
      "-------------------------------\n",
      "loss tensor(0.8049, dtype=torch.float64)\n",
      "loss tensor(1.1180, dtype=torch.float64)\n",
      "dev loss: 1.9228438983080314\n",
      "\n",
      "Train Epoch 888\n",
      "-------------------------------\n",
      "loss: 0.364684  [  260/ 2430]\n",
      "Test Epoch 888\n",
      "-------------------------------\n",
      "loss tensor(0.9001, dtype=torch.float64)\n",
      "loss tensor(0.6056, dtype=torch.float64)\n",
      "dev loss: 1.505745627311236\n",
      "\n",
      "Train Epoch 889\n",
      "-------------------------------\n",
      "loss: 0.566042  [  260/ 2430]\n",
      "Test Epoch 889\n",
      "-------------------------------\n",
      "loss tensor(0.7933, dtype=torch.float64)\n",
      "loss tensor(0.9984, dtype=torch.float64)\n",
      "dev loss: 1.7917222728446247\n",
      "\n",
      "Train Epoch 890\n",
      "-------------------------------\n",
      "loss: 0.451854  [  260/ 2430]\n",
      "Test Epoch 890\n",
      "-------------------------------\n",
      "loss tensor(0.8160, dtype=torch.float64)\n",
      "loss tensor(1.2560, dtype=torch.float64)\n",
      "dev loss: 2.0720212282319457\n",
      "\n",
      "Train Epoch 891\n",
      "-------------------------------\n",
      "loss: 0.379082  [  260/ 2430]\n",
      "Test Epoch 891\n",
      "-------------------------------\n",
      "loss tensor(0.8847, dtype=torch.float64)\n",
      "loss tensor(0.8009, dtype=torch.float64)\n",
      "dev loss: 1.685563169592808\n",
      "\n",
      "Train Epoch 892\n",
      "-------------------------------\n",
      "loss: 0.378923  [  260/ 2430]\n",
      "Test Epoch 892\n",
      "-------------------------------\n",
      "loss tensor(0.7843, dtype=torch.float64)\n",
      "loss tensor(0.9877, dtype=torch.float64)\n",
      "dev loss: 1.7720251430297775\n",
      "\n",
      "Train Epoch 893\n",
      "-------------------------------\n",
      "loss: 0.324987  [  260/ 2430]\n",
      "Test Epoch 893\n",
      "-------------------------------\n",
      "loss tensor(0.8440, dtype=torch.float64)\n",
      "loss tensor(0.7933, dtype=torch.float64)\n",
      "dev loss: 1.637271826021141\n",
      "\n",
      "Train Epoch 894\n",
      "-------------------------------\n",
      "loss: 0.411167  [  260/ 2430]\n",
      "Test Epoch 894\n",
      "-------------------------------\n",
      "loss tensor(0.8499, dtype=torch.float64)\n",
      "loss tensor(1.2528, dtype=torch.float64)\n",
      "dev loss: 2.1027622555217604\n",
      "\n",
      "Train Epoch 895\n",
      "-------------------------------\n",
      "loss: 0.372017  [  260/ 2430]\n",
      "Test Epoch 895\n",
      "-------------------------------\n",
      "loss tensor(0.8375, dtype=torch.float64)\n",
      "loss tensor(0.8041, dtype=torch.float64)\n",
      "dev loss: 1.6416649893597584\n",
      "\n",
      "Train Epoch 896\n",
      "-------------------------------\n",
      "loss: 0.312618  [  260/ 2430]\n",
      "Test Epoch 896\n",
      "-------------------------------\n",
      "loss tensor(0.8220, dtype=torch.float64)\n",
      "loss tensor(0.8827, dtype=torch.float64)\n",
      "dev loss: 1.704758863040523\n",
      "\n",
      "Train Epoch 897\n",
      "-------------------------------\n",
      "loss: 0.420582  [  260/ 2430]\n",
      "Test Epoch 897\n",
      "-------------------------------\n",
      "loss tensor(0.7960, dtype=torch.float64)\n",
      "loss tensor(1.5072, dtype=torch.float64)\n",
      "dev loss: 2.3031300015963776\n",
      "\n",
      "Train Epoch 898\n",
      "-------------------------------\n",
      "loss: 0.435244  [  260/ 2430]\n",
      "Test Epoch 898\n",
      "-------------------------------\n",
      "loss tensor(0.8495, dtype=torch.float64)\n",
      "loss tensor(1.0940, dtype=torch.float64)\n",
      "dev loss: 1.9435249073471037\n",
      "\n",
      "Train Epoch 899\n",
      "-------------------------------\n",
      "loss: 0.328279  [  260/ 2430]\n",
      "Test Epoch 899\n",
      "-------------------------------\n",
      "loss tensor(0.7984, dtype=torch.float64)\n",
      "loss tensor(1.2875, dtype=torch.float64)\n",
      "dev loss: 2.0858771804591387\n",
      "\n",
      "Train Epoch 900\n",
      "-------------------------------\n",
      "loss: 0.337832  [  260/ 2430]\n",
      "Test Epoch 900\n",
      "-------------------------------\n",
      "loss tensor(0.8005, dtype=torch.float64)\n",
      "loss tensor(0.9295, dtype=torch.float64)\n",
      "dev loss: 1.7300955644342582\n",
      "\n",
      "Train Epoch 901\n",
      "-------------------------------\n",
      "loss: 0.391014  [  260/ 2430]\n",
      "Test Epoch 901\n",
      "-------------------------------\n",
      "loss tensor(0.8045, dtype=torch.float64)\n",
      "loss tensor(1.1546, dtype=torch.float64)\n",
      "dev loss: 1.9590963872477891\n",
      "\n",
      "Train Epoch 902\n",
      "-------------------------------\n",
      "loss: 0.347185  [  260/ 2430]\n",
      "Test Epoch 902\n",
      "-------------------------------\n",
      "loss tensor(0.8381, dtype=torch.float64)\n",
      "loss tensor(0.9122, dtype=torch.float64)\n",
      "dev loss: 1.7502644613827978\n",
      "\n",
      "Train Epoch 903\n",
      "-------------------------------\n",
      "loss: 0.336993  [  260/ 2430]\n",
      "Test Epoch 903\n",
      "-------------------------------\n",
      "loss tensor(0.8538, dtype=torch.float64)\n",
      "loss tensor(0.7536, dtype=torch.float64)\n",
      "dev loss: 1.6074610128317999\n",
      "\n",
      "Train Epoch 904\n",
      "-------------------------------\n",
      "loss: 0.344904  [  260/ 2430]\n",
      "Test Epoch 904\n",
      "-------------------------------\n",
      "loss tensor(0.8833, dtype=torch.float64)\n",
      "loss tensor(1.0279, dtype=torch.float64)\n",
      "dev loss: 1.9112086077626325\n",
      "\n",
      "Train Epoch 905\n",
      "-------------------------------\n",
      "loss: 0.388947  [  260/ 2430]\n",
      "Test Epoch 905\n",
      "-------------------------------\n",
      "loss tensor(0.8012, dtype=torch.float64)\n",
      "loss tensor(1.2106, dtype=torch.float64)\n",
      "dev loss: 2.011875207566389\n",
      "\n",
      "Train Epoch 906\n",
      "-------------------------------\n",
      "loss: 0.324943  [  260/ 2430]\n",
      "Test Epoch 906\n",
      "-------------------------------\n",
      "loss tensor(0.8721, dtype=torch.float64)\n",
      "loss tensor(1.3589, dtype=torch.float64)\n",
      "dev loss: 2.2310405378732767\n",
      "\n",
      "Train Epoch 907\n",
      "-------------------------------\n",
      "loss: 0.487610  [  260/ 2430]\n",
      "Test Epoch 907\n",
      "-------------------------------\n",
      "loss tensor(0.8351, dtype=torch.float64)\n",
      "loss tensor(1.1005, dtype=torch.float64)\n",
      "dev loss: 1.935571063664379\n",
      "\n",
      "Train Epoch 908\n",
      "-------------------------------\n",
      "loss: 0.424022  [  260/ 2430]\n",
      "Test Epoch 908\n",
      "-------------------------------\n",
      "loss tensor(0.8653, dtype=torch.float64)\n",
      "loss tensor(1.0007, dtype=torch.float64)\n",
      "dev loss: 1.8660185576873851\n",
      "\n",
      "Train Epoch 909\n",
      "-------------------------------\n",
      "loss: 0.404168  [  260/ 2430]\n",
      "Test Epoch 909\n",
      "-------------------------------\n",
      "loss tensor(0.8071, dtype=torch.float64)\n",
      "loss tensor(0.9614, dtype=torch.float64)\n",
      "dev loss: 1.7685840901516854\n",
      "\n",
      "Train Epoch 910\n",
      "-------------------------------\n",
      "loss: 0.364622  [  260/ 2430]\n",
      "Test Epoch 910\n",
      "-------------------------------\n",
      "loss tensor(0.8191, dtype=torch.float64)\n",
      "loss tensor(1.3317, dtype=torch.float64)\n",
      "dev loss: 2.1508319019774347\n",
      "\n",
      "Train Epoch 911\n",
      "-------------------------------\n",
      "loss: 0.370105  [  260/ 2430]\n",
      "Test Epoch 911\n",
      "-------------------------------\n",
      "loss tensor(0.8329, dtype=torch.float64)\n",
      "loss tensor(1.0544, dtype=torch.float64)\n",
      "dev loss: 1.887358674512917\n",
      "\n",
      "Train Epoch 912\n",
      "-------------------------------\n",
      "loss: 0.362421  [  260/ 2430]\n",
      "Test Epoch 912\n",
      "-------------------------------\n",
      "loss tensor(0.7980, dtype=torch.float64)\n",
      "loss tensor(0.8807, dtype=torch.float64)\n",
      "dev loss: 1.6786550784624126\n",
      "\n",
      "Train Epoch 913\n",
      "-------------------------------\n",
      "loss: 0.326310  [  260/ 2430]\n",
      "Test Epoch 913\n",
      "-------------------------------\n",
      "loss tensor(0.8922, dtype=torch.float64)\n",
      "loss tensor(1.6286, dtype=torch.float64)\n",
      "dev loss: 2.5207986410910217\n",
      "\n",
      "Train Epoch 914\n",
      "-------------------------------\n",
      "loss: 0.373947  [  260/ 2430]\n",
      "Test Epoch 914\n",
      "-------------------------------\n",
      "loss tensor(0.9042, dtype=torch.float64)\n",
      "loss tensor(0.8165, dtype=torch.float64)\n",
      "dev loss: 1.7207252472128771\n",
      "\n",
      "Train Epoch 915\n",
      "-------------------------------\n",
      "loss: 0.402316  [  260/ 2430]\n",
      "Test Epoch 915\n",
      "-------------------------------\n",
      "loss tensor(0.8473, dtype=torch.float64)\n",
      "loss tensor(1.6033, dtype=torch.float64)\n",
      "dev loss: 2.4505754951294043\n",
      "\n",
      "Train Epoch 916\n",
      "-------------------------------\n",
      "loss: 0.416401  [  260/ 2430]\n",
      "Test Epoch 916\n",
      "-------------------------------\n",
      "loss tensor(0.7913, dtype=torch.float64)\n",
      "loss tensor(0.9832, dtype=torch.float64)\n",
      "dev loss: 1.774517172123288\n",
      "\n",
      "Train Epoch 917\n",
      "-------------------------------\n",
      "loss: 0.292352  [  260/ 2430]\n",
      "Test Epoch 917\n",
      "-------------------------------\n",
      "loss tensor(0.8480, dtype=torch.float64)\n",
      "loss tensor(0.9487, dtype=torch.float64)\n",
      "dev loss: 1.7967179178533705\n",
      "\n",
      "Train Epoch 918\n",
      "-------------------------------\n",
      "loss: 0.316595  [  260/ 2430]\n",
      "Test Epoch 918\n",
      "-------------------------------\n",
      "loss tensor(0.7955, dtype=torch.float64)\n",
      "loss tensor(1.2134, dtype=torch.float64)\n",
      "dev loss: 2.0088484032079243\n",
      "\n",
      "Train Epoch 919\n",
      "-------------------------------\n",
      "loss: 0.373600  [  260/ 2430]\n",
      "Test Epoch 919\n",
      "-------------------------------\n",
      "loss tensor(0.8208, dtype=torch.float64)\n",
      "loss tensor(1.1763, dtype=torch.float64)\n",
      "dev loss: 1.9970959569715472\n",
      "\n",
      "Train Epoch 920\n",
      "-------------------------------\n",
      "loss: 0.344849  [  260/ 2430]\n",
      "Test Epoch 920\n",
      "-------------------------------\n",
      "loss tensor(0.8521, dtype=torch.float64)\n",
      "loss tensor(1.4428, dtype=torch.float64)\n",
      "dev loss: 2.2948948059584673\n",
      "\n",
      "Train Epoch 921\n",
      "-------------------------------\n",
      "loss: 0.364678  [  260/ 2430]\n",
      "Test Epoch 921\n",
      "-------------------------------\n",
      "loss tensor(0.7987, dtype=torch.float64)\n",
      "loss tensor(0.6977, dtype=torch.float64)\n",
      "dev loss: 1.4963946268001442\n",
      "\n",
      "Train Epoch 922\n",
      "-------------------------------\n",
      "loss: 0.330691  [  260/ 2430]\n",
      "Test Epoch 922\n",
      "-------------------------------\n",
      "loss tensor(0.8446, dtype=torch.float64)\n",
      "loss tensor(0.8277, dtype=torch.float64)\n",
      "dev loss: 1.6723005572936889\n",
      "\n",
      "Train Epoch 923\n",
      "-------------------------------\n",
      "loss: 0.351278  [  260/ 2430]\n",
      "Test Epoch 923\n",
      "-------------------------------\n",
      "loss tensor(0.8406, dtype=torch.float64)\n",
      "loss tensor(0.9133, dtype=torch.float64)\n",
      "dev loss: 1.7539338110291025\n",
      "\n",
      "Train Epoch 924\n",
      "-------------------------------\n",
      "loss: 0.327347  [  260/ 2430]\n",
      "Test Epoch 924\n",
      "-------------------------------\n",
      "loss tensor(0.8912, dtype=torch.float64)\n",
      "loss tensor(1.2505, dtype=torch.float64)\n",
      "dev loss: 2.1416487490341134\n",
      "\n",
      "Train Epoch 925\n",
      "-------------------------------\n",
      "loss: 0.451059  [  260/ 2430]\n",
      "Test Epoch 925\n",
      "-------------------------------\n",
      "loss tensor(0.8431, dtype=torch.float64)\n",
      "loss tensor(0.8937, dtype=torch.float64)\n",
      "dev loss: 1.736769274052039\n",
      "\n",
      "Train Epoch 926\n",
      "-------------------------------\n",
      "loss: 0.402904  [  260/ 2430]\n",
      "Test Epoch 926\n",
      "-------------------------------\n",
      "loss tensor(0.7918, dtype=torch.float64)\n",
      "loss tensor(1.1956, dtype=torch.float64)\n",
      "dev loss: 1.9873968302766933\n",
      "\n",
      "Train Epoch 927\n",
      "-------------------------------\n",
      "loss: 0.345863  [  260/ 2430]\n",
      "Test Epoch 927\n",
      "-------------------------------\n",
      "loss tensor(0.9021, dtype=torch.float64)\n",
      "loss tensor(1.5733, dtype=torch.float64)\n",
      "dev loss: 2.4753510072187206\n",
      "\n",
      "Train Epoch 928\n",
      "-------------------------------\n",
      "loss: 0.421055  [  260/ 2430]\n",
      "Test Epoch 928\n",
      "-------------------------------\n",
      "loss tensor(0.9146, dtype=torch.float64)\n",
      "loss tensor(1.0511, dtype=torch.float64)\n",
      "dev loss: 1.965739108970068\n",
      "\n",
      "Train Epoch 929\n",
      "-------------------------------\n",
      "loss: 0.527285  [  260/ 2430]\n",
      "Test Epoch 929\n",
      "-------------------------------\n",
      "loss tensor(0.8471, dtype=torch.float64)\n",
      "loss tensor(1.1865, dtype=torch.float64)\n",
      "dev loss: 2.0335855185062544\n",
      "\n",
      "Train Epoch 930\n",
      "-------------------------------\n",
      "loss: 0.379537  [  260/ 2430]\n",
      "Test Epoch 930\n",
      "-------------------------------\n",
      "loss tensor(0.8835, dtype=torch.float64)\n",
      "loss tensor(1.4252, dtype=torch.float64)\n",
      "dev loss: 2.308681591495021\n",
      "\n",
      "Train Epoch 931\n",
      "-------------------------------\n",
      "loss: 0.444604  [  260/ 2430]\n",
      "Test Epoch 931\n",
      "-------------------------------\n",
      "loss tensor(0.8234, dtype=torch.float64)\n",
      "loss tensor(1.3428, dtype=torch.float64)\n",
      "dev loss: 2.166195778835335\n",
      "\n",
      "Train Epoch 932\n",
      "-------------------------------\n",
      "loss: 0.347230  [  260/ 2430]\n",
      "Test Epoch 932\n",
      "-------------------------------\n",
      "loss tensor(0.7954, dtype=torch.float64)\n",
      "loss tensor(1.2293, dtype=torch.float64)\n",
      "dev loss: 2.0246935120203204\n",
      "\n",
      "Train Epoch 933\n",
      "-------------------------------\n",
      "loss: 0.324509  [  260/ 2430]\n",
      "Test Epoch 933\n",
      "-------------------------------\n",
      "loss tensor(0.8924, dtype=torch.float64)\n",
      "loss tensor(0.9084, dtype=torch.float64)\n",
      "dev loss: 1.8008085055477212\n",
      "\n",
      "Train Epoch 934\n",
      "-------------------------------\n",
      "loss: 0.376539  [  260/ 2430]\n",
      "Test Epoch 934\n",
      "-------------------------------\n",
      "loss tensor(0.7964, dtype=torch.float64)\n",
      "loss tensor(1.1582, dtype=torch.float64)\n",
      "dev loss: 1.954640396898855\n",
      "\n",
      "Train Epoch 935\n",
      "-------------------------------\n",
      "loss: 0.377450  [  260/ 2430]\n",
      "Test Epoch 935\n",
      "-------------------------------\n",
      "loss tensor(0.8259, dtype=torch.float64)\n",
      "loss tensor(1.4586, dtype=torch.float64)\n",
      "dev loss: 2.2844127016935447\n",
      "\n",
      "Train Epoch 936\n",
      "-------------------------------\n",
      "loss: 0.412656  [  260/ 2430]\n",
      "Test Epoch 936\n",
      "-------------------------------\n",
      "loss tensor(0.8290, dtype=torch.float64)\n",
      "loss tensor(1.2082, dtype=torch.float64)\n",
      "dev loss: 2.0371284180193268\n",
      "\n",
      "Train Epoch 937\n",
      "-------------------------------\n",
      "loss: 0.385894  [  260/ 2430]\n",
      "Test Epoch 937\n",
      "-------------------------------\n",
      "loss tensor(0.8040, dtype=torch.float64)\n",
      "loss tensor(1.1452, dtype=torch.float64)\n",
      "dev loss: 1.9492044147920744\n",
      "\n",
      "Train Epoch 938\n",
      "-------------------------------\n",
      "loss: 0.334682  [  260/ 2430]\n",
      "Test Epoch 938\n",
      "-------------------------------\n",
      "loss tensor(0.8457, dtype=torch.float64)\n",
      "loss tensor(0.7434, dtype=torch.float64)\n",
      "dev loss: 1.5891112406567944\n",
      "\n",
      "Train Epoch 939\n",
      "-------------------------------\n",
      "loss: 0.349701  [  260/ 2430]\n",
      "Test Epoch 939\n",
      "-------------------------------\n",
      "loss tensor(1.0566, dtype=torch.float64)\n",
      "loss tensor(0.9156, dtype=torch.float64)\n",
      "dev loss: 1.9721746781832659\n",
      "\n",
      "Train Epoch 940\n",
      "-------------------------------\n",
      "loss: 0.542436  [  260/ 2430]\n",
      "Test Epoch 940\n",
      "-------------------------------\n",
      "loss tensor(1.1635, dtype=torch.float64)\n",
      "loss tensor(0.6880, dtype=torch.float64)\n",
      "dev loss: 1.8514460957888406\n",
      "\n",
      "Train Epoch 941\n",
      "-------------------------------\n",
      "loss: 0.703406  [  260/ 2430]\n",
      "Test Epoch 941\n",
      "-------------------------------\n",
      "loss tensor(0.9750, dtype=torch.float64)\n",
      "loss tensor(0.9634, dtype=torch.float64)\n",
      "dev loss: 1.9384272279462857\n",
      "\n",
      "Train Epoch 942\n",
      "-------------------------------\n",
      "loss: 0.486699  [  260/ 2430]\n",
      "Test Epoch 942\n",
      "-------------------------------\n",
      "loss tensor(0.8986, dtype=torch.float64)\n",
      "loss tensor(0.7851, dtype=torch.float64)\n",
      "dev loss: 1.683674201953917\n",
      "\n",
      "Train Epoch 943\n",
      "-------------------------------\n",
      "loss: 0.373094  [  260/ 2430]\n",
      "Test Epoch 943\n",
      "-------------------------------\n",
      "loss tensor(0.8291, dtype=torch.float64)\n",
      "loss tensor(0.7935, dtype=torch.float64)\n",
      "dev loss: 1.6225624201472488\n",
      "\n",
      "Train Epoch 944\n",
      "-------------------------------\n",
      "loss: 0.399953  [  260/ 2430]\n",
      "Test Epoch 944\n",
      "-------------------------------\n",
      "loss tensor(0.8141, dtype=torch.float64)\n",
      "loss tensor(1.0399, dtype=torch.float64)\n",
      "dev loss: 1.8540203287065506\n",
      "\n",
      "Train Epoch 945\n",
      "-------------------------------\n",
      "loss: 0.345230  [  260/ 2430]\n",
      "Test Epoch 945\n",
      "-------------------------------\n",
      "loss tensor(0.8013, dtype=torch.float64)\n",
      "loss tensor(0.8922, dtype=torch.float64)\n",
      "dev loss: 1.693481411061967\n",
      "\n",
      "Train Epoch 946\n",
      "-------------------------------\n",
      "loss: 0.432121  [  260/ 2430]\n",
      "Test Epoch 946\n",
      "-------------------------------\n",
      "loss tensor(0.8515, dtype=torch.float64)\n",
      "loss tensor(0.9800, dtype=torch.float64)\n",
      "dev loss: 1.831592726496001\n",
      "\n",
      "Train Epoch 947\n",
      "-------------------------------\n",
      "loss: 0.351337  [  260/ 2430]\n",
      "Test Epoch 947\n",
      "-------------------------------\n",
      "loss tensor(0.8335, dtype=torch.float64)\n",
      "loss tensor(0.9617, dtype=torch.float64)\n",
      "dev loss: 1.7952294655384406\n",
      "\n",
      "Train Epoch 948\n",
      "-------------------------------\n",
      "loss: 0.384183  [  260/ 2430]\n",
      "Test Epoch 948\n",
      "-------------------------------\n",
      "loss tensor(0.8821, dtype=torch.float64)\n",
      "loss tensor(0.9464, dtype=torch.float64)\n",
      "dev loss: 1.8284769138555037\n",
      "\n",
      "Train Epoch 949\n",
      "-------------------------------\n",
      "loss: 0.396321  [  260/ 2430]\n",
      "Test Epoch 949\n",
      "-------------------------------\n",
      "loss tensor(0.8278, dtype=torch.float64)\n",
      "loss tensor(0.7517, dtype=torch.float64)\n",
      "dev loss: 1.5794437105334898\n",
      "\n",
      "Train Epoch 950\n",
      "-------------------------------\n",
      "loss: 0.354182  [  260/ 2430]\n",
      "Test Epoch 950\n",
      "-------------------------------\n",
      "loss tensor(0.8412, dtype=torch.float64)\n",
      "loss tensor(0.9784, dtype=torch.float64)\n",
      "dev loss: 1.8196088323812467\n",
      "\n",
      "Train Epoch 951\n",
      "-------------------------------\n",
      "loss: 0.331444  [  260/ 2430]\n",
      "Test Epoch 951\n",
      "-------------------------------\n",
      "loss tensor(0.7663, dtype=torch.float64)\n",
      "loss tensor(1.0527, dtype=torch.float64)\n",
      "dev loss: 1.8189665053315238\n",
      "\n",
      "Train Epoch 952\n",
      "-------------------------------\n",
      "loss: 0.335485  [  260/ 2430]\n",
      "Test Epoch 952\n",
      "-------------------------------\n",
      "loss tensor(0.7848, dtype=torch.float64)\n",
      "loss tensor(1.2812, dtype=torch.float64)\n",
      "dev loss: 2.0660795144818804\n",
      "\n",
      "Train Epoch 953\n",
      "-------------------------------\n",
      "loss: 0.339841  [  260/ 2430]\n",
      "Test Epoch 953\n",
      "-------------------------------\n",
      "loss tensor(0.8977, dtype=torch.float64)\n",
      "loss tensor(0.8748, dtype=torch.float64)\n",
      "dev loss: 1.7725004162234232\n",
      "\n",
      "Train Epoch 954\n",
      "-------------------------------\n",
      "loss: 0.409579  [  260/ 2430]\n",
      "Test Epoch 954\n",
      "-------------------------------\n",
      "loss tensor(0.8939, dtype=torch.float64)\n",
      "loss tensor(0.8532, dtype=torch.float64)\n",
      "dev loss: 1.7471609679273379\n",
      "\n",
      "Train Epoch 955\n",
      "-------------------------------\n",
      "loss: 0.341368  [  260/ 2430]\n",
      "Test Epoch 955\n",
      "-------------------------------\n",
      "loss tensor(0.8184, dtype=torch.float64)\n",
      "loss tensor(0.9055, dtype=torch.float64)\n",
      "dev loss: 1.7239578983736614\n",
      "\n",
      "Train Epoch 956\n",
      "-------------------------------\n",
      "loss: 0.388571  [  260/ 2430]\n",
      "Test Epoch 956\n",
      "-------------------------------\n",
      "loss tensor(0.8226, dtype=torch.float64)\n",
      "loss tensor(1.0534, dtype=torch.float64)\n",
      "dev loss: 1.876064813775291\n",
      "\n",
      "Train Epoch 957\n",
      "-------------------------------\n",
      "loss: 0.362190  [  260/ 2430]\n",
      "Test Epoch 957\n",
      "-------------------------------\n",
      "loss tensor(0.8664, dtype=torch.float64)\n",
      "loss tensor(1.3426, dtype=torch.float64)\n",
      "dev loss: 2.209046895331059\n",
      "\n",
      "Train Epoch 958\n",
      "-------------------------------\n",
      "loss: 0.393599  [  260/ 2430]\n",
      "Test Epoch 958\n",
      "-------------------------------\n",
      "loss tensor(0.8158, dtype=torch.float64)\n",
      "loss tensor(1.1350, dtype=torch.float64)\n",
      "dev loss: 1.950798968414957\n",
      "\n",
      "Train Epoch 959\n",
      "-------------------------------\n",
      "loss: 0.295667  [  260/ 2430]\n",
      "Test Epoch 959\n",
      "-------------------------------\n",
      "loss tensor(0.7731, dtype=torch.float64)\n",
      "loss tensor(1.2183, dtype=torch.float64)\n",
      "dev loss: 1.9914064565466618\n",
      "\n",
      "Train Epoch 960\n",
      "-------------------------------\n",
      "loss: 0.286567  [  260/ 2430]\n",
      "Test Epoch 960\n",
      "-------------------------------\n",
      "loss tensor(0.8190, dtype=torch.float64)\n",
      "loss tensor(1.0741, dtype=torch.float64)\n",
      "dev loss: 1.8931132915243583\n",
      "\n",
      "Train Epoch 961\n",
      "-------------------------------\n",
      "loss: 0.324031  [  260/ 2430]\n",
      "Test Epoch 961\n",
      "-------------------------------\n",
      "loss tensor(0.8843, dtype=torch.float64)\n",
      "loss tensor(1.4387, dtype=torch.float64)\n",
      "dev loss: 2.323019528148466\n",
      "\n",
      "Train Epoch 962\n",
      "-------------------------------\n",
      "loss: 0.367383  [  260/ 2430]\n",
      "Test Epoch 962\n",
      "-------------------------------\n",
      "loss tensor(0.7806, dtype=torch.float64)\n",
      "loss tensor(1.3070, dtype=torch.float64)\n",
      "dev loss: 2.08764447271055\n",
      "\n",
      "Train Epoch 963\n",
      "-------------------------------\n",
      "loss: 0.371004  [  260/ 2430]\n",
      "Test Epoch 963\n",
      "-------------------------------\n",
      "loss tensor(0.7984, dtype=torch.float64)\n",
      "loss tensor(0.7662, dtype=torch.float64)\n",
      "dev loss: 1.564625112922856\n",
      "\n",
      "Train Epoch 964\n",
      "-------------------------------\n",
      "loss: 0.352493  [  260/ 2430]\n",
      "Test Epoch 964\n",
      "-------------------------------\n",
      "loss tensor(0.8624, dtype=torch.float64)\n",
      "loss tensor(0.6962, dtype=torch.float64)\n",
      "dev loss: 1.5585529812164642\n",
      "\n",
      "Train Epoch 965\n",
      "-------------------------------\n",
      "loss: 0.333967  [  260/ 2430]\n",
      "Test Epoch 965\n",
      "-------------------------------\n",
      "loss tensor(0.8048, dtype=torch.float64)\n",
      "loss tensor(1.0733, dtype=torch.float64)\n",
      "dev loss: 1.8780806636591676\n",
      "\n",
      "Train Epoch 966\n",
      "-------------------------------\n",
      "loss: 0.325107  [  260/ 2430]\n",
      "Test Epoch 966\n",
      "-------------------------------\n",
      "loss tensor(0.8082, dtype=torch.float64)\n",
      "loss tensor(0.6798, dtype=torch.float64)\n",
      "dev loss: 1.4879626770269931\n",
      "\n",
      "Train Epoch 967\n",
      "-------------------------------\n",
      "loss: 0.335784  [  260/ 2430]\n",
      "Test Epoch 967\n",
      "-------------------------------\n",
      "loss tensor(0.8086, dtype=torch.float64)\n",
      "loss tensor(1.1144, dtype=torch.float64)\n",
      "dev loss: 1.9230126501575655\n",
      "\n",
      "Train Epoch 968\n",
      "-------------------------------\n",
      "loss: 0.337280  [  260/ 2430]\n",
      "Test Epoch 968\n",
      "-------------------------------\n",
      "loss tensor(0.7989, dtype=torch.float64)\n",
      "loss tensor(0.9502, dtype=torch.float64)\n",
      "dev loss: 1.7491205248336705\n",
      "\n",
      "Train Epoch 969\n",
      "-------------------------------\n",
      "loss: 0.303037  [  260/ 2430]\n",
      "Test Epoch 969\n",
      "-------------------------------\n",
      "loss tensor(0.8653, dtype=torch.float64)\n",
      "loss tensor(1.1490, dtype=torch.float64)\n",
      "dev loss: 2.0142914774163554\n",
      "\n",
      "Train Epoch 970\n",
      "-------------------------------\n",
      "loss: 0.302463  [  260/ 2430]\n",
      "Test Epoch 970\n",
      "-------------------------------\n",
      "loss tensor(0.8555, dtype=torch.float64)\n",
      "loss tensor(0.8298, dtype=torch.float64)\n",
      "dev loss: 1.685284808808897\n",
      "\n",
      "Train Epoch 971\n",
      "-------------------------------\n",
      "loss: 0.351416  [  260/ 2430]\n",
      "Test Epoch 971\n",
      "-------------------------------\n",
      "loss tensor(0.8166, dtype=torch.float64)\n",
      "loss tensor(0.9970, dtype=torch.float64)\n",
      "dev loss: 1.813572268540963\n",
      "\n",
      "Train Epoch 972\n",
      "-------------------------------\n",
      "loss: 0.334447  [  260/ 2430]\n",
      "Test Epoch 972\n",
      "-------------------------------\n",
      "loss tensor(0.7992, dtype=torch.float64)\n",
      "loss tensor(1.1171, dtype=torch.float64)\n",
      "dev loss: 1.9162958334508415\n",
      "\n",
      "Train Epoch 973\n",
      "-------------------------------\n",
      "loss: 0.310691  [  260/ 2430]\n",
      "Test Epoch 973\n",
      "-------------------------------\n",
      "loss tensor(0.8640, dtype=torch.float64)\n",
      "loss tensor(1.0554, dtype=torch.float64)\n",
      "dev loss: 1.9193613267964482\n",
      "\n",
      "Train Epoch 974\n",
      "-------------------------------\n",
      "loss: 0.346789  [  260/ 2430]\n",
      "Test Epoch 974\n",
      "-------------------------------\n",
      "loss tensor(0.7996, dtype=torch.float64)\n",
      "loss tensor(0.7352, dtype=torch.float64)\n",
      "dev loss: 1.5347641945889194\n",
      "\n",
      "Train Epoch 975\n",
      "-------------------------------\n",
      "loss: 0.357160  [  260/ 2430]\n",
      "Test Epoch 975\n",
      "-------------------------------\n",
      "loss tensor(0.9110, dtype=torch.float64)\n",
      "loss tensor(1.1862, dtype=torch.float64)\n",
      "dev loss: 2.097132756034185\n",
      "\n",
      "Train Epoch 976\n",
      "-------------------------------\n",
      "loss: 0.438304  [  260/ 2430]\n",
      "Test Epoch 976\n",
      "-------------------------------\n",
      "loss tensor(0.8081, dtype=torch.float64)\n",
      "loss tensor(1.0219, dtype=torch.float64)\n",
      "dev loss: 1.83000194235944\n",
      "\n",
      "Train Epoch 977\n",
      "-------------------------------\n",
      "loss: 0.319674  [  260/ 2430]\n",
      "Test Epoch 977\n",
      "-------------------------------\n",
      "loss tensor(0.7844, dtype=torch.float64)\n",
      "loss tensor(1.0537, dtype=torch.float64)\n",
      "dev loss: 1.8380936356219686\n",
      "\n",
      "Train Epoch 978\n",
      "-------------------------------\n",
      "loss: 0.321801  [  260/ 2430]\n",
      "Test Epoch 978\n",
      "-------------------------------\n",
      "loss tensor(0.9133, dtype=torch.float64)\n",
      "loss tensor(1.5699, dtype=torch.float64)\n",
      "dev loss: 2.4832605598761504\n",
      "\n",
      "Train Epoch 979\n",
      "-------------------------------\n",
      "loss: 0.399975  [  260/ 2430]\n",
      "Test Epoch 979\n",
      "-------------------------------\n",
      "loss tensor(0.8663, dtype=torch.float64)\n",
      "loss tensor(1.3301, dtype=torch.float64)\n",
      "dev loss: 2.19642581690506\n",
      "\n",
      "Train Epoch 980\n",
      "-------------------------------\n",
      "loss: 0.414655  [  260/ 2430]\n",
      "Test Epoch 980\n",
      "-------------------------------\n",
      "loss tensor(0.9290, dtype=torch.float64)\n",
      "loss tensor(0.8876, dtype=torch.float64)\n",
      "dev loss: 1.8165931461031808\n",
      "\n",
      "Train Epoch 981\n",
      "-------------------------------\n",
      "loss: 0.395558  [  260/ 2430]\n",
      "Test Epoch 981\n",
      "-------------------------------\n",
      "loss tensor(0.8403, dtype=torch.float64)\n",
      "loss tensor(1.2461, dtype=torch.float64)\n",
      "dev loss: 2.0864584805105775\n",
      "\n",
      "Train Epoch 982\n",
      "-------------------------------\n",
      "loss: 0.344684  [  260/ 2430]\n",
      "Test Epoch 982\n",
      "-------------------------------\n",
      "loss tensor(0.8164, dtype=torch.float64)\n",
      "loss tensor(0.9811, dtype=torch.float64)\n",
      "dev loss: 1.7974888974854348\n",
      "\n",
      "Train Epoch 983\n",
      "-------------------------------\n",
      "loss: 0.397563  [  260/ 2430]\n",
      "Test Epoch 983\n",
      "-------------------------------\n",
      "loss tensor(0.8292, dtype=torch.float64)\n",
      "loss tensor(1.4727, dtype=torch.float64)\n",
      "dev loss: 2.3018857143763234\n",
      "\n",
      "Train Epoch 984\n",
      "-------------------------------\n",
      "loss: 0.296969  [  260/ 2430]\n",
      "Test Epoch 984\n",
      "-------------------------------\n",
      "loss tensor(0.8271, dtype=torch.float64)\n",
      "loss tensor(1.2091, dtype=torch.float64)\n",
      "dev loss: 2.036137085203094\n",
      "\n",
      "Train Epoch 985\n",
      "-------------------------------\n",
      "loss: 0.303675  [  260/ 2430]\n",
      "Test Epoch 985\n",
      "-------------------------------\n",
      "loss tensor(0.8217, dtype=torch.float64)\n",
      "loss tensor(1.1667, dtype=torch.float64)\n",
      "dev loss: 1.9884489174362026\n",
      "\n",
      "Train Epoch 986\n",
      "-------------------------------\n",
      "loss: 0.296297  [  260/ 2430]\n",
      "Test Epoch 986\n",
      "-------------------------------\n",
      "loss tensor(0.8234, dtype=torch.float64)\n",
      "loss tensor(1.1799, dtype=torch.float64)\n",
      "dev loss: 2.0033102912145626\n",
      "\n",
      "Train Epoch 987\n",
      "-------------------------------\n",
      "loss: 0.295967  [  260/ 2430]\n",
      "Test Epoch 987\n",
      "-------------------------------\n",
      "loss tensor(0.8137, dtype=torch.float64)\n",
      "loss tensor(0.9087, dtype=torch.float64)\n",
      "dev loss: 1.7223891902289554\n",
      "\n",
      "Train Epoch 988\n",
      "-------------------------------\n",
      "loss: 0.323188  [  260/ 2430]\n",
      "Test Epoch 988\n",
      "-------------------------------\n",
      "loss tensor(0.8099, dtype=torch.float64)\n",
      "loss tensor(1.3713, dtype=torch.float64)\n",
      "dev loss: 2.1812077280679674\n",
      "\n",
      "Train Epoch 989\n",
      "-------------------------------\n",
      "loss: 0.319665  [  260/ 2430]\n",
      "Test Epoch 989\n",
      "-------------------------------\n",
      "loss tensor(0.8720, dtype=torch.float64)\n",
      "loss tensor(1.2673, dtype=torch.float64)\n",
      "dev loss: 2.1392751612821814\n",
      "\n",
      "Train Epoch 990\n",
      "-------------------------------\n",
      "loss: 0.395587  [  260/ 2430]\n",
      "Test Epoch 990\n",
      "-------------------------------\n",
      "loss tensor(0.8263, dtype=torch.float64)\n",
      "loss tensor(1.4875, dtype=torch.float64)\n",
      "dev loss: 2.3137650335445947\n",
      "\n",
      "Train Epoch 991\n",
      "-------------------------------\n",
      "loss: 0.339025  [  260/ 2430]\n",
      "Test Epoch 991\n",
      "-------------------------------\n",
      "loss tensor(0.9520, dtype=torch.float64)\n",
      "loss tensor(1.8608, dtype=torch.float64)\n",
      "dev loss: 2.812771660573432\n",
      "\n",
      "Train Epoch 992\n",
      "-------------------------------\n",
      "loss: 0.428740  [  260/ 2430]\n",
      "Test Epoch 992\n",
      "-------------------------------\n",
      "loss tensor(0.8001, dtype=torch.float64)\n",
      "loss tensor(1.0459, dtype=torch.float64)\n",
      "dev loss: 1.8460142327770472\n",
      "\n",
      "Train Epoch 993\n",
      "-------------------------------\n",
      "loss: 0.335783  [  260/ 2430]\n",
      "Test Epoch 993\n",
      "-------------------------------\n",
      "loss tensor(0.7887, dtype=torch.float64)\n",
      "loss tensor(1.5665, dtype=torch.float64)\n",
      "dev loss: 2.3552220773144192\n",
      "\n",
      "Train Epoch 994\n",
      "-------------------------------\n",
      "loss: 0.369651  [  260/ 2430]\n",
      "Test Epoch 994\n",
      "-------------------------------\n",
      "loss tensor(0.8183, dtype=torch.float64)\n",
      "loss tensor(1.3013, dtype=torch.float64)\n",
      "dev loss: 2.119604855366199\n",
      "\n",
      "Train Epoch 995\n",
      "-------------------------------\n",
      "loss: 0.338995  [  260/ 2430]\n",
      "Test Epoch 995\n",
      "-------------------------------\n",
      "loss tensor(0.8557, dtype=torch.float64)\n",
      "loss tensor(1.2750, dtype=torch.float64)\n",
      "dev loss: 2.130729846772934\n",
      "\n",
      "Train Epoch 996\n",
      "-------------------------------\n",
      "loss: 0.371751  [  260/ 2430]\n",
      "Test Epoch 996\n",
      "-------------------------------\n",
      "loss tensor(0.8081, dtype=torch.float64)\n",
      "loss tensor(1.2824, dtype=torch.float64)\n",
      "dev loss: 2.0904243117206027\n",
      "\n",
      "Train Epoch 997\n",
      "-------------------------------\n",
      "loss: 0.248465  [  260/ 2430]\n",
      "Test Epoch 997\n",
      "-------------------------------\n",
      "loss tensor(0.8331, dtype=torch.float64)\n",
      "loss tensor(0.9693, dtype=torch.float64)\n",
      "dev loss: 1.8024188564932841\n",
      "\n",
      "Train Epoch 998\n",
      "-------------------------------\n",
      "loss: 0.319427  [  260/ 2430]\n",
      "Test Epoch 998\n",
      "-------------------------------\n",
      "loss tensor(0.8093, dtype=torch.float64)\n",
      "loss tensor(0.9493, dtype=torch.float64)\n",
      "dev loss: 1.7585441410975862\n",
      "\n",
      "Train Epoch 999\n",
      "-------------------------------\n",
      "loss: 0.343131  [  260/ 2430]\n",
      "Test Epoch 999\n",
      "-------------------------------\n",
      "loss tensor(0.8555, dtype=torch.float64)\n",
      "loss tensor(1.0995, dtype=torch.float64)\n",
      "dev loss: 1.954942058649979\n",
      "\n",
      "Train Epoch 1000\n",
      "-------------------------------\n",
      "loss: 0.327797  [  260/ 2430]\n",
      "Test Epoch 1000\n",
      "-------------------------------\n",
      "loss tensor(0.8471, dtype=torch.float64)\n",
      "loss tensor(1.1956, dtype=torch.float64)\n",
      "dev loss: 2.042712170413284\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([19.8329,  3.5950,  4.0446, 11.0046,  2.6423, 28.6713, 25.0947,  6.0935,\n",
      "        11.9987, 11.7236, 18.1084, 21.6119, 22.2126, 15.5525, 21.1969, 19.7749,\n",
      "        28.4910, 10.0772, 22.9195,  2.7317,  5.7707, 23.2723, 17.4264, 14.4493,\n",
      "        12.7581, 26.5767, 25.5381, 26.4109,  7.7265, 19.7250, 10.2927,  6.6725,\n",
      "        27.0511, 10.9815, 20.5625, 31.1810, 13.5000,  6.7966, 16.0915,  9.9410,\n",
      "        16.1380,  4.1366,  8.4543, 16.8537,  6.6200, 16.8814, 15.3704,  3.0619,\n",
      "         5.7782, 16.6228, 16.7948, 16.0238, 12.3808,  8.2918, 13.2355, 29.8763,\n",
      "         9.8237, 16.1679, 32.3603, 11.2694, 20.2873, 21.5657, 25.5708, 22.5331,\n",
      "        18.2208,  4.7393, 13.7251, 25.0564, 33.5658, 17.2657, 19.8978, 17.1569,\n",
      "        13.8790, 26.1817, 11.5808,  5.0159, 19.7328, 13.9221,  7.6991,  7.0698,\n",
      "        17.8865, 10.6967, 12.1355, 15.0963, 11.6311, 16.9550, 23.7377, 10.9401,\n",
      "         5.7538, 17.4222, 28.0004, 34.7108, 17.8701,  5.4245, 12.0813, 11.5603,\n",
      "         9.6313, 22.3074, 13.2399, 12.2157,  7.5949,  8.3302, 15.2200, 12.5579,\n",
      "         4.7114, 29.9376, 28.6942, 11.3730, 20.0384,  8.9243, 13.8016,  6.0917,\n",
      "        20.4588, 24.4572, 15.2117, 10.5842,  9.4468, 29.6497, 19.8741, 11.8295,\n",
      "        12.7289, 23.4383,  9.3547, 19.9770, 31.5521, 13.4606, 12.8160,  6.7679,\n",
      "         3.0227, 10.3313, 17.7217, 15.0755, 13.2618, 16.6859, 23.7282, 19.2446,\n",
      "        22.3287,  5.2780, 16.9109, 34.4217, 24.1454, 18.8056, 24.7685, 26.0500,\n",
      "        11.9985, 35.5090,  7.1649, 12.4424, 20.8228, 17.0660, 20.8798,  8.7199,\n",
      "        20.5693,  4.7540, 33.5324, 17.1725, 17.4187, 26.6372, 18.5443, 17.0914,\n",
      "        18.0056, 23.8152, 12.1145, 15.7050, 15.4047, 25.3383, 28.3644, 16.7606,\n",
      "        28.6473, 17.5224, 12.5090, 12.6759, 25.3267, 39.1731, 21.2844,  5.2201,\n",
      "         8.8739, 33.6682, 29.2196, 23.8643, 17.8734, 23.6952, 26.3248,  9.7777,\n",
      "        19.5391,  8.3472, 20.5492,  4.6616, 12.9412, 26.6851,  3.8536, 22.9405,\n",
      "        24.7572,  7.9152, 12.6668, 30.6534,  8.8085, 26.7635, 10.8072, 19.0782,\n",
      "        23.9794, 15.6458, 22.7088, 14.1626, 26.7445, 21.2841, 14.7906, 27.7046,\n",
      "        17.8991,  8.1889,  9.7255, 16.4987, 29.9058,  5.8194, 11.8548, 15.7165,\n",
      "         8.1450, 14.2748, 11.1108, 10.4914, 15.5681,  9.6963, 15.9142, 13.5965,\n",
      "        23.8733, 21.8884, 15.0861,  6.9409, 11.4609, 31.8925, 15.8498,  6.5235,\n",
      "        14.2391, 21.2064,  9.2581, 33.7380, 19.8515, 20.4578, 20.5525, 31.0747,\n",
      "        12.0333, 27.4824, 27.5951, 29.0308, 13.6537, 10.5406, 15.3983, 27.0811,\n",
      "        20.1647, 11.7661, 22.4426, 17.1069, 20.4737, 10.9306, 12.5657,  9.9812,\n",
      "         6.8722, 18.6930, 15.3483,  6.6054, 13.3912,  2.6336,  7.0346, 21.7366,\n",
      "        14.8238,  7.3641, 24.7212, 25.3345,  7.0580, 13.2013, 15.3452,  8.5857,\n",
      "        21.7916, 11.1463,  7.8361, 17.0492, 20.2561,  8.3845, 15.1318, 17.4111,\n",
      "        20.8504, 20.9054, 17.4755, 24.5590, 18.3181, 21.6302, 21.8008, 28.9230,\n",
      "         6.3136, 20.6693, 13.8356, 17.0349,  3.9112,  3.1464, 21.8242, 15.2954,\n",
      "         5.8100, 23.4713, 10.7348, 10.9654, 11.9227, 21.5051, 11.2353, 18.2389,\n",
      "        13.8020, 19.0083, 23.7674, 31.4964, 12.3580, 13.3804,  9.9582, 25.0642,\n",
      "         9.5649, 22.6170, 17.4942, 15.4749, 29.6832, 19.4915, 18.9614, 15.4487,\n",
      "         2.8860,  6.2743, 27.0274, 18.5544,  8.1582, 19.5258,  3.4261, 24.4000,\n",
      "        18.4810, 25.0952, 10.2822, 25.4973, 13.1774,  5.7991, 13.3967, 12.7063,\n",
      "        24.8306,  9.7759, 10.3956, 21.7974, 14.3633, 10.2837, 21.4691, 15.0241,\n",
      "         4.1510, 17.9685, 27.4761, 22.6452, 20.8674, 16.5891,  9.3991, 24.5724,\n",
      "        18.4861, 14.1336,  5.8390, 18.3279, 22.7640, 16.9528, 22.3188, 15.5019,\n",
      "         5.6635, 18.8618,  8.3543, 13.5129, 18.1267, 33.3795, 16.8805, 17.1942,\n",
      "         6.0497, 14.9018, 16.3127, 15.8379, 17.7701, 11.2811,  3.4842, 14.6255,\n",
      "        10.4861,  9.8343, 16.7011, 20.2500, 26.3341, 14.6875, 10.6455, 20.9623,\n",
      "         5.3788, 15.4295, 17.0897, 16.8211, 15.3547, 16.9082, 29.6249, 12.0007,\n",
      "        28.5793, 17.6639,  6.6146, 14.7618, 28.0998, 18.9274, 22.5844, 13.5213,\n",
      "        16.4956, 19.2354,  6.3880, 30.8315, 32.6126, 11.8998, 10.0706, 24.4694,\n",
      "        12.5607, 16.8993, 20.4522,  9.7232, 24.5981, 12.1125, 11.2018, 18.6642,\n",
      "        20.4602, 24.3296, 26.2195,  9.8556, 14.1931,  4.3418,  7.9981, 11.5498,\n",
      "        21.7215, 14.5410, 17.8703,  9.5730, 31.3194, 22.7805, 11.1732, 19.8198,\n",
      "        16.0746, 13.1072,  5.4297, 21.7149, 33.4680, 13.8499,  3.0665,  3.3210,\n",
      "         9.8706,  6.4096, 20.7189, 19.9020, 20.7141,  3.0377,  2.8156, 13.9756,\n",
      "        24.7440, 23.3312, 16.1340,  8.4011, 27.4993, 14.8590, 11.9184,  9.2353,\n",
      "        24.9545, 17.0926, 20.6339,  6.4561, 22.1929,  9.5408, 17.0754, 16.5025,\n",
      "         8.1379, 10.0352,  9.4891,  6.6520, 11.0833, 24.2325, 18.4337, 12.4259,\n",
      "        24.3120, 11.3291, 14.5671,  5.6962, 12.5469, 18.2673, 22.1062, 22.2990,\n",
      "        23.2918, 21.0323, 26.2525, 20.7886, 26.6818,  6.6298, 18.6339, 16.5974,\n",
      "        23.0846,  9.0922, 12.9589, 22.0067, 18.4851, 15.4459, 17.2184, 16.0520,\n",
      "        15.5300,  5.5841,  4.2230, 12.4659, 15.1842,  5.8816, 19.2975, 12.3756,\n",
      "        23.7898, 25.4494, 22.0726, 10.9932, 10.2708, 16.8859, 18.1475, 14.3597,\n",
      "        19.1192,  8.6372, 25.2788, 13.3348, 13.4231, 13.4156, 14.1982, 29.0686,\n",
      "        26.2629, 16.0197, 17.6479, 22.7095, 11.9626, 31.9913,  4.2879, 17.3665,\n",
      "        16.1615,  8.9612, 16.7427, 10.9186, 11.8119, 13.0132, 22.5352,  4.0744,\n",
      "        25.4591,  9.9244, 23.6997, 15.5587, 40.3084, 14.8051, 19.9075,  4.9437,\n",
      "         9.6444, 10.2537,  7.2412, 24.6858, 20.7366, 12.9317, 12.3872,  6.0441,\n",
      "        20.1705, 28.4552, 22.3996,  5.7252, 14.3509, 16.0794, 25.0058, 17.6241,\n",
      "        11.0297, 12.7291, 23.2626, 19.2915, 24.0970, 11.8410, 22.6559, 25.9533,\n",
      "         4.8943, 25.8511, 14.1736, 15.6084, 27.5521, 24.0880,  4.5590, 19.0236,\n",
      "        24.6130, 34.7021, 18.7282, 20.1936, 12.3754,  9.6682, 14.4162, 12.3826,\n",
      "        16.6807,  9.5954, 32.3556, 12.0333,  7.3423, 11.0199, 10.9487, 22.0649,\n",
      "        20.9134, 32.5521, 29.6947, 31.7593,  9.8216, 14.3939, 23.3856, 24.6341,\n",
      "         6.5561,  8.6818, 18.4281, 17.6210, 19.1645, 22.4081, 17.4722, 28.4970,\n",
      "        14.7647, 22.1625,  5.2437, 15.9064, 11.2879, 27.1043, 23.9432, 10.2477,\n",
      "        24.3733, 12.8411, 19.7307, 11.5580,  8.5654, 14.3008,  9.9216, 22.7996,\n",
      "        11.3015, 27.6839, 14.0439, 16.7474, 18.7655, 10.2283, 28.9160, 18.2949,\n",
      "        28.9742, 25.4952, 19.9567, 10.4701, 17.9999, 11.1930, 19.6511, 20.3854,\n",
      "         9.6957, 11.1914, 25.8913, 30.7346, 22.5590, 29.7774,  7.2446, 24.3635,\n",
      "         4.2476, 12.6685, 15.0713, 17.9181, 11.5525, 12.6709, 13.6668, 15.7297,\n",
      "        14.0399, 30.0395, 12.1355,  5.0191, 27.1545, 26.0404,  9.3086, 11.5434,\n",
      "        10.7270,  5.2917, 24.6655, 10.1910, 22.2933, 22.4255, 13.1495,  7.0117,\n",
      "         8.0428, 15.3186,  3.1677, 13.8356, 23.0694, 24.6632, 15.9241, 11.4108,\n",
      "        15.9093, 11.0107, 16.7912, 20.3337,  2.8786, 26.9243, 17.8198, 14.8734,\n",
      "         5.7170, 11.8225, 17.2184,  7.1645, 15.1849,  4.5600,  2.9686, 10.7671,\n",
      "        23.2572, 30.5927, 27.8517,  5.1116,  6.6553,  4.8813, 18.7972, 15.3140,\n",
      "        28.1711,  5.8635, 16.6836,  6.9130, 11.5577, 13.2888, 20.6345,  8.9540,\n",
      "        24.0952, 11.1639,  4.9195,  2.9004,  9.1446, 21.2115, 12.2511, 21.1374,\n",
      "         7.5620,  4.2903, 20.3663, 25.5490,  6.7848, 18.4077,  4.9104, 13.6758,\n",
      "         5.7921, 17.0787,  7.0870,  9.6609, 21.7182,  6.8447, 20.4501, 15.0093,\n",
      "         5.8356, 21.5590,  6.3876, 23.6405, 17.0551, 17.3634, 11.2252, 24.6769,\n",
      "        14.1840, 13.6864, 27.3135, 29.8502, 16.5818,  6.9767,  8.1741, 23.7897,\n",
      "         5.4731, 10.4249, 20.1398, 20.7177, 19.7563, 16.4277, 11.5240,  9.8003,\n",
      "        15.5668, 11.0030,  5.3316, 24.4435, 29.6337, 21.5874, 20.1015,  7.2025,\n",
      "        13.0038, 13.5954, 18.9862, 13.0732, 25.0000, 19.8423,  4.5803, 10.7076,\n",
      "        23.6176, 21.8951, 18.2532, 12.6798, 22.4720, 31.6039, 17.8521, 20.4478,\n",
      "         9.5730,  5.3013,  5.4647, 21.2807, 22.5146, 16.0678, 20.0217,  6.7280,\n",
      "        11.6907, 15.6124, 11.0282, 19.6454, 13.9726, 13.0364,  7.5015, 15.8661,\n",
      "        13.4404,  9.5765, 10.1574, 12.9150, 11.8399, 22.6556, 11.1542, 20.8069,\n",
      "        20.5492, 27.8916, 15.0476, 18.3249, 14.3403, 16.6980,  7.6688,  3.0056,\n",
      "         5.8924, 16.9948, 28.6144,  8.3977, 23.7785, 15.8038, 16.4340, 19.4647,\n",
      "        24.0521,  4.9842,  9.7785, 19.9788, 20.6737, 15.8149, 17.8934,  5.4032,\n",
      "        13.4737, 11.4371, 14.1277, 27.4666, 12.9710, 17.3581, 21.8161, 18.4587,\n",
      "        19.8406, 37.1047, 31.7340, 18.9733, 19.7874,  8.7559, 14.3438, 17.9624,\n",
      "         8.2751, 14.1827,  5.0702,  5.8871,  6.5740,  9.3172,  6.6732, 23.9717,\n",
      "        23.2139, 21.2096,  8.7966, 13.9150, 21.0776, 20.3874, 21.4332, 22.2418,\n",
      "        13.0736,  6.1252,  8.8817,  9.3002, 24.4483, 23.8299,  7.9492, 12.3188,\n",
      "        15.9047, 32.9949, 27.6876, 18.2878, 18.2047, 11.5198, 20.5269, 16.7305,\n",
      "         8.1939, 14.0868,  9.7978, 15.4492, 27.5351, 15.6130, 29.1445, 19.7948,\n",
      "        13.8369,  5.0114, 32.0965, 25.4841, 19.2904], dtype=torch.float64)\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(93, 64),\n",
    "            # nn.Linear(42, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "        )\n",
    "        # self.linear = nn.Linear(94, 1)\n",
    "        self.double()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        # logits = self.linear(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()\n",
    "print(model)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "loss_record = {\"train\": [], \"dev\": []}\n",
    "pred_record = []\n",
    "target_record = []\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X, y\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        pred = pred.squeeze()\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss_record[\"train\"].append(loss.item())\n",
    "\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "def dev(dataloader, model, loss_fn):\n",
    "    global pred_record, target_record\n",
    "\n",
    "    pred_record = []\n",
    "    target_record = []\n",
    "\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            pred = pred.squeeze()\n",
    "            loss = loss_fn(pred, y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            loss_record[\"dev\"].append(loss.item())\n",
    "            \n",
    "            pred_record.append(pred)\n",
    "            target_record.append(y)\n",
    "            print(\"loss\", loss)\n",
    "\n",
    "    pred_record = torch.cat(pred_record, dim=0).numpy()\n",
    "    target_record = torch.cat(target_record, dim=0).numpy()\n",
    "    \n",
    "    return test_loss     \n",
    "            # correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    # test_loss /= num_batches\n",
    "    # correct /= size\n",
    "    # print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "def test(dataloader, model):\n",
    "    del model\n",
    "    model = NeuralNetwork()\n",
    "    ckpt = torch.load(\"model.pth\")\n",
    "    model.load_state_dict(ckpt)\n",
    "    model.eval()\n",
    "\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for X in dataloader:\n",
    "            pred = model(X)\n",
    "            pred = pred.squeeze()\n",
    "\n",
    "            preds.append(pred)\n",
    "    \n",
    "    preds = torch.cat(preds, dim=0)\n",
    "    print(preds)\n",
    "    save_result(preds)\n",
    "\n",
    "epochs = 1000\n",
    "min_loss = 1000\n",
    "for t in range(epochs):\n",
    "    print(f\"Train Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "\n",
    "    print(f\"Test Epoch {t+1}\\n-------------------------------\")\n",
    "    dev_loss = dev(dev_dataloader, model, loss_fn)\n",
    "    if dev_loss < min_loss:\n",
    "        min_loss = dev_loss\n",
    "        torch.save(model.state_dict(), \"model.pth\")\n",
    "\n",
    "    print(f\"dev loss: {dev_loss}\\n\")\n",
    "\n",
    "plot_learning_curve(loss_record, \"\")\n",
    "plot_pred(pred_record, target_record)\n",
    "\n",
    "test(test_dataloader, model)\n",
    "    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 3]\n",
      " [4 5]]\n",
      "[[2 3]\n",
      " [4 5]]\n"
     ]
    }
   ],
   "source": [
    "m = np.array([[3,4],[5,6]])\n",
    "n = np.array([1,1])\n",
    "o = np.array([[1],[1]])\n",
    "\n",
    "print(m - n)\n",
    "print(m - o)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False,  True, False,  ..., False, False, False],\n",
      "        [False,  True, False,  ...,  True, False, False],\n",
      "        [ True, False, False,  ..., False,  True, False],\n",
      "        ...,\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ...,  True, False, False],\n",
      "        [False, False, False,  ..., False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "def selected_features():\n",
    "    model = NeuralNetwork()\n",
    "    ckpt = torch.load(\"model.pth\")\n",
    "    model.load_state_dict(ckpt)\n",
    "\n",
    "    threshold = 0.1  # 设置阈值\n",
    "\n",
    "    # model.eval()\n",
    "    # for param in model.parameters():\n",
    "    #     print(param)\n",
    "\n",
    "    # for idx, module in enumerate(model):\n",
    "    #     print(f\"Module {idx}:\")\n",
    "    #     for param_tensor in module.parameters():\n",
    "    #         print(param_tensor)\n",
    "    weights = model.linear_relu_stack[0].weight.data\n",
    "    \n",
    "    selected_features = weights.abs() > threshold  # 选择权重系数大于某个阈值的特征\n",
    "    print(selected_features)\n",
    "    # selected_features = selected_features.sum(dim=1) > 0  # 选择至少有一个权重系数大于阈值的特征\n",
    "\n",
    "selected_features()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
