{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/github/ga642381/ML2021-Spring/blob/main/HW01/HW01.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "# loss_record = {\"train\": [338.8324902564143, 300.1206629882458, 309.41258023480356, 145.69407738887747, 289.8134118404678, 291.6760574651674, 334.3836256452923, 315.8737479800885, 209.23943781182967, 236.81603356977706, 189.86549218773973, 160.52760421077537, 280.0016513158014, 267.7548507515039, 278.15674118761734, 166.38160025578884, 319.30744387980326, 204.28195872446082, 181.99305093328454, 218.22128231221092, 162.26057086959662, 64.73518166531008, 110.40258774163587, 208.9251678678948, 182.64052071440454, 138.2169423235864, 48.413575080988345, 37.013469786220625, 62.99520617263815, 69.15946845151676, 110.16284090167868, 41.1942962857465, 46.32759518000252, 142.99812931242602, 58.73162433100042, 55.741664041161016, 49.190149569387515, 26.404623853955812, 18.912452561765697, 17.669911249111045, 20.72781174209063, 19.919526969337554, 39.410228585289396, 22.26082406120151, 40.471893575253404, 17.94784264800843, 30.74124783206616, 25.516015454757937, 84.51766995716228, 12.456371135225762, 14.916355149908838, 39.8936788554534, 32.389191253323276, 90.53185351897793, 46.0995767945348, 18.168463603125815, 10.794420508844489, 10.18186166360786, 10.259098709170187, 17.319344481075102, 37.41348503515055, 15.208015544289328, 24.550035608751806, 6.0516923346327705, 13.597652870867075, 8.548588873450464, 20.203206332631538, 34.185418266576434, 101.15420361278828, 43.30534397889758, 12.104288293881314, 6.9079855555659435, 11.631063355645985, 34.59621762418044, 17.665342942023553, 46.71013883192069, 17.886925583594394, 32.29982244201045, 9.346324818388478, 5.033926815280658, 7.9935383060262915, 5.315085720233034, 12.088883773529194, 33.44042677147008, 101.23013100210238, 33.60359628756596, 17.060327134306736, 5.070760844804232, 4.829672690335647, 6.5203425038814755, 7.679362754432013, 12.12700061009711, 9.146226402790425, 13.214848511170244, 22.076412487453517, 28.800103605506905, 4.8163071459594065, 4.4151502815184624, 4.593426453944634, 3.3734777140873513, 5.836599846037702, 6.078169516505016, 6.185683332588629, 12.68662644699382, 14.412390848977656, 37.649593458904825, 4.953026723853803, 3.78695703619407, 4.778810923145538, 3.2315823358099633, 5.48613161230946, 5.647293319228187, 4.706884808532317, 7.868641051882097, 10.299799199530614, 22.948683678946725, 3.149560684151326, 2.556178629706999, 2.553797902547779, 2.6397642984473277, 2.338417790399397, 3.032053577197205, 3.298178388926864, 3.6566909491310375, 2.7725958726210846, 2.19557134095202, 2.9903543561589805, 2.2274606042966716, 2.523463354915582, 2.9741672586312546, 3.3500289798218064, 4.018673068248991, 6.862403806092638, 14.691688853633618, 10.797449834068814, 22.54570148673751, 12.63021488214626, 31.280814934741993, 4.855538566256259, 6.19676942019173, 4.947974120020652, 7.144830023742216, 7.020977280273048, 16.829164625733963, 9.20165889591547, 18.767181840816615, 5.231822188083326, 4.312841907055251, 2.756975032879118, 3.00673263163634, 3.0690577547640467, 3.389446699092436, 2.8037595907918895, 2.4709425238820595, 1.6854016864923587, 2.2027052441748824, 2.338260172502632, 3.2526473142446197, 2.824131940401834, 2.3709247179451243, 1.8106800730126524, 2.4727933867050207, 2.280975189310591, 2.2806716723283316, 3.1487829412189736, 3.6615328175418593, 4.412943561715539, 5.814765116371505, 14.395957618379036, 12.411907185991561, 12.067936234966893, 4.4370873856858815, 2.981066989823312, 6.575822140698181, 4.992138071069727, 16.62753141240558, 19.67860888432716, 40.24788501500451, 6.820352192342027, 2.762915425671733, 4.870327421324715, 11.766566599650009, 16.64568249378288, 7.5187213829234025, 13.996754709195127, 36.9150145415871, 3.583460992957627, 2.2308804695987106, 2.5937633560009927, 3.402988703972714, 3.009843919162161, 3.554928753101159, 2.902871415878872, 1.6784578599706417, 1.9035268578268174, 1.4994215683164525, 2.212154976063716, 2.913067086973258, 2.89644793815801, 2.805163547413807, 3.1978842774976277, 3.3998559990996244, 2.408560725122295, 3.610500604296875, 5.76626710270368, 8.828009039672782, 5.903157650896437, 8.808886389821609, 7.4998162288074255, 32.316752056491474, 15.56654417585118, 31.55867070324998, 7.509705103686097, 9.680599796916788, 8.35078297009555, 15.966606762975719, 5.950185767705416, 12.608185701285569, 3.7232026981107817, 3.53089177187456, 3.5547475499944596, 3.663786270000208, 4.126064182256027, 5.174231558656941, 4.4237306418925355, 5.433056584131636, 3.470354543924751, 5.164539924347686, 4.233809537323239, 8.391455056756438, 6.690663208880638, 10.474103741852897, 1.613650192436233, 3.0146048313622824, 4.024272786458019, 2.7060164577500605, 1.8123976088056506, 1.5907337861321715, 2.203556214319332, 1.654234966566828, 1.766649085333844, 1.9679593495350594, 1.665219140202436, 1.9885870851142515, 2.3917139556634197, 1.999903891801406, 2.671947996203217, 3.0502915927220915, 3.942301594004851, 4.011532869583377, 3.162243843949412, 1.3468152595462952, 2.1681200419674074, 3.3878347956910284, 5.527790374441944, 10.936712649144898, 28.811973092317295, 4.538650928494108, 8.363785948562608, 2.9295432599225384, 2.646880741454667, 2.1735672111357935, 2.766817348539472, 2.8717450965745255, 2.9330304933989435, 3.318409123189922, 4.806196543367503, 1.9522280632833469, 1.3708521585762732, 1.829975849410967, 1.407096709547014, 2.196378200939091, 1.3288149330576953, 1.2743600183728472, 2.0869588737518, 1.7367286024683808, 1.6018070067959131, 1.5512993700688655, 1.2273916584228708, 2.2499543484082722, 3.83738183170415, 6.1099766986455135, 10.679657185821613, 7.76944763274611, 21.304239699275023, 4.702645436044126, 5.166137393291516, 2.047993197875022, 3.1065958919357257, 3.0948922024296968, 7.3516691827346765, 7.901220507002014, 15.485394785284958, 3.8154232363213074, 4.713157686134897, 2.9158238912346093, 5.034881269291264, 7.969565151028514, 8.327658490486161, 3.6833281374533264, 1.7485732878929219, 1.386503247681834, 1.970354567232902, 1.67495071784802, 1.6027755197200704, 1.4352633778698844, 2.224163215032316, 1.8930573623995617, 1.8000540953353896, 1.3286156779367588, 0.9708548527232406, 1.1894302280821867, 1.5281317953227482, 1.4750071739658654, 1.480193687000633, 1.0526797404784565, 1.3545740739972507, 1.4447871204908531, 1.748989458831172, 1.1069160901004726, 1.4055766222573134, 1.6134106989576296, 1.3619627389795599, 1.3153847626723734, 1.2055676990533408, 1.8147765658656039, 4.135409726108141, 6.146914944847218, 10.501950430049865, 11.905179437084705, 32.482997056250134, 2.20383688886092, 4.160669150814636, 6.4559550598550235, 8.982688130156323, 10.732979003098878, 14.409562197781927, 3.0282393007026416, 2.339565614372817, 1.8200000595431343, 2.3535825717999943, 3.1698306906827813, 2.2777988034058367, 1.2889046704602247, 1.340621090269041, 1.8303797086189368, 1.1979113734175837, 1.1141500436109422, 1.2903472328565377, 1.2228389874519257, 1.1774012241628653, 1.3172786928873408, 2.245039405233087, 3.085560771985923, 5.807804927047279, 8.889862198562625, 2.5514752007712493, 3.6178941722861717, 4.388028624884762, 5.211645885059664, 3.468907245211062, 4.0690766394448215, 2.349025837760106, 1.5287088622455483, 1.7085556127168755, 1.5504187125323758, 1.4397823574135038, 1.3027397419379494, 1.5559180216412596, 1.1435223091881748, 1.481474904665061, 1.212523817983449, 1.0058059205013534, 1.1750261830512176, 1.2946482648345339, 1.5499493839784517, 1.4137841826842537, 0.9464607912169936, 1.3365920344038935, 1.1020059166000453], \"dev\": [1.4857651925851194, 1.224646761269773, 1.841398691583461, 1.8329253902329778, 1.2566564951547168, 1.6972442943938906, 1.734018899823199, 1.7836332131760515, 0.985714850741864, 2.098369763892207, 1.9320754018422004, 1.8782117995315595, 1.307407662313678, 1.2786151500071823, 1.2037468813339653, 1.7649545465549212, 1.3718124899733175, 1.902692921338833, 1.498282390749725, 0.5571990645468765, 1.7232709331942684, 1.68511789762043, 1.3079610898902536, 1.432101530335346, 2.336814596325156, 1.65491977133395, 1.6745350326582158, 1.451490687212408, 1.405749553087569, 2.161986741459578, 1.9664424279208172, 1.2252144251511234, 1.1152825729641274, 2.0366173757151995, 1.4449044216464364, 1.9027467789148818, 1.1693799332674626, 1.2422343580263475, 2.106106553520988, 1.0933120926022648, 1.3781996727197097, 1.440145342467146, 1.7738356584882613, 1.4198945432363297, 2.9602488094856167, 1.6132376082999296, 1.602102654261877, 1.6159830834251352, 1.520130011123483, 1.40880588286172]}\n",
    "def plot_learning_curve(loss_record, title=''):\n",
    "    # 打开文件进行写入（'wb' 表示写入二进制模式）\n",
    "    # with open('new_data.json', 'w', encoding='utf-8') as file:\n",
    "    #     file.write(json.dumps(loss_record))\n",
    "\n",
    "    # return\n",
    "\n",
    "    ''' Plot learning curve of your DNN (train & dev loss) '''\n",
    "    total_steps = len(loss_record['train'])\n",
    "    x_1 = range(total_steps)\n",
    "    x_2 = x_1[::math.ceil(len(loss_record['train']) / len(loss_record['dev']))]\n",
    "    # plt.figure(figsize=(6, 4))\n",
    "    plt.plot(x_1, loss_record['train'], c='tab:red', label='train')\n",
    "    plt.plot(x_2, loss_record['dev'][:len(x_2)], c='tab:cyan', label='dev')\n",
    "    plt.ylim(0.0, 5.)\n",
    "    plt.xlabel('Training steps')\n",
    "    plt.ylabel('MSE loss')\n",
    "    plt.title('Learning curve of {}'.format(title))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "class CovidDataset(Dataset):\n",
    "    def __init__(self, data_path, mode=\"train\", transform=None):\n",
    "        self.data = pd.read_csv(data_path)\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "        self.data.drop(\"id\", axis=1, inplace=True)\n",
    "        self.data.iloc[:,40:93] = (self.data.iloc[:,40:93] - self.data.iloc[:, 40:93].mean())/self.data.iloc[:,40:93].std()\n",
    "        \n",
    "        # 需要区分 train data 和 test data\n",
    "        if self.mode == \"train\":\n",
    "            indices = [i for i in range(len(self.data)) if i % 10 != 0]\n",
    "            self.data = self.data.iloc[indices, :]\n",
    "        elif self.mode == \"dev\":\n",
    "            indices = [i for i in range(len(self.data)) if i % 10 == 0]\n",
    "            self.data = self.data.iloc[indices, :]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data.iloc[idx, :93].values\n",
    "        y = self.data.iloc[idx, -1]\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        \n",
    "        if self.mode != \"test\":\n",
    "            return x, y\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "\n",
    "train_data = CovidDataset(\"./data/covid.train.csv\", \"train\")\n",
    "dev_data = CovidDataset(\"./data/covid.train.csv\", \"dev\")\n",
    "test_data = CovidDataset(\"./data/covid.test.csv\", \"test\")\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=256, shuffle=True)\n",
    "dev_dataloader = DataLoader(dev_data, batch_size=256, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=32, shuffle=True)\n",
    "\n",
    "# train_features, train_labels = next(iter(train_dataloader))\n",
    "# print(f\"Feature batch shape: {train_features.size()}\")\n",
    "# print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "# print(train_features[0], train_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=93, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Epoch 1\n",
      "-------------------------------\n",
      "loss: 276.919857  [   32/ 2430]\n",
      "loss: 44.628488  [  352/ 2430]\n",
      "loss: 209.651650  [  672/ 2430]\n",
      "loss: 166.007030  [  992/ 2430]\n",
      "loss: 13.803063  [ 1312/ 2430]\n",
      "loss: 11.281647  [ 1632/ 2430]\n",
      "loss: 13.820902  [ 1952/ 2430]\n",
      "loss: 6.567769  [ 2272/ 2430]\n",
      "Train Epoch 2\n",
      "-------------------------------\n",
      "loss: 14.178843  [   32/ 2430]\n",
      "loss: 5.897413  [  352/ 2430]\n",
      "loss: 3.663202  [  672/ 2430]\n",
      "loss: 13.876282  [  992/ 2430]\n",
      "loss: 11.788131  [ 1312/ 2430]\n",
      "loss: 3.715159  [ 1632/ 2430]\n",
      "loss: 2.731345  [ 1952/ 2430]\n",
      "loss: 4.286840  [ 2272/ 2430]\n",
      "Train Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.848787  [   32/ 2430]\n",
      "loss: 3.298317  [  352/ 2430]\n",
      "loss: 2.207934  [  672/ 2430]\n",
      "loss: 1.074797  [  992/ 2430]\n",
      "loss: 1.147989  [ 1312/ 2430]\n",
      "loss: 2.995739  [ 1632/ 2430]\n",
      "loss: 2.820820  [ 1952/ 2430]\n",
      "loss: 3.687680  [ 2272/ 2430]\n",
      "Train Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.633491  [   32/ 2430]\n",
      "loss: 2.129895  [  352/ 2430]\n",
      "loss: 2.121506  [  672/ 2430]\n",
      "loss: 1.596459  [  992/ 2430]\n",
      "loss: 1.606999  [ 1312/ 2430]\n",
      "loss: 1.561370  [ 1632/ 2430]\n",
      "loss: 3.252750  [ 1952/ 2430]\n",
      "loss: 7.177874  [ 2272/ 2430]\n",
      "Train Epoch 5\n",
      "-------------------------------\n",
      "loss: 4.425273  [   32/ 2430]\n",
      "loss: 3.076835  [  352/ 2430]\n",
      "loss: 1.882943  [  672/ 2430]\n",
      "loss: 2.781346  [  992/ 2430]\n",
      "loss: 1.957736  [ 1312/ 2430]\n",
      "loss: 0.862099  [ 1632/ 2430]\n",
      "loss: 2.421683  [ 1952/ 2430]\n",
      "loss: 1.367063  [ 2272/ 2430]\n",
      "Train Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.274529  [   32/ 2430]\n",
      "loss: 0.991064  [  352/ 2430]\n",
      "loss: 2.104183  [  672/ 2430]\n",
      "loss: 2.228454  [  992/ 2430]\n",
      "loss: 1.068548  [ 1312/ 2430]\n",
      "loss: 3.599416  [ 1632/ 2430]\n",
      "loss: 1.611328  [ 1952/ 2430]\n",
      "loss: 2.606003  [ 2272/ 2430]\n",
      "Train Epoch 7\n",
      "-------------------------------\n",
      "loss: 2.013577  [   32/ 2430]\n",
      "loss: 2.152320  [  352/ 2430]\n",
      "loss: 1.101081  [  672/ 2430]\n",
      "loss: 1.040195  [  992/ 2430]\n",
      "loss: 1.930523  [ 1312/ 2430]\n",
      "loss: 0.579108  [ 1632/ 2430]\n",
      "loss: 2.033291  [ 1952/ 2430]\n",
      "loss: 0.804249  [ 2272/ 2430]\n",
      "Train Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.866778  [   32/ 2430]\n",
      "loss: 1.908654  [  352/ 2430]\n",
      "loss: 2.383091  [  672/ 2430]\n",
      "loss: 1.723652  [  992/ 2430]\n",
      "loss: 1.854371  [ 1312/ 2430]\n",
      "loss: 1.607269  [ 1632/ 2430]\n",
      "loss: 2.084348  [ 1952/ 2430]\n",
      "loss: 4.048617  [ 2272/ 2430]\n",
      "Train Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.592731  [   32/ 2430]\n",
      "loss: 1.873755  [  352/ 2430]\n",
      "loss: 1.283184  [  672/ 2430]\n",
      "loss: 1.539974  [  992/ 2430]\n",
      "loss: 0.713293  [ 1312/ 2430]\n",
      "loss: 1.044636  [ 1632/ 2430]\n",
      "loss: 0.860266  [ 1952/ 2430]\n",
      "loss: 1.046645  [ 2272/ 2430]\n",
      "Train Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.442692  [   32/ 2430]\n",
      "loss: 0.979929  [  352/ 2430]\n",
      "loss: 0.848178  [  672/ 2430]\n",
      "loss: 1.416445  [  992/ 2430]\n",
      "loss: 0.607287  [ 1312/ 2430]\n",
      "loss: 3.146874  [ 1632/ 2430]\n",
      "loss: 2.749346  [ 1952/ 2430]\n",
      "loss: 4.486399  [ 2272/ 2430]\n",
      "Train Epoch 11\n",
      "-------------------------------\n",
      "loss: 1.300955  [   32/ 2430]\n",
      "loss: 2.526449  [  352/ 2430]\n",
      "loss: 0.711876  [  672/ 2430]\n",
      "loss: 0.912274  [  992/ 2430]\n",
      "loss: 1.903652  [ 1312/ 2430]\n",
      "loss: 1.061686  [ 1632/ 2430]\n",
      "loss: 2.228610  [ 1952/ 2430]\n",
      "loss: 2.303518  [ 2272/ 2430]\n",
      "Train Epoch 12\n",
      "-------------------------------\n",
      "loss: 3.147053  [   32/ 2430]\n",
      "loss: 1.160331  [  352/ 2430]\n",
      "loss: 2.057928  [  672/ 2430]\n",
      "loss: 1.523578  [  992/ 2430]\n",
      "loss: 1.067687  [ 1312/ 2430]\n",
      "loss: 1.204290  [ 1632/ 2430]\n",
      "loss: 1.183069  [ 1952/ 2430]\n",
      "loss: 1.267262  [ 2272/ 2430]\n",
      "Train Epoch 13\n",
      "-------------------------------\n",
      "loss: 1.412318  [   32/ 2430]\n",
      "loss: 1.799457  [  352/ 2430]\n",
      "loss: 0.716392  [  672/ 2430]\n",
      "loss: 1.353169  [  992/ 2430]\n",
      "loss: 4.853070  [ 1312/ 2430]\n",
      "loss: 1.119353  [ 1632/ 2430]\n",
      "loss: 1.980835  [ 1952/ 2430]\n",
      "loss: 1.175922  [ 2272/ 2430]\n",
      "Train Epoch 14\n",
      "-------------------------------\n",
      "loss: 1.044963  [   32/ 2430]\n",
      "loss: 0.584763  [  352/ 2430]\n",
      "loss: 1.406297  [  672/ 2430]\n",
      "loss: 0.652706  [  992/ 2430]\n",
      "loss: 0.821906  [ 1312/ 2430]\n",
      "loss: 1.833068  [ 1632/ 2430]\n",
      "loss: 1.156108  [ 1952/ 2430]\n",
      "loss: 0.718158  [ 2272/ 2430]\n",
      "Train Epoch 15\n",
      "-------------------------------\n",
      "loss: 2.740225  [   32/ 2430]\n",
      "loss: 0.711163  [  352/ 2430]\n",
      "loss: 1.103868  [  672/ 2430]\n",
      "loss: 0.683263  [  992/ 2430]\n",
      "loss: 1.019247  [ 1312/ 2430]\n",
      "loss: 3.512878  [ 1632/ 2430]\n",
      "loss: 1.928182  [ 1952/ 2430]\n",
      "loss: 1.354530  [ 2272/ 2430]\n",
      "Train Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.990831  [   32/ 2430]\n",
      "loss: 1.285767  [  352/ 2430]\n",
      "loss: 1.124818  [  672/ 2430]\n",
      "loss: 0.705446  [  992/ 2430]\n",
      "loss: 1.619146  [ 1312/ 2430]\n",
      "loss: 1.668582  [ 1632/ 2430]\n",
      "loss: 0.543837  [ 1952/ 2430]\n",
      "loss: 4.795601  [ 2272/ 2430]\n",
      "Train Epoch 17\n",
      "-------------------------------\n",
      "loss: 2.186791  [   32/ 2430]\n",
      "loss: 0.823097  [  352/ 2430]\n",
      "loss: 0.876938  [  672/ 2430]\n",
      "loss: 1.156781  [  992/ 2430]\n",
      "loss: 1.028347  [ 1312/ 2430]\n",
      "loss: 1.103323  [ 1632/ 2430]\n",
      "loss: 0.748403  [ 1952/ 2430]\n",
      "loss: 0.698443  [ 2272/ 2430]\n",
      "Train Epoch 18\n",
      "-------------------------------\n",
      "loss: 3.978118  [   32/ 2430]\n",
      "loss: 0.640544  [  352/ 2430]\n",
      "loss: 1.122078  [  672/ 2430]\n",
      "loss: 0.588276  [  992/ 2430]\n",
      "loss: 0.772180  [ 1312/ 2430]\n",
      "loss: 0.383309  [ 1632/ 2430]\n",
      "loss: 1.961098  [ 1952/ 2430]\n",
      "loss: 0.910034  [ 2272/ 2430]\n",
      "Train Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.795782  [   32/ 2430]\n",
      "loss: 0.699104  [  352/ 2430]\n",
      "loss: 3.611551  [  672/ 2430]\n",
      "loss: 1.346192  [  992/ 2430]\n",
      "loss: 0.597885  [ 1312/ 2430]\n",
      "loss: 1.300320  [ 1632/ 2430]\n",
      "loss: 1.280845  [ 1952/ 2430]\n",
      "loss: 1.412430  [ 2272/ 2430]\n",
      "Train Epoch 20\n",
      "-------------------------------\n",
      "loss: 1.793389  [   32/ 2430]\n",
      "loss: 4.646082  [  352/ 2430]\n",
      "loss: 0.595609  [  672/ 2430]\n",
      "loss: 0.946936  [  992/ 2430]\n",
      "loss: 1.228813  [ 1312/ 2430]\n",
      "loss: 0.736927  [ 1632/ 2430]\n",
      "loss: 3.356715  [ 1952/ 2430]\n",
      "loss: 0.570026  [ 2272/ 2430]\n",
      "Train Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.689886  [   32/ 2430]\n",
      "loss: 1.676075  [  352/ 2430]\n",
      "loss: 0.917966  [  672/ 2430]\n",
      "loss: 0.984213  [  992/ 2430]\n",
      "loss: 0.884282  [ 1312/ 2430]\n",
      "loss: 1.253548  [ 1632/ 2430]\n",
      "loss: 1.192230  [ 1952/ 2430]\n",
      "loss: 2.945651  [ 2272/ 2430]\n",
      "Train Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.780144  [   32/ 2430]\n",
      "loss: 3.680426  [  352/ 2430]\n",
      "loss: 2.362669  [  672/ 2430]\n",
      "loss: 0.678226  [  992/ 2430]\n",
      "loss: 0.883033  [ 1312/ 2430]\n",
      "loss: 1.435565  [ 1632/ 2430]\n",
      "loss: 0.653010  [ 1952/ 2430]\n",
      "loss: 1.281443  [ 2272/ 2430]\n",
      "Train Epoch 23\n",
      "-------------------------------\n",
      "loss: 1.335901  [   32/ 2430]\n",
      "loss: 1.436010  [  352/ 2430]\n",
      "loss: 1.058008  [  672/ 2430]\n",
      "loss: 0.884757  [  992/ 2430]\n",
      "loss: 0.484992  [ 1312/ 2430]\n",
      "loss: 1.276529  [ 1632/ 2430]\n",
      "loss: 1.090175  [ 1952/ 2430]\n",
      "loss: 0.540760  [ 2272/ 2430]\n",
      "Train Epoch 24\n",
      "-------------------------------\n",
      "loss: 1.121190  [   32/ 2430]\n",
      "loss: 0.600350  [  352/ 2430]\n",
      "loss: 0.722389  [  672/ 2430]\n",
      "loss: 1.117963  [  992/ 2430]\n",
      "loss: 0.741775  [ 1312/ 2430]\n",
      "loss: 0.531715  [ 1632/ 2430]\n",
      "loss: 0.784250  [ 1952/ 2430]\n",
      "loss: 0.729738  [ 2272/ 2430]\n",
      "Train Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.725267  [   32/ 2430]\n",
      "loss: 1.278752  [  352/ 2430]\n",
      "loss: 2.019174  [  672/ 2430]\n",
      "loss: 0.727237  [  992/ 2430]\n",
      "loss: 0.766397  [ 1312/ 2430]\n",
      "loss: 0.679371  [ 1632/ 2430]\n",
      "loss: 1.149572  [ 1952/ 2430]\n",
      "loss: 1.027721  [ 2272/ 2430]\n",
      "Train Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.977231  [   32/ 2430]\n",
      "loss: 0.596708  [  352/ 2430]\n",
      "loss: 0.783730  [  672/ 2430]\n",
      "loss: 1.962006  [  992/ 2430]\n",
      "loss: 0.761874  [ 1312/ 2430]\n",
      "loss: 0.717338  [ 1632/ 2430]\n",
      "loss: 0.789943  [ 1952/ 2430]\n",
      "loss: 0.451548  [ 2272/ 2430]\n",
      "Train Epoch 27\n",
      "-------------------------------\n",
      "loss: 1.010175  [   32/ 2430]\n",
      "loss: 0.564955  [  352/ 2430]\n",
      "loss: 0.986855  [  672/ 2430]\n",
      "loss: 1.236331  [  992/ 2430]\n",
      "loss: 1.011160  [ 1312/ 2430]\n",
      "loss: 4.978355  [ 1632/ 2430]\n",
      "loss: 0.736773  [ 1952/ 2430]\n",
      "loss: 1.058178  [ 2272/ 2430]\n",
      "Train Epoch 28\n",
      "-------------------------------\n",
      "loss: 1.540518  [   32/ 2430]\n",
      "loss: 1.118992  [  352/ 2430]\n",
      "loss: 0.668878  [  672/ 2430]\n",
      "loss: 0.732637  [  992/ 2430]\n",
      "loss: 0.530772  [ 1312/ 2430]\n",
      "loss: 2.599232  [ 1632/ 2430]\n",
      "loss: 1.076609  [ 1952/ 2430]\n",
      "loss: 0.851364  [ 2272/ 2430]\n",
      "Train Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.895887  [   32/ 2430]\n",
      "loss: 0.454261  [  352/ 2430]\n",
      "loss: 0.945685  [  672/ 2430]\n",
      "loss: 1.224985  [  992/ 2430]\n",
      "loss: 0.489911  [ 1312/ 2430]\n",
      "loss: 1.461828  [ 1632/ 2430]\n",
      "loss: 0.775005  [ 1952/ 2430]\n",
      "loss: 0.974426  [ 2272/ 2430]\n",
      "Train Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.800284  [   32/ 2430]\n",
      "loss: 0.883140  [  352/ 2430]\n",
      "loss: 4.480036  [  672/ 2430]\n",
      "loss: 0.452220  [  992/ 2430]\n",
      "loss: 1.065768  [ 1312/ 2430]\n",
      "loss: 0.958814  [ 1632/ 2430]\n",
      "loss: 0.728531  [ 1952/ 2430]\n",
      "loss: 1.540630  [ 2272/ 2430]\n",
      "Train Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.772396  [   32/ 2430]\n",
      "loss: 1.330174  [  352/ 2430]\n",
      "loss: 0.608378  [  672/ 2430]\n",
      "loss: 2.292147  [  992/ 2430]\n",
      "loss: 0.582906  [ 1312/ 2430]\n",
      "loss: 1.184123  [ 1632/ 2430]\n",
      "loss: 0.772051  [ 1952/ 2430]\n",
      "loss: 0.890741  [ 2272/ 2430]\n",
      "Train Epoch 32\n",
      "-------------------------------\n",
      "loss: 1.922351  [   32/ 2430]\n",
      "loss: 0.888503  [  352/ 2430]\n",
      "loss: 0.544935  [  672/ 2430]\n",
      "loss: 1.138188  [  992/ 2430]\n",
      "loss: 0.606458  [ 1312/ 2430]\n",
      "loss: 2.002905  [ 1632/ 2430]\n",
      "loss: 1.351342  [ 1952/ 2430]\n",
      "loss: 0.705193  [ 2272/ 2430]\n",
      "Train Epoch 33\n",
      "-------------------------------\n",
      "loss: 1.113607  [   32/ 2430]\n",
      "loss: 1.301509  [  352/ 2430]\n",
      "loss: 0.308722  [  672/ 2430]\n",
      "loss: 0.701105  [  992/ 2430]\n",
      "loss: 0.827448  [ 1312/ 2430]\n",
      "loss: 0.695285  [ 1632/ 2430]\n",
      "loss: 1.815326  [ 1952/ 2430]\n",
      "loss: 2.323992  [ 2272/ 2430]\n",
      "Train Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.784858  [   32/ 2430]\n",
      "loss: 1.101584  [  352/ 2430]\n",
      "loss: 0.764296  [  672/ 2430]\n",
      "loss: 0.931385  [  992/ 2430]\n",
      "loss: 1.148331  [ 1312/ 2430]\n",
      "loss: 0.791869  [ 1632/ 2430]\n",
      "loss: 0.445506  [ 1952/ 2430]\n",
      "loss: 0.554855  [ 2272/ 2430]\n",
      "Train Epoch 35\n",
      "-------------------------------\n",
      "loss: 1.428083  [   32/ 2430]\n",
      "loss: 0.737636  [  352/ 2430]\n",
      "loss: 1.035505  [  672/ 2430]\n",
      "loss: 0.644069  [  992/ 2430]\n",
      "loss: 1.496469  [ 1312/ 2430]\n",
      "loss: 0.641185  [ 1632/ 2430]\n",
      "loss: 0.822119  [ 1952/ 2430]\n",
      "loss: 0.785955  [ 2272/ 2430]\n",
      "Train Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.486177  [   32/ 2430]\n",
      "loss: 0.431703  [  352/ 2430]\n",
      "loss: 0.736710  [  672/ 2430]\n",
      "loss: 0.952380  [  992/ 2430]\n",
      "loss: 1.698515  [ 1312/ 2430]\n",
      "loss: 1.044168  [ 1632/ 2430]\n",
      "loss: 0.681014  [ 1952/ 2430]\n",
      "loss: 2.515562  [ 2272/ 2430]\n",
      "Train Epoch 37\n",
      "-------------------------------\n",
      "loss: 1.022377  [   32/ 2430]\n",
      "loss: 1.104194  [  352/ 2430]\n",
      "loss: 0.588202  [  672/ 2430]\n",
      "loss: 1.256422  [  992/ 2430]\n",
      "loss: 0.968225  [ 1312/ 2430]\n",
      "loss: 1.125562  [ 1632/ 2430]\n",
      "loss: 1.929298  [ 1952/ 2430]\n",
      "loss: 1.021577  [ 2272/ 2430]\n",
      "Train Epoch 38\n",
      "-------------------------------\n",
      "loss: 1.379804  [   32/ 2430]\n",
      "loss: 1.011771  [  352/ 2430]\n",
      "loss: 1.062303  [  672/ 2430]\n",
      "loss: 0.873945  [  992/ 2430]\n",
      "loss: 1.132309  [ 1312/ 2430]\n",
      "loss: 0.730919  [ 1632/ 2430]\n",
      "loss: 0.949822  [ 1952/ 2430]\n",
      "loss: 0.809080  [ 2272/ 2430]\n",
      "Train Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.705982  [   32/ 2430]\n",
      "loss: 0.719061  [  352/ 2430]\n",
      "loss: 1.016783  [  672/ 2430]\n",
      "loss: 1.027174  [  992/ 2430]\n",
      "loss: 0.540824  [ 1312/ 2430]\n",
      "loss: 0.779229  [ 1632/ 2430]\n",
      "loss: 0.682259  [ 1952/ 2430]\n",
      "loss: 1.057037  [ 2272/ 2430]\n",
      "Train Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.769722  [   32/ 2430]\n",
      "loss: 1.121379  [  352/ 2430]\n",
      "loss: 1.710347  [  672/ 2430]\n",
      "loss: 2.103280  [  992/ 2430]\n",
      "loss: 0.943782  [ 1312/ 2430]\n",
      "loss: 0.791411  [ 1632/ 2430]\n",
      "loss: 0.891620  [ 1952/ 2430]\n",
      "loss: 1.323456  [ 2272/ 2430]\n",
      "Train Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.916656  [   32/ 2430]\n",
      "loss: 0.741822  [  352/ 2430]\n",
      "loss: 1.190548  [  672/ 2430]\n",
      "loss: 0.516018  [  992/ 2430]\n",
      "loss: 0.470538  [ 1312/ 2430]\n",
      "loss: 1.100397  [ 1632/ 2430]\n",
      "loss: 1.411321  [ 1952/ 2430]\n",
      "loss: 0.788128  [ 2272/ 2430]\n",
      "Train Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.943769  [   32/ 2430]\n",
      "loss: 1.157595  [  352/ 2430]\n",
      "loss: 1.165092  [  672/ 2430]\n",
      "loss: 1.679322  [  992/ 2430]\n",
      "loss: 1.754703  [ 1312/ 2430]\n",
      "loss: 0.494520  [ 1632/ 2430]\n",
      "loss: 0.759018  [ 1952/ 2430]\n",
      "loss: 1.266574  [ 2272/ 2430]\n",
      "Train Epoch 43\n",
      "-------------------------------\n",
      "loss: 1.555897  [   32/ 2430]\n",
      "loss: 0.881344  [  352/ 2430]\n",
      "loss: 1.500405  [  672/ 2430]\n",
      "loss: 0.804266  [  992/ 2430]\n",
      "loss: 0.582802  [ 1312/ 2430]\n",
      "loss: 1.193887  [ 1632/ 2430]\n",
      "loss: 1.212675  [ 1952/ 2430]\n",
      "loss: 1.635205  [ 2272/ 2430]\n",
      "Train Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.699096  [   32/ 2430]\n",
      "loss: 1.231841  [  352/ 2430]\n",
      "loss: 1.068801  [  672/ 2430]\n",
      "loss: 0.553802  [  992/ 2430]\n",
      "loss: 1.021983  [ 1312/ 2430]\n",
      "loss: 0.777046  [ 1632/ 2430]\n",
      "loss: 0.876066  [ 1952/ 2430]\n",
      "loss: 0.298192  [ 2272/ 2430]\n",
      "Train Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.665658  [   32/ 2430]\n",
      "loss: 4.126994  [  352/ 2430]\n",
      "loss: 0.687899  [  672/ 2430]\n",
      "loss: 0.588530  [  992/ 2430]\n",
      "loss: 0.960939  [ 1312/ 2430]\n",
      "loss: 0.803662  [ 1632/ 2430]\n",
      "loss: 0.621827  [ 1952/ 2430]\n",
      "loss: 0.983526  [ 2272/ 2430]\n",
      "Train Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.762718  [   32/ 2430]\n",
      "loss: 0.916451  [  352/ 2430]\n",
      "loss: 0.640995  [  672/ 2430]\n",
      "loss: 0.971054  [  992/ 2430]\n",
      "loss: 0.920084  [ 1312/ 2430]\n",
      "loss: 0.806017  [ 1632/ 2430]\n",
      "loss: 1.431161  [ 1952/ 2430]\n",
      "loss: 0.932020  [ 2272/ 2430]\n",
      "Train Epoch 47\n",
      "-------------------------------\n",
      "loss: 1.883343  [   32/ 2430]\n",
      "loss: 0.719024  [  352/ 2430]\n",
      "loss: 1.120614  [  672/ 2430]\n",
      "loss: 0.693352  [  992/ 2430]\n",
      "loss: 0.940082  [ 1312/ 2430]\n",
      "loss: 1.581881  [ 1632/ 2430]\n",
      "loss: 0.847279  [ 1952/ 2430]\n",
      "loss: 1.709712  [ 2272/ 2430]\n",
      "Train Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.393855  [   32/ 2430]\n",
      "loss: 1.009739  [  352/ 2430]\n",
      "loss: 0.654896  [  672/ 2430]\n",
      "loss: 1.000850  [  992/ 2430]\n",
      "loss: 0.719945  [ 1312/ 2430]\n",
      "loss: 1.197393  [ 1632/ 2430]\n",
      "loss: 0.702465  [ 1952/ 2430]\n",
      "loss: 1.223655  [ 2272/ 2430]\n",
      "Train Epoch 49\n",
      "-------------------------------\n",
      "loss: 1.521984  [   32/ 2430]\n",
      "loss: 1.203408  [  352/ 2430]\n",
      "loss: 1.003862  [  672/ 2430]\n",
      "loss: 0.503683  [  992/ 2430]\n",
      "loss: 0.753183  [ 1312/ 2430]\n",
      "loss: 0.721110  [ 1632/ 2430]\n",
      "loss: 0.456582  [ 1952/ 2430]\n",
      "loss: 1.091413  [ 2272/ 2430]\n",
      "Train Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.884324  [   32/ 2430]\n",
      "loss: 0.600680  [  352/ 2430]\n",
      "loss: 0.987850  [  672/ 2430]\n",
      "loss: 0.676634  [  992/ 2430]\n",
      "loss: 1.159709  [ 1312/ 2430]\n",
      "loss: 0.854804  [ 1632/ 2430]\n",
      "loss: 1.494989  [ 1952/ 2430]\n",
      "loss: 0.920469  [ 2272/ 2430]\n",
      "Train Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.875511  [   32/ 2430]\n",
      "loss: 0.995730  [  352/ 2430]\n",
      "loss: 0.687503  [  672/ 2430]\n",
      "loss: 0.696453  [  992/ 2430]\n",
      "loss: 0.828014  [ 1312/ 2430]\n",
      "loss: 1.212859  [ 1632/ 2430]\n",
      "loss: 1.186734  [ 1952/ 2430]\n",
      "loss: 0.882373  [ 2272/ 2430]\n",
      "Train Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.868894  [   32/ 2430]\n",
      "loss: 0.678655  [  352/ 2430]\n",
      "loss: 0.594680  [  672/ 2430]\n",
      "loss: 0.751067  [  992/ 2430]\n",
      "loss: 1.085602  [ 1312/ 2430]\n",
      "loss: 1.057092  [ 1632/ 2430]\n",
      "loss: 1.661511  [ 1952/ 2430]\n",
      "loss: 0.708662  [ 2272/ 2430]\n",
      "Train Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.559271  [   32/ 2430]\n",
      "loss: 0.788181  [  352/ 2430]\n",
      "loss: 0.809190  [  672/ 2430]\n",
      "loss: 0.665378  [  992/ 2430]\n",
      "loss: 1.259377  [ 1312/ 2430]\n",
      "loss: 0.525669  [ 1632/ 2430]\n",
      "loss: 0.613146  [ 1952/ 2430]\n",
      "loss: 0.655676  [ 2272/ 2430]\n",
      "Train Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.581666  [   32/ 2430]\n",
      "loss: 0.382791  [  352/ 2430]\n",
      "loss: 0.684887  [  672/ 2430]\n",
      "loss: 0.641291  [  992/ 2430]\n",
      "loss: 0.369517  [ 1312/ 2430]\n",
      "loss: 0.530241  [ 1632/ 2430]\n",
      "loss: 1.761300  [ 1952/ 2430]\n",
      "loss: 0.759138  [ 2272/ 2430]\n",
      "Train Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.539292  [   32/ 2430]\n",
      "loss: 1.073337  [  352/ 2430]\n",
      "loss: 0.889175  [  672/ 2430]\n",
      "loss: 0.566001  [  992/ 2430]\n",
      "loss: 0.714234  [ 1312/ 2430]\n",
      "loss: 0.513513  [ 1632/ 2430]\n",
      "loss: 0.848681  [ 1952/ 2430]\n",
      "loss: 0.686606  [ 2272/ 2430]\n",
      "Train Epoch 56\n",
      "-------------------------------\n",
      "loss: 1.563079  [   32/ 2430]\n",
      "loss: 0.755827  [  352/ 2430]\n",
      "loss: 0.978689  [  672/ 2430]\n",
      "loss: 0.403663  [  992/ 2430]\n",
      "loss: 1.400793  [ 1312/ 2430]\n",
      "loss: 0.483209  [ 1632/ 2430]\n",
      "loss: 0.750769  [ 1952/ 2430]\n",
      "loss: 0.534883  [ 2272/ 2430]\n",
      "Train Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.810907  [   32/ 2430]\n",
      "loss: 0.751504  [  352/ 2430]\n",
      "loss: 1.535315  [  672/ 2430]\n",
      "loss: 1.377977  [  992/ 2430]\n",
      "loss: 0.758495  [ 1312/ 2430]\n",
      "loss: 0.603997  [ 1632/ 2430]\n",
      "loss: 1.522092  [ 1952/ 2430]\n",
      "loss: 0.453052  [ 2272/ 2430]\n",
      "Train Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.831714  [   32/ 2430]\n",
      "loss: 0.938814  [  352/ 2430]\n",
      "loss: 0.915187  [  672/ 2430]\n",
      "loss: 0.492211  [  992/ 2430]\n",
      "loss: 0.585399  [ 1312/ 2430]\n",
      "loss: 0.862373  [ 1632/ 2430]\n",
      "loss: 1.206768  [ 1952/ 2430]\n",
      "loss: 0.578967  [ 2272/ 2430]\n",
      "Train Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.469673  [   32/ 2430]\n",
      "loss: 0.561312  [  352/ 2430]\n",
      "loss: 0.608232  [  672/ 2430]\n",
      "loss: 0.767828  [  992/ 2430]\n",
      "loss: 0.847472  [ 1312/ 2430]\n",
      "loss: 0.616586  [ 1632/ 2430]\n",
      "loss: 1.212448  [ 1952/ 2430]\n",
      "loss: 0.908668  [ 2272/ 2430]\n",
      "Train Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.541996  [   32/ 2430]\n",
      "loss: 0.547479  [  352/ 2430]\n",
      "loss: 1.511342  [  672/ 2430]\n",
      "loss: 0.254001  [  992/ 2430]\n",
      "loss: 0.640043  [ 1312/ 2430]\n",
      "loss: 0.851011  [ 1632/ 2430]\n",
      "loss: 0.840885  [ 1952/ 2430]\n",
      "loss: 1.055094  [ 2272/ 2430]\n",
      "Train Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.718325  [   32/ 2430]\n",
      "loss: 0.958597  [  352/ 2430]\n",
      "loss: 0.807732  [  672/ 2430]\n",
      "loss: 0.638811  [  992/ 2430]\n",
      "loss: 0.636795  [ 1312/ 2430]\n",
      "loss: 0.631208  [ 1632/ 2430]\n",
      "loss: 0.792892  [ 1952/ 2430]\n",
      "loss: 0.896326  [ 2272/ 2430]\n",
      "Train Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.701323  [   32/ 2430]\n",
      "loss: 1.469250  [  352/ 2430]\n",
      "loss: 0.799041  [  672/ 2430]\n",
      "loss: 0.449440  [  992/ 2430]\n",
      "loss: 1.144257  [ 1312/ 2430]\n",
      "loss: 0.738634  [ 1632/ 2430]\n",
      "loss: 0.466346  [ 1952/ 2430]\n",
      "loss: 1.577050  [ 2272/ 2430]\n",
      "Train Epoch 63\n",
      "-------------------------------\n",
      "loss: 1.153725  [   32/ 2430]\n",
      "loss: 0.776927  [  352/ 2430]\n",
      "loss: 0.637127  [  672/ 2430]\n",
      "loss: 1.170416  [  992/ 2430]\n",
      "loss: 1.034427  [ 1312/ 2430]\n",
      "loss: 0.599258  [ 1632/ 2430]\n",
      "loss: 0.944412  [ 1952/ 2430]\n",
      "loss: 0.954138  [ 2272/ 2430]\n",
      "Train Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.926780  [   32/ 2430]\n",
      "loss: 0.920241  [  352/ 2430]\n",
      "loss: 0.463650  [  672/ 2430]\n",
      "loss: 0.386068  [  992/ 2430]\n",
      "loss: 0.849308  [ 1312/ 2430]\n",
      "loss: 0.661601  [ 1632/ 2430]\n",
      "loss: 1.614080  [ 1952/ 2430]\n",
      "loss: 0.392628  [ 2272/ 2430]\n",
      "Train Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.659523  [   32/ 2430]\n",
      "loss: 0.866505  [  352/ 2430]\n",
      "loss: 0.760407  [  672/ 2430]\n",
      "loss: 0.898815  [  992/ 2430]\n",
      "loss: 1.031453  [ 1312/ 2430]\n",
      "loss: 1.054102  [ 1632/ 2430]\n",
      "loss: 1.372883  [ 1952/ 2430]\n",
      "loss: 1.014407  [ 2272/ 2430]\n",
      "Train Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.528188  [   32/ 2430]\n",
      "loss: 0.638946  [  352/ 2430]\n",
      "loss: 1.047972  [  672/ 2430]\n",
      "loss: 1.030943  [  992/ 2430]\n",
      "loss: 0.874333  [ 1312/ 2430]\n",
      "loss: 0.563833  [ 1632/ 2430]\n",
      "loss: 1.199014  [ 1952/ 2430]\n",
      "loss: 0.403927  [ 2272/ 2430]\n",
      "Train Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.817190  [   32/ 2430]\n",
      "loss: 0.974519  [  352/ 2430]\n",
      "loss: 0.527864  [  672/ 2430]\n",
      "loss: 0.616072  [  992/ 2430]\n",
      "loss: 4.892973  [ 1312/ 2430]\n",
      "loss: 1.141036  [ 1632/ 2430]\n",
      "loss: 0.771876  [ 1952/ 2430]\n",
      "loss: 0.514988  [ 2272/ 2430]\n",
      "Train Epoch 68\n",
      "-------------------------------\n",
      "loss: 1.227795  [   32/ 2430]\n",
      "loss: 0.717901  [  352/ 2430]\n",
      "loss: 1.180807  [  672/ 2430]\n",
      "loss: 1.433522  [  992/ 2430]\n",
      "loss: 0.774090  [ 1312/ 2430]\n",
      "loss: 1.134234  [ 1632/ 2430]\n",
      "loss: 0.738684  [ 1952/ 2430]\n",
      "loss: 1.080088  [ 2272/ 2430]\n",
      "Train Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.828623  [   32/ 2430]\n",
      "loss: 1.141117  [  352/ 2430]\n",
      "loss: 0.518205  [  672/ 2430]\n",
      "loss: 0.546251  [  992/ 2430]\n",
      "loss: 0.793284  [ 1312/ 2430]\n",
      "loss: 0.891213  [ 1632/ 2430]\n",
      "loss: 1.383391  [ 1952/ 2430]\n",
      "loss: 0.724897  [ 2272/ 2430]\n",
      "Train Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.997493  [   32/ 2430]\n",
      "loss: 0.671884  [  352/ 2430]\n",
      "loss: 0.660225  [  672/ 2430]\n",
      "loss: 0.717967  [  992/ 2430]\n",
      "loss: 0.452423  [ 1312/ 2430]\n",
      "loss: 0.878510  [ 1632/ 2430]\n",
      "loss: 1.066907  [ 1952/ 2430]\n",
      "loss: 0.692480  [ 2272/ 2430]\n",
      "Train Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.406776  [   32/ 2430]\n",
      "loss: 0.904741  [  352/ 2430]\n",
      "loss: 0.523444  [  672/ 2430]\n",
      "loss: 0.709321  [  992/ 2430]\n",
      "loss: 1.306823  [ 1312/ 2430]\n",
      "loss: 0.570839  [ 1632/ 2430]\n",
      "loss: 0.379129  [ 1952/ 2430]\n",
      "loss: 0.661587  [ 2272/ 2430]\n",
      "Train Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.758527  [   32/ 2430]\n",
      "loss: 0.593744  [  352/ 2430]\n",
      "loss: 0.311134  [  672/ 2430]\n",
      "loss: 0.880893  [  992/ 2430]\n",
      "loss: 1.692976  [ 1312/ 2430]\n",
      "loss: 0.352066  [ 1632/ 2430]\n",
      "loss: 0.469447  [ 1952/ 2430]\n",
      "loss: 0.849909  [ 2272/ 2430]\n",
      "Train Epoch 73\n",
      "-------------------------------\n",
      "loss: 1.186791  [   32/ 2430]\n",
      "loss: 0.395399  [  352/ 2430]\n",
      "loss: 2.900189  [  672/ 2430]\n",
      "loss: 1.430420  [  992/ 2430]\n",
      "loss: 1.004402  [ 1312/ 2430]\n",
      "loss: 0.539423  [ 1632/ 2430]\n",
      "loss: 0.581581  [ 1952/ 2430]\n",
      "loss: 0.636527  [ 2272/ 2430]\n",
      "Train Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.696697  [   32/ 2430]\n",
      "loss: 0.754161  [  352/ 2430]\n",
      "loss: 0.447614  [  672/ 2430]\n",
      "loss: 0.358334  [  992/ 2430]\n",
      "loss: 0.712312  [ 1312/ 2430]\n",
      "loss: 0.573558  [ 1632/ 2430]\n",
      "loss: 1.123541  [ 1952/ 2430]\n",
      "loss: 0.838313  [ 2272/ 2430]\n",
      "Train Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.914303  [   32/ 2430]\n",
      "loss: 0.521513  [  352/ 2430]\n",
      "loss: 0.653018  [  672/ 2430]\n",
      "loss: 0.707067  [  992/ 2430]\n",
      "loss: 0.637868  [ 1312/ 2430]\n",
      "loss: 1.193325  [ 1632/ 2430]\n",
      "loss: 0.413479  [ 1952/ 2430]\n",
      "loss: 0.850846  [ 2272/ 2430]\n",
      "Train Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.819121  [   32/ 2430]\n",
      "loss: 0.769867  [  352/ 2430]\n",
      "loss: 0.575305  [  672/ 2430]\n",
      "loss: 0.968200  [  992/ 2430]\n",
      "loss: 0.682038  [ 1312/ 2430]\n",
      "loss: 1.028040  [ 1632/ 2430]\n",
      "loss: 1.054741  [ 1952/ 2430]\n",
      "loss: 0.659933  [ 2272/ 2430]\n",
      "Train Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.841599  [   32/ 2430]\n",
      "loss: 0.980321  [  352/ 2430]\n",
      "loss: 0.798779  [  672/ 2430]\n",
      "loss: 0.531041  [  992/ 2430]\n",
      "loss: 0.782443  [ 1312/ 2430]\n",
      "loss: 0.416311  [ 1632/ 2430]\n",
      "loss: 0.449694  [ 1952/ 2430]\n",
      "loss: 0.662744  [ 2272/ 2430]\n",
      "Train Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.527269  [   32/ 2430]\n",
      "loss: 0.682527  [  352/ 2430]\n",
      "loss: 0.506136  [  672/ 2430]\n",
      "loss: 0.636396  [  992/ 2430]\n",
      "loss: 0.318393  [ 1312/ 2430]\n",
      "loss: 0.765245  [ 1632/ 2430]\n",
      "loss: 0.708133  [ 1952/ 2430]\n",
      "loss: 0.411552  [ 2272/ 2430]\n",
      "Train Epoch 79\n",
      "-------------------------------\n",
      "loss: 1.223124  [   32/ 2430]\n",
      "loss: 1.441428  [  352/ 2430]\n",
      "loss: 0.332782  [  672/ 2430]\n",
      "loss: 0.672001  [  992/ 2430]\n",
      "loss: 1.149037  [ 1312/ 2430]\n",
      "loss: 0.558104  [ 1632/ 2430]\n",
      "loss: 0.527172  [ 1952/ 2430]\n",
      "loss: 0.579097  [ 2272/ 2430]\n",
      "Train Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.249061  [   32/ 2430]\n",
      "loss: 0.621665  [  352/ 2430]\n",
      "loss: 0.593350  [  672/ 2430]\n",
      "loss: 0.451273  [  992/ 2430]\n",
      "loss: 0.755227  [ 1312/ 2430]\n",
      "loss: 1.045574  [ 1632/ 2430]\n",
      "loss: 1.947751  [ 1952/ 2430]\n",
      "loss: 0.761056  [ 2272/ 2430]\n",
      "Train Epoch 81\n",
      "-------------------------------\n",
      "loss: 1.587309  [   32/ 2430]\n",
      "loss: 1.096333  [  352/ 2430]\n",
      "loss: 0.887919  [  672/ 2430]\n",
      "loss: 0.445703  [  992/ 2430]\n",
      "loss: 0.576336  [ 1312/ 2430]\n",
      "loss: 0.655722  [ 1632/ 2430]\n",
      "loss: 0.848078  [ 1952/ 2430]\n",
      "loss: 0.760798  [ 2272/ 2430]\n",
      "Train Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.985289  [   32/ 2430]\n",
      "loss: 0.838573  [  352/ 2430]\n",
      "loss: 0.707903  [  672/ 2430]\n",
      "loss: 0.589580  [  992/ 2430]\n",
      "loss: 0.833082  [ 1312/ 2430]\n",
      "loss: 1.217485  [ 1632/ 2430]\n",
      "loss: 0.625157  [ 1952/ 2430]\n",
      "loss: 0.434262  [ 2272/ 2430]\n",
      "Train Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.897883  [   32/ 2430]\n",
      "loss: 0.600472  [  352/ 2430]\n",
      "loss: 0.785273  [  672/ 2430]\n",
      "loss: 0.869043  [  992/ 2430]\n",
      "loss: 0.683283  [ 1312/ 2430]\n",
      "loss: 0.762691  [ 1632/ 2430]\n",
      "loss: 0.520643  [ 1952/ 2430]\n",
      "loss: 0.761894  [ 2272/ 2430]\n",
      "Train Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.494519  [   32/ 2430]\n",
      "loss: 0.489350  [  352/ 2430]\n",
      "loss: 0.865268  [  672/ 2430]\n",
      "loss: 0.877443  [  992/ 2430]\n",
      "loss: 0.880471  [ 1312/ 2430]\n",
      "loss: 0.688849  [ 1632/ 2430]\n",
      "loss: 1.423913  [ 1952/ 2430]\n",
      "loss: 1.233698  [ 2272/ 2430]\n",
      "Train Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.671933  [   32/ 2430]\n",
      "loss: 1.309567  [  352/ 2430]\n",
      "loss: 0.866268  [  672/ 2430]\n",
      "loss: 0.936635  [  992/ 2430]\n",
      "loss: 0.410033  [ 1312/ 2430]\n",
      "loss: 0.564863  [ 1632/ 2430]\n",
      "loss: 0.433419  [ 1952/ 2430]\n",
      "loss: 0.572222  [ 2272/ 2430]\n",
      "Train Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.601475  [   32/ 2430]\n",
      "loss: 0.679687  [  352/ 2430]\n",
      "loss: 1.046634  [  672/ 2430]\n",
      "loss: 0.753589  [  992/ 2430]\n",
      "loss: 0.910553  [ 1312/ 2430]\n",
      "loss: 0.589293  [ 1632/ 2430]\n",
      "loss: 0.779181  [ 1952/ 2430]\n",
      "loss: 0.435973  [ 2272/ 2430]\n",
      "Train Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.345471  [   32/ 2430]\n",
      "loss: 0.366715  [  352/ 2430]\n",
      "loss: 1.106121  [  672/ 2430]\n",
      "loss: 0.832506  [  992/ 2430]\n",
      "loss: 0.964376  [ 1312/ 2430]\n",
      "loss: 0.457157  [ 1632/ 2430]\n",
      "loss: 0.693658  [ 1952/ 2430]\n",
      "loss: 0.640395  [ 2272/ 2430]\n",
      "Train Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.603933  [   32/ 2430]\n",
      "loss: 0.812325  [  352/ 2430]\n",
      "loss: 1.219375  [  672/ 2430]\n",
      "loss: 0.426009  [  992/ 2430]\n",
      "loss: 0.497725  [ 1312/ 2430]\n",
      "loss: 0.267989  [ 1632/ 2430]\n",
      "loss: 1.300454  [ 1952/ 2430]\n",
      "loss: 0.599103  [ 2272/ 2430]\n",
      "Train Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.707621  [   32/ 2430]\n",
      "loss: 0.608241  [  352/ 2430]\n",
      "loss: 0.415240  [  672/ 2430]\n",
      "loss: 0.466092  [  992/ 2430]\n",
      "loss: 0.612844  [ 1312/ 2430]\n",
      "loss: 0.686104  [ 1632/ 2430]\n",
      "loss: 0.864107  [ 1952/ 2430]\n",
      "loss: 1.260944  [ 2272/ 2430]\n",
      "Train Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.612720  [   32/ 2430]\n",
      "loss: 0.527094  [  352/ 2430]\n",
      "loss: 0.477739  [  672/ 2430]\n",
      "loss: 1.212464  [  992/ 2430]\n",
      "loss: 0.614379  [ 1312/ 2430]\n",
      "loss: 0.660823  [ 1632/ 2430]\n",
      "loss: 0.846585  [ 1952/ 2430]\n",
      "loss: 0.993788  [ 2272/ 2430]\n",
      "Train Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.447957  [   32/ 2430]\n",
      "loss: 1.035696  [  352/ 2430]\n",
      "loss: 0.565128  [  672/ 2430]\n",
      "loss: 0.744811  [  992/ 2430]\n",
      "loss: 0.425687  [ 1312/ 2430]\n",
      "loss: 1.503155  [ 1632/ 2430]\n",
      "loss: 0.532958  [ 1952/ 2430]\n",
      "loss: 0.530260  [ 2272/ 2430]\n",
      "Train Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.592107  [   32/ 2430]\n",
      "loss: 0.486873  [  352/ 2430]\n",
      "loss: 1.460327  [  672/ 2430]\n",
      "loss: 1.773856  [  992/ 2430]\n",
      "loss: 0.461771  [ 1312/ 2430]\n",
      "loss: 0.588666  [ 1632/ 2430]\n",
      "loss: 0.670312  [ 1952/ 2430]\n",
      "loss: 0.683029  [ 2272/ 2430]\n",
      "Train Epoch 93\n",
      "-------------------------------\n",
      "loss: 1.314390  [   32/ 2430]\n",
      "loss: 0.551866  [  352/ 2430]\n",
      "loss: 0.825904  [  672/ 2430]\n",
      "loss: 0.310940  [  992/ 2430]\n",
      "loss: 0.871920  [ 1312/ 2430]\n",
      "loss: 0.787550  [ 1632/ 2430]\n",
      "loss: 0.588392  [ 1952/ 2430]\n",
      "loss: 0.784056  [ 2272/ 2430]\n",
      "Train Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.686087  [   32/ 2430]\n",
      "loss: 1.106644  [  352/ 2430]\n",
      "loss: 0.289562  [  672/ 2430]\n",
      "loss: 0.652833  [  992/ 2430]\n",
      "loss: 0.713844  [ 1312/ 2430]\n",
      "loss: 0.789712  [ 1632/ 2430]\n",
      "loss: 1.122094  [ 1952/ 2430]\n",
      "loss: 0.635284  [ 2272/ 2430]\n",
      "Train Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.511162  [   32/ 2430]\n",
      "loss: 0.619177  [  352/ 2430]\n",
      "loss: 0.692250  [  672/ 2430]\n",
      "loss: 2.255854  [  992/ 2430]\n",
      "loss: 0.323084  [ 1312/ 2430]\n",
      "loss: 0.656582  [ 1632/ 2430]\n",
      "loss: 0.427010  [ 1952/ 2430]\n",
      "loss: 0.565880  [ 2272/ 2430]\n",
      "Train Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.494211  [   32/ 2430]\n",
      "loss: 1.249619  [  352/ 2430]\n",
      "loss: 0.740937  [  672/ 2430]\n",
      "loss: 0.689224  [  992/ 2430]\n",
      "loss: 0.643844  [ 1312/ 2430]\n",
      "loss: 1.330144  [ 1632/ 2430]\n",
      "loss: 0.638646  [ 1952/ 2430]\n",
      "loss: 0.636263  [ 2272/ 2430]\n",
      "Train Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.370289  [   32/ 2430]\n",
      "loss: 1.136188  [  352/ 2430]\n",
      "loss: 0.556765  [  672/ 2430]\n",
      "loss: 1.014125  [  992/ 2430]\n",
      "loss: 0.514034  [ 1312/ 2430]\n",
      "loss: 0.610631  [ 1632/ 2430]\n",
      "loss: 0.516197  [ 1952/ 2430]\n",
      "loss: 1.090775  [ 2272/ 2430]\n",
      "Train Epoch 98\n",
      "-------------------------------\n",
      "loss: 1.412748  [   32/ 2430]\n",
      "loss: 1.063665  [  352/ 2430]\n",
      "loss: 0.550806  [  672/ 2430]\n",
      "loss: 0.387021  [  992/ 2430]\n",
      "loss: 0.847526  [ 1312/ 2430]\n",
      "loss: 1.972613  [ 1632/ 2430]\n",
      "loss: 0.395943  [ 1952/ 2430]\n",
      "loss: 0.653140  [ 2272/ 2430]\n",
      "Train Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.651816  [   32/ 2430]\n",
      "loss: 0.584724  [  352/ 2430]\n",
      "loss: 0.582533  [  672/ 2430]\n",
      "loss: 0.867758  [  992/ 2430]\n",
      "loss: 0.747960  [ 1312/ 2430]\n",
      "loss: 0.408434  [ 1632/ 2430]\n",
      "loss: 0.512181  [ 1952/ 2430]\n",
      "loss: 0.754204  [ 2272/ 2430]\n",
      "Train Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.427678  [   32/ 2430]\n",
      "loss: 0.834889  [  352/ 2430]\n",
      "loss: 0.473050  [  672/ 2430]\n",
      "loss: 1.316184  [  992/ 2430]\n",
      "loss: 0.640926  [ 1312/ 2430]\n",
      "loss: 1.428462  [ 1632/ 2430]\n",
      "loss: 0.991340  [ 1952/ 2430]\n",
      "loss: 0.573532  [ 2272/ 2430]\n",
      "Test Epoch 1\n",
      "-------------------------------\n",
      "loss tensor(0.5106, dtype=torch.float64)\n",
      "loss tensor(1.2555, dtype=torch.float64)\n",
      "loss tensor(0.6759, dtype=torch.float64)\n",
      "loss tensor(0.5980, dtype=torch.float64)\n",
      "loss tensor(0.7595, dtype=torch.float64)\n",
      "loss tensor(1.0003, dtype=torch.float64)\n",
      "loss tensor(0.9114, dtype=torch.float64)\n",
      "loss tensor(0.9689, dtype=torch.float64)\n",
      "loss tensor(0.6681, dtype=torch.float64)\n",
      "Test Epoch 2\n",
      "-------------------------------\n",
      "loss tensor(0.8549, dtype=torch.float64)\n",
      "loss tensor(1.0416, dtype=torch.float64)\n",
      "loss tensor(0.8504, dtype=torch.float64)\n",
      "loss tensor(0.7873, dtype=torch.float64)\n",
      "loss tensor(1.0144, dtype=torch.float64)\n",
      "loss tensor(0.4085, dtype=torch.float64)\n",
      "loss tensor(1.0134, dtype=torch.float64)\n",
      "loss tensor(0.8044, dtype=torch.float64)\n",
      "loss tensor(0.4513, dtype=torch.float64)\n",
      "Test Epoch 3\n",
      "-------------------------------\n",
      "loss tensor(0.6472, dtype=torch.float64)\n",
      "loss tensor(0.9064, dtype=torch.float64)\n",
      "loss tensor(0.7987, dtype=torch.float64)\n",
      "loss tensor(0.9888, dtype=torch.float64)\n",
      "loss tensor(0.7793, dtype=torch.float64)\n",
      "loss tensor(0.8501, dtype=torch.float64)\n",
      "loss tensor(0.8134, dtype=torch.float64)\n",
      "loss tensor(0.7095, dtype=torch.float64)\n",
      "loss tensor(1.0947, dtype=torch.float64)\n",
      "Test Epoch 4\n",
      "-------------------------------\n",
      "loss tensor(0.7346, dtype=torch.float64)\n",
      "loss tensor(0.7958, dtype=torch.float64)\n",
      "loss tensor(0.9916, dtype=torch.float64)\n",
      "loss tensor(0.9472, dtype=torch.float64)\n",
      "loss tensor(0.5842, dtype=torch.float64)\n",
      "loss tensor(1.0634, dtype=torch.float64)\n",
      "loss tensor(0.8547, dtype=torch.float64)\n",
      "loss tensor(0.6901, dtype=torch.float64)\n",
      "loss tensor(0.7105, dtype=torch.float64)\n",
      "Test Epoch 5\n",
      "-------------------------------\n",
      "loss tensor(0.6912, dtype=torch.float64)\n",
      "loss tensor(0.9263, dtype=torch.float64)\n",
      "loss tensor(0.8524, dtype=torch.float64)\n",
      "loss tensor(0.7812, dtype=torch.float64)\n",
      "loss tensor(1.0632, dtype=torch.float64)\n",
      "loss tensor(0.8241, dtype=torch.float64)\n",
      "loss tensor(0.8160, dtype=torch.float64)\n",
      "loss tensor(0.6672, dtype=torch.float64)\n",
      "loss tensor(0.8017, dtype=torch.float64)\n",
      "Test Epoch 6\n",
      "-------------------------------\n",
      "loss tensor(0.8408, dtype=torch.float64)\n",
      "loss tensor(0.6942, dtype=torch.float64)\n",
      "loss tensor(0.9853, dtype=torch.float64)\n",
      "loss tensor(0.8335, dtype=torch.float64)\n",
      "loss tensor(0.6597, dtype=torch.float64)\n",
      "loss tensor(0.7363, dtype=torch.float64)\n",
      "loss tensor(0.9290, dtype=torch.float64)\n",
      "loss tensor(0.8796, dtype=torch.float64)\n",
      "loss tensor(0.9462, dtype=torch.float64)\n",
      "Test Epoch 7\n",
      "-------------------------------\n",
      "loss tensor(1.2189, dtype=torch.float64)\n",
      "loss tensor(1.1112, dtype=torch.float64)\n",
      "loss tensor(0.9461, dtype=torch.float64)\n",
      "loss tensor(0.6216, dtype=torch.float64)\n",
      "loss tensor(0.7550, dtype=torch.float64)\n",
      "loss tensor(0.7186, dtype=torch.float64)\n",
      "loss tensor(0.6283, dtype=torch.float64)\n",
      "loss tensor(0.5690, dtype=torch.float64)\n",
      "loss tensor(0.9223, dtype=torch.float64)\n",
      "Test Epoch 8\n",
      "-------------------------------\n",
      "loss tensor(0.6901, dtype=torch.float64)\n",
      "loss tensor(0.7190, dtype=torch.float64)\n",
      "loss tensor(0.8548, dtype=torch.float64)\n",
      "loss tensor(0.6277, dtype=torch.float64)\n",
      "loss tensor(0.7934, dtype=torch.float64)\n",
      "loss tensor(0.9595, dtype=torch.float64)\n",
      "loss tensor(1.5093, dtype=torch.float64)\n",
      "loss tensor(0.5319, dtype=torch.float64)\n",
      "loss tensor(0.6555, dtype=torch.float64)\n",
      "Test Epoch 9\n",
      "-------------------------------\n",
      "loss tensor(0.4427, dtype=torch.float64)\n",
      "loss tensor(0.9402, dtype=torch.float64)\n",
      "loss tensor(0.7110, dtype=torch.float64)\n",
      "loss tensor(0.7919, dtype=torch.float64)\n",
      "loss tensor(1.0397, dtype=torch.float64)\n",
      "loss tensor(0.8081, dtype=torch.float64)\n",
      "loss tensor(1.0156, dtype=torch.float64)\n",
      "loss tensor(1.0553, dtype=torch.float64)\n",
      "loss tensor(0.3839, dtype=torch.float64)\n",
      "Test Epoch 10\n",
      "-------------------------------\n",
      "loss tensor(0.8847, dtype=torch.float64)\n",
      "loss tensor(0.9062, dtype=torch.float64)\n",
      "loss tensor(0.6464, dtype=torch.float64)\n",
      "loss tensor(0.8407, dtype=torch.float64)\n",
      "loss tensor(0.8790, dtype=torch.float64)\n",
      "loss tensor(0.8150, dtype=torch.float64)\n",
      "loss tensor(0.4450, dtype=torch.float64)\n",
      "loss tensor(1.0865, dtype=torch.float64)\n",
      "loss tensor(1.0721, dtype=torch.float64)\n",
      "Test Epoch 11\n",
      "-------------------------------\n",
      "loss tensor(0.8272, dtype=torch.float64)\n",
      "loss tensor(0.9900, dtype=torch.float64)\n",
      "loss tensor(0.4561, dtype=torch.float64)\n",
      "loss tensor(0.8668, dtype=torch.float64)\n",
      "loss tensor(0.6452, dtype=torch.float64)\n",
      "loss tensor(0.8177, dtype=torch.float64)\n",
      "loss tensor(1.1937, dtype=torch.float64)\n",
      "loss tensor(0.8497, dtype=torch.float64)\n",
      "loss tensor(0.7450, dtype=torch.float64)\n",
      "Test Epoch 12\n",
      "-------------------------------\n",
      "loss tensor(1.1232, dtype=torch.float64)\n",
      "loss tensor(0.8080, dtype=torch.float64)\n",
      "loss tensor(0.7949, dtype=torch.float64)\n",
      "loss tensor(0.6076, dtype=torch.float64)\n",
      "loss tensor(1.1428, dtype=torch.float64)\n",
      "loss tensor(0.7396, dtype=torch.float64)\n",
      "loss tensor(0.9448, dtype=torch.float64)\n",
      "loss tensor(0.5552, dtype=torch.float64)\n",
      "loss tensor(0.5859, dtype=torch.float64)\n",
      "Test Epoch 13\n",
      "-------------------------------\n",
      "loss tensor(0.6718, dtype=torch.float64)\n",
      "loss tensor(0.5554, dtype=torch.float64)\n",
      "loss tensor(1.0399, dtype=torch.float64)\n",
      "loss tensor(0.5719, dtype=torch.float64)\n",
      "loss tensor(1.0520, dtype=torch.float64)\n",
      "loss tensor(0.5987, dtype=torch.float64)\n",
      "loss tensor(0.8660, dtype=torch.float64)\n",
      "loss tensor(1.0894, dtype=torch.float64)\n",
      "loss tensor(1.2053, dtype=torch.float64)\n",
      "Test Epoch 14\n",
      "-------------------------------\n",
      "loss tensor(0.7271, dtype=torch.float64)\n",
      "loss tensor(1.2448, dtype=torch.float64)\n",
      "loss tensor(0.8086, dtype=torch.float64)\n",
      "loss tensor(0.7825, dtype=torch.float64)\n",
      "loss tensor(0.9422, dtype=torch.float64)\n",
      "loss tensor(0.8689, dtype=torch.float64)\n",
      "loss tensor(0.4855, dtype=torch.float64)\n",
      "loss tensor(0.7860, dtype=torch.float64)\n",
      "loss tensor(0.7473, dtype=torch.float64)\n",
      "Test Epoch 15\n",
      "-------------------------------\n",
      "loss tensor(0.7473, dtype=torch.float64)\n",
      "loss tensor(1.0560, dtype=torch.float64)\n",
      "loss tensor(0.6992, dtype=torch.float64)\n",
      "loss tensor(1.2436, dtype=torch.float64)\n",
      "loss tensor(0.8726, dtype=torch.float64)\n",
      "loss tensor(0.5979, dtype=torch.float64)\n",
      "loss tensor(0.7166, dtype=torch.float64)\n",
      "loss tensor(0.6697, dtype=torch.float64)\n",
      "loss tensor(0.8444, dtype=torch.float64)\n",
      "Test Epoch 16\n",
      "-------------------------------\n",
      "loss tensor(1.0444, dtype=torch.float64)\n",
      "loss tensor(1.0362, dtype=torch.float64)\n",
      "loss tensor(0.3427, dtype=torch.float64)\n",
      "loss tensor(0.8157, dtype=torch.float64)\n",
      "loss tensor(0.9615, dtype=torch.float64)\n",
      "loss tensor(0.7552, dtype=torch.float64)\n",
      "loss tensor(0.8382, dtype=torch.float64)\n",
      "loss tensor(0.8178, dtype=torch.float64)\n",
      "loss tensor(0.8246, dtype=torch.float64)\n",
      "Test Epoch 17\n",
      "-------------------------------\n",
      "loss tensor(0.7557, dtype=torch.float64)\n",
      "loss tensor(0.7814, dtype=torch.float64)\n",
      "loss tensor(0.8420, dtype=torch.float64)\n",
      "loss tensor(1.0327, dtype=torch.float64)\n",
      "loss tensor(0.7541, dtype=torch.float64)\n",
      "loss tensor(1.0766, dtype=torch.float64)\n",
      "loss tensor(0.7460, dtype=torch.float64)\n",
      "loss tensor(0.6363, dtype=torch.float64)\n",
      "loss tensor(0.7943, dtype=torch.float64)\n",
      "Test Epoch 18\n",
      "-------------------------------\n",
      "loss tensor(1.3456, dtype=torch.float64)\n",
      "loss tensor(0.8419, dtype=torch.float64)\n",
      "loss tensor(0.8755, dtype=torch.float64)\n",
      "loss tensor(0.5071, dtype=torch.float64)\n",
      "loss tensor(0.8380, dtype=torch.float64)\n",
      "loss tensor(0.7625, dtype=torch.float64)\n",
      "loss tensor(0.5756, dtype=torch.float64)\n",
      "loss tensor(0.7790, dtype=torch.float64)\n",
      "loss tensor(1.0220, dtype=torch.float64)\n",
      "Test Epoch 19\n",
      "-------------------------------\n",
      "loss tensor(1.1410, dtype=torch.float64)\n",
      "loss tensor(0.5620, dtype=torch.float64)\n",
      "loss tensor(0.7772, dtype=torch.float64)\n",
      "loss tensor(0.9980, dtype=torch.float64)\n",
      "loss tensor(0.4478, dtype=torch.float64)\n",
      "loss tensor(1.2229, dtype=torch.float64)\n",
      "loss tensor(0.7296, dtype=torch.float64)\n",
      "loss tensor(0.7707, dtype=torch.float64)\n",
      "loss tensor(0.7389, dtype=torch.float64)\n",
      "Test Epoch 20\n",
      "-------------------------------\n",
      "loss tensor(0.4449, dtype=torch.float64)\n",
      "loss tensor(0.8215, dtype=torch.float64)\n",
      "loss tensor(1.0683, dtype=torch.float64)\n",
      "loss tensor(0.8968, dtype=torch.float64)\n",
      "loss tensor(0.6876, dtype=torch.float64)\n",
      "loss tensor(1.0218, dtype=torch.float64)\n",
      "loss tensor(0.8541, dtype=torch.float64)\n",
      "loss tensor(0.8031, dtype=torch.float64)\n",
      "loss tensor(0.8558, dtype=torch.float64)\n",
      "Test Epoch 21\n",
      "-------------------------------\n",
      "loss tensor(0.7874, dtype=torch.float64)\n",
      "loss tensor(0.6398, dtype=torch.float64)\n",
      "loss tensor(0.7628, dtype=torch.float64)\n",
      "loss tensor(0.3841, dtype=torch.float64)\n",
      "loss tensor(1.0791, dtype=torch.float64)\n",
      "loss tensor(1.2215, dtype=torch.float64)\n",
      "loss tensor(0.9616, dtype=torch.float64)\n",
      "loss tensor(0.7317, dtype=torch.float64)\n",
      "loss tensor(0.9244, dtype=torch.float64)\n",
      "Test Epoch 22\n",
      "-------------------------------\n",
      "loss tensor(0.6751, dtype=torch.float64)\n",
      "loss tensor(0.7134, dtype=torch.float64)\n",
      "loss tensor(0.9065, dtype=torch.float64)\n",
      "loss tensor(0.7678, dtype=torch.float64)\n",
      "loss tensor(1.3652, dtype=torch.float64)\n",
      "loss tensor(0.6455, dtype=torch.float64)\n",
      "loss tensor(0.9999, dtype=torch.float64)\n",
      "loss tensor(0.5740, dtype=torch.float64)\n",
      "loss tensor(0.7430, dtype=torch.float64)\n",
      "Test Epoch 23\n",
      "-------------------------------\n",
      "loss tensor(0.8056, dtype=torch.float64)\n",
      "loss tensor(0.9749, dtype=torch.float64)\n",
      "loss tensor(0.6179, dtype=torch.float64)\n",
      "loss tensor(0.9387, dtype=torch.float64)\n",
      "loss tensor(1.1422, dtype=torch.float64)\n",
      "loss tensor(0.5973, dtype=torch.float64)\n",
      "loss tensor(0.6203, dtype=torch.float64)\n",
      "loss tensor(0.9811, dtype=torch.float64)\n",
      "loss tensor(0.6728, dtype=torch.float64)\n",
      "Test Epoch 24\n",
      "-------------------------------\n",
      "loss tensor(0.9744, dtype=torch.float64)\n",
      "loss tensor(0.8469, dtype=torch.float64)\n",
      "loss tensor(0.7861, dtype=torch.float64)\n",
      "loss tensor(0.9966, dtype=torch.float64)\n",
      "loss tensor(0.7893, dtype=torch.float64)\n",
      "loss tensor(0.6093, dtype=torch.float64)\n",
      "loss tensor(0.7008, dtype=torch.float64)\n",
      "loss tensor(0.7182, dtype=torch.float64)\n",
      "loss tensor(1.2594, dtype=torch.float64)\n",
      "Test Epoch 25\n",
      "-------------------------------\n",
      "loss tensor(0.7376, dtype=torch.float64)\n",
      "loss tensor(0.8719, dtype=torch.float64)\n",
      "loss tensor(1.0024, dtype=torch.float64)\n",
      "loss tensor(0.8481, dtype=torch.float64)\n",
      "loss tensor(0.8144, dtype=torch.float64)\n",
      "loss tensor(0.4610, dtype=torch.float64)\n",
      "loss tensor(0.9088, dtype=torch.float64)\n",
      "loss tensor(0.8536, dtype=torch.float64)\n",
      "loss tensor(1.0844, dtype=torch.float64)\n",
      "Test Epoch 26\n",
      "-------------------------------\n",
      "loss tensor(0.8846, dtype=torch.float64)\n",
      "loss tensor(0.9383, dtype=torch.float64)\n",
      "loss tensor(0.6907, dtype=torch.float64)\n",
      "loss tensor(0.5410, dtype=torch.float64)\n",
      "loss tensor(0.8817, dtype=torch.float64)\n",
      "loss tensor(1.0005, dtype=torch.float64)\n",
      "loss tensor(0.6099, dtype=torch.float64)\n",
      "loss tensor(1.1982, dtype=torch.float64)\n",
      "loss tensor(0.5204, dtype=torch.float64)\n",
      "Test Epoch 27\n",
      "-------------------------------\n",
      "loss tensor(1.2823, dtype=torch.float64)\n",
      "loss tensor(0.5931, dtype=torch.float64)\n",
      "loss tensor(0.9654, dtype=torch.float64)\n",
      "loss tensor(0.6115, dtype=torch.float64)\n",
      "loss tensor(0.9780, dtype=torch.float64)\n",
      "loss tensor(0.6377, dtype=torch.float64)\n",
      "loss tensor(0.9811, dtype=torch.float64)\n",
      "loss tensor(0.6064, dtype=torch.float64)\n",
      "loss tensor(0.7245, dtype=torch.float64)\n",
      "Test Epoch 28\n",
      "-------------------------------\n",
      "loss tensor(0.6674, dtype=torch.float64)\n",
      "loss tensor(0.9490, dtype=torch.float64)\n",
      "loss tensor(0.8679, dtype=torch.float64)\n",
      "loss tensor(1.0742, dtype=torch.float64)\n",
      "loss tensor(0.7570, dtype=torch.float64)\n",
      "loss tensor(0.7062, dtype=torch.float64)\n",
      "loss tensor(0.6013, dtype=torch.float64)\n",
      "loss tensor(0.7163, dtype=torch.float64)\n",
      "loss tensor(1.4471, dtype=torch.float64)\n",
      "Test Epoch 29\n",
      "-------------------------------\n",
      "loss tensor(0.7579, dtype=torch.float64)\n",
      "loss tensor(0.8116, dtype=torch.float64)\n",
      "loss tensor(1.2783, dtype=torch.float64)\n",
      "loss tensor(0.7297, dtype=torch.float64)\n",
      "loss tensor(0.8780, dtype=torch.float64)\n",
      "loss tensor(0.9595, dtype=torch.float64)\n",
      "loss tensor(0.7570, dtype=torch.float64)\n",
      "loss tensor(0.6427, dtype=torch.float64)\n",
      "loss tensor(0.3605, dtype=torch.float64)\n",
      "Test Epoch 30\n",
      "-------------------------------\n",
      "loss tensor(1.4120, dtype=torch.float64)\n",
      "loss tensor(1.0774, dtype=torch.float64)\n",
      "loss tensor(0.7570, dtype=torch.float64)\n",
      "loss tensor(0.5614, dtype=torch.float64)\n",
      "loss tensor(0.7314, dtype=torch.float64)\n",
      "loss tensor(0.8406, dtype=torch.float64)\n",
      "loss tensor(0.8791, dtype=torch.float64)\n",
      "loss tensor(0.4875, dtype=torch.float64)\n",
      "loss tensor(0.5166, dtype=torch.float64)\n",
      "Test Epoch 31\n",
      "-------------------------------\n",
      "loss tensor(1.2042, dtype=torch.float64)\n",
      "loss tensor(0.6632, dtype=torch.float64)\n",
      "loss tensor(0.7367, dtype=torch.float64)\n",
      "loss tensor(0.9756, dtype=torch.float64)\n",
      "loss tensor(0.9098, dtype=torch.float64)\n",
      "loss tensor(0.9838, dtype=torch.float64)\n",
      "loss tensor(0.5838, dtype=torch.float64)\n",
      "loss tensor(0.5400, dtype=torch.float64)\n",
      "loss tensor(0.8578, dtype=torch.float64)\n",
      "Test Epoch 32\n",
      "-------------------------------\n",
      "loss tensor(1.0757, dtype=torch.float64)\n",
      "loss tensor(1.0477, dtype=torch.float64)\n",
      "loss tensor(0.4878, dtype=torch.float64)\n",
      "loss tensor(0.4497, dtype=torch.float64)\n",
      "loss tensor(0.6212, dtype=torch.float64)\n",
      "loss tensor(1.1625, dtype=torch.float64)\n",
      "loss tensor(0.7658, dtype=torch.float64)\n",
      "loss tensor(0.7720, dtype=torch.float64)\n",
      "loss tensor(1.3490, dtype=torch.float64)\n",
      "Test Epoch 33\n",
      "-------------------------------\n",
      "loss tensor(0.9460, dtype=torch.float64)\n",
      "loss tensor(1.3345, dtype=torch.float64)\n",
      "loss tensor(1.0562, dtype=torch.float64)\n",
      "loss tensor(0.5991, dtype=torch.float64)\n",
      "loss tensor(0.8042, dtype=torch.float64)\n",
      "loss tensor(0.4855, dtype=torch.float64)\n",
      "loss tensor(0.7053, dtype=torch.float64)\n",
      "loss tensor(0.6367, dtype=torch.float64)\n",
      "loss tensor(0.9254, dtype=torch.float64)\n",
      "Test Epoch 34\n",
      "-------------------------------\n",
      "loss tensor(0.9023, dtype=torch.float64)\n",
      "loss tensor(0.6734, dtype=torch.float64)\n",
      "loss tensor(0.8237, dtype=torch.float64)\n",
      "loss tensor(0.5364, dtype=torch.float64)\n",
      "loss tensor(0.7125, dtype=torch.float64)\n",
      "loss tensor(1.1860, dtype=torch.float64)\n",
      "loss tensor(0.8929, dtype=torch.float64)\n",
      "loss tensor(0.6786, dtype=torch.float64)\n",
      "loss tensor(1.2952, dtype=torch.float64)\n",
      "Test Epoch 35\n",
      "-------------------------------\n",
      "loss tensor(0.9091, dtype=torch.float64)\n",
      "loss tensor(0.8329, dtype=torch.float64)\n",
      "loss tensor(1.2881, dtype=torch.float64)\n",
      "loss tensor(0.5485, dtype=torch.float64)\n",
      "loss tensor(0.6548, dtype=torch.float64)\n",
      "loss tensor(0.8468, dtype=torch.float64)\n",
      "loss tensor(0.8825, dtype=torch.float64)\n",
      "loss tensor(0.5877, dtype=torch.float64)\n",
      "loss tensor(0.9648, dtype=torch.float64)\n",
      "Test Epoch 36\n",
      "-------------------------------\n",
      "loss tensor(1.1387, dtype=torch.float64)\n",
      "loss tensor(0.6488, dtype=torch.float64)\n",
      "loss tensor(0.5383, dtype=torch.float64)\n",
      "loss tensor(0.7436, dtype=torch.float64)\n",
      "loss tensor(0.7654, dtype=torch.float64)\n",
      "loss tensor(0.8018, dtype=torch.float64)\n",
      "loss tensor(0.7308, dtype=torch.float64)\n",
      "loss tensor(1.1540, dtype=torch.float64)\n",
      "loss tensor(1.0310, dtype=torch.float64)\n",
      "Test Epoch 37\n",
      "-------------------------------\n",
      "loss tensor(1.0600, dtype=torch.float64)\n",
      "loss tensor(1.1737, dtype=torch.float64)\n",
      "loss tensor(0.6213, dtype=torch.float64)\n",
      "loss tensor(1.0100, dtype=torch.float64)\n",
      "loss tensor(0.6655, dtype=torch.float64)\n",
      "loss tensor(0.9868, dtype=torch.float64)\n",
      "loss tensor(0.7410, dtype=torch.float64)\n",
      "loss tensor(0.5292, dtype=torch.float64)\n",
      "loss tensor(0.4230, dtype=torch.float64)\n",
      "Test Epoch 38\n",
      "-------------------------------\n",
      "loss tensor(0.8325, dtype=torch.float64)\n",
      "loss tensor(0.8505, dtype=torch.float64)\n",
      "loss tensor(0.6199, dtype=torch.float64)\n",
      "loss tensor(1.0583, dtype=torch.float64)\n",
      "loss tensor(0.7151, dtype=torch.float64)\n",
      "loss tensor(0.9150, dtype=torch.float64)\n",
      "loss tensor(0.7702, dtype=torch.float64)\n",
      "loss tensor(0.8651, dtype=torch.float64)\n",
      "loss tensor(0.7904, dtype=torch.float64)\n",
      "Test Epoch 39\n",
      "-------------------------------\n",
      "loss tensor(0.4759, dtype=torch.float64)\n",
      "loss tensor(0.3917, dtype=torch.float64)\n",
      "loss tensor(0.9213, dtype=torch.float64)\n",
      "loss tensor(1.0353, dtype=torch.float64)\n",
      "loss tensor(0.6615, dtype=torch.float64)\n",
      "loss tensor(1.1453, dtype=torch.float64)\n",
      "loss tensor(1.1406, dtype=torch.float64)\n",
      "loss tensor(0.7858, dtype=torch.float64)\n",
      "loss tensor(0.9489, dtype=torch.float64)\n",
      "Test Epoch 40\n",
      "-------------------------------\n",
      "loss tensor(0.9680, dtype=torch.float64)\n",
      "loss tensor(0.9685, dtype=torch.float64)\n",
      "loss tensor(0.6247, dtype=torch.float64)\n",
      "loss tensor(0.7032, dtype=torch.float64)\n",
      "loss tensor(0.8600, dtype=torch.float64)\n",
      "loss tensor(0.7061, dtype=torch.float64)\n",
      "loss tensor(0.8717, dtype=torch.float64)\n",
      "loss tensor(0.7973, dtype=torch.float64)\n",
      "loss tensor(1.0808, dtype=torch.float64)\n",
      "Test Epoch 41\n",
      "-------------------------------\n",
      "loss tensor(0.8516, dtype=torch.float64)\n",
      "loss tensor(0.7865, dtype=torch.float64)\n",
      "loss tensor(0.8690, dtype=torch.float64)\n",
      "loss tensor(0.8945, dtype=torch.float64)\n",
      "loss tensor(0.9962, dtype=torch.float64)\n",
      "loss tensor(0.7680, dtype=torch.float64)\n",
      "loss tensor(0.8006, dtype=torch.float64)\n",
      "loss tensor(0.6933, dtype=torch.float64)\n",
      "loss tensor(0.7149, dtype=torch.float64)\n",
      "Test Epoch 42\n",
      "-------------------------------\n",
      "loss tensor(0.9044, dtype=torch.float64)\n",
      "loss tensor(0.7610, dtype=torch.float64)\n",
      "loss tensor(0.6826, dtype=torch.float64)\n",
      "loss tensor(0.7880, dtype=torch.float64)\n",
      "loss tensor(0.8904, dtype=torch.float64)\n",
      "loss tensor(1.0131, dtype=torch.float64)\n",
      "loss tensor(1.1089, dtype=torch.float64)\n",
      "loss tensor(0.7328, dtype=torch.float64)\n",
      "loss tensor(0.2082, dtype=torch.float64)\n",
      "Test Epoch 43\n",
      "-------------------------------\n",
      "loss tensor(0.9779, dtype=torch.float64)\n",
      "loss tensor(0.6698, dtype=torch.float64)\n",
      "loss tensor(0.6937, dtype=torch.float64)\n",
      "loss tensor(1.0233, dtype=torch.float64)\n",
      "loss tensor(0.5823, dtype=torch.float64)\n",
      "loss tensor(1.0247, dtype=torch.float64)\n",
      "loss tensor(0.5634, dtype=torch.float64)\n",
      "loss tensor(1.0167, dtype=torch.float64)\n",
      "loss tensor(0.9617, dtype=torch.float64)\n",
      "Test Epoch 44\n",
      "-------------------------------\n",
      "loss tensor(0.7341, dtype=torch.float64)\n",
      "loss tensor(0.9485, dtype=torch.float64)\n",
      "loss tensor(1.1516, dtype=torch.float64)\n",
      "loss tensor(1.2062, dtype=torch.float64)\n",
      "loss tensor(0.8070, dtype=torch.float64)\n",
      "loss tensor(0.3969, dtype=torch.float64)\n",
      "loss tensor(0.6896, dtype=torch.float64)\n",
      "loss tensor(0.8332, dtype=torch.float64)\n",
      "loss tensor(0.4694, dtype=torch.float64)\n",
      "Test Epoch 45\n",
      "-------------------------------\n",
      "loss tensor(1.2008, dtype=torch.float64)\n",
      "loss tensor(0.8815, dtype=torch.float64)\n",
      "loss tensor(0.9756, dtype=torch.float64)\n",
      "loss tensor(0.8690, dtype=torch.float64)\n",
      "loss tensor(0.8493, dtype=torch.float64)\n",
      "loss tensor(0.7740, dtype=torch.float64)\n",
      "loss tensor(0.3867, dtype=torch.float64)\n",
      "loss tensor(0.7466, dtype=torch.float64)\n",
      "loss tensor(0.6604, dtype=torch.float64)\n",
      "Test Epoch 46\n",
      "-------------------------------\n",
      "loss tensor(0.4647, dtype=torch.float64)\n",
      "loss tensor(0.7519, dtype=torch.float64)\n",
      "loss tensor(0.6698, dtype=torch.float64)\n",
      "loss tensor(0.8054, dtype=torch.float64)\n",
      "loss tensor(0.9413, dtype=torch.float64)\n",
      "loss tensor(0.9497, dtype=torch.float64)\n",
      "loss tensor(1.1717, dtype=torch.float64)\n",
      "loss tensor(1.0345, dtype=torch.float64)\n",
      "loss tensor(0.4190, dtype=torch.float64)\n",
      "Test Epoch 47\n",
      "-------------------------------\n",
      "loss tensor(0.8512, dtype=torch.float64)\n",
      "loss tensor(1.0523, dtype=torch.float64)\n",
      "loss tensor(0.5657, dtype=torch.float64)\n",
      "loss tensor(0.7333, dtype=torch.float64)\n",
      "loss tensor(1.0266, dtype=torch.float64)\n",
      "loss tensor(0.5352, dtype=torch.float64)\n",
      "loss tensor(0.8569, dtype=torch.float64)\n",
      "loss tensor(0.9877, dtype=torch.float64)\n",
      "loss tensor(0.8310, dtype=torch.float64)\n",
      "Test Epoch 48\n",
      "-------------------------------\n",
      "loss tensor(1.1940, dtype=torch.float64)\n",
      "loss tensor(0.8234, dtype=torch.float64)\n",
      "loss tensor(0.4039, dtype=torch.float64)\n",
      "loss tensor(0.7900, dtype=torch.float64)\n",
      "loss tensor(0.7011, dtype=torch.float64)\n",
      "loss tensor(0.7930, dtype=torch.float64)\n",
      "loss tensor(1.2662, dtype=torch.float64)\n",
      "loss tensor(0.6731, dtype=torch.float64)\n",
      "loss tensor(0.7492, dtype=torch.float64)\n",
      "Test Epoch 49\n",
      "-------------------------------\n",
      "loss tensor(1.0858, dtype=torch.float64)\n",
      "loss tensor(0.9595, dtype=torch.float64)\n",
      "loss tensor(1.1570, dtype=torch.float64)\n",
      "loss tensor(0.7397, dtype=torch.float64)\n",
      "loss tensor(0.8080, dtype=torch.float64)\n",
      "loss tensor(0.6222, dtype=torch.float64)\n",
      "loss tensor(0.7221, dtype=torch.float64)\n",
      "loss tensor(0.7568, dtype=torch.float64)\n",
      "loss tensor(0.2775, dtype=torch.float64)\n",
      "Test Epoch 50\n",
      "-------------------------------\n",
      "loss tensor(0.8108, dtype=torch.float64)\n",
      "loss tensor(0.8679, dtype=torch.float64)\n",
      "loss tensor(0.4838, dtype=torch.float64)\n",
      "loss tensor(0.8486, dtype=torch.float64)\n",
      "loss tensor(1.2043, dtype=torch.float64)\n",
      "loss tensor(0.8207, dtype=torch.float64)\n",
      "loss tensor(1.1235, dtype=torch.float64)\n",
      "loss tensor(0.5689, dtype=torch.float64)\n",
      "loss tensor(0.5575, dtype=torch.float64)\n",
      "Test Epoch 51\n",
      "-------------------------------\n",
      "loss tensor(0.9497, dtype=torch.float64)\n",
      "loss tensor(0.8758, dtype=torch.float64)\n",
      "loss tensor(0.9186, dtype=torch.float64)\n",
      "loss tensor(1.0955, dtype=torch.float64)\n",
      "loss tensor(0.6173, dtype=torch.float64)\n",
      "loss tensor(0.6390, dtype=torch.float64)\n",
      "loss tensor(1.2142, dtype=torch.float64)\n",
      "loss tensor(0.4935, dtype=torch.float64)\n",
      "loss tensor(0.3864, dtype=torch.float64)\n",
      "Test Epoch 52\n",
      "-------------------------------\n",
      "loss tensor(0.6340, dtype=torch.float64)\n",
      "loss tensor(0.9694, dtype=torch.float64)\n",
      "loss tensor(0.7080, dtype=torch.float64)\n",
      "loss tensor(1.0006, dtype=torch.float64)\n",
      "loss tensor(0.7566, dtype=torch.float64)\n",
      "loss tensor(1.0722, dtype=torch.float64)\n",
      "loss tensor(0.3922, dtype=torch.float64)\n",
      "loss tensor(1.2498, dtype=torch.float64)\n",
      "loss tensor(0.4330, dtype=torch.float64)\n",
      "Test Epoch 53\n",
      "-------------------------------\n",
      "loss tensor(0.6332, dtype=torch.float64)\n",
      "loss tensor(0.8654, dtype=torch.float64)\n",
      "loss tensor(0.6933, dtype=torch.float64)\n",
      "loss tensor(0.7477, dtype=torch.float64)\n",
      "loss tensor(0.9476, dtype=torch.float64)\n",
      "loss tensor(1.3559, dtype=torch.float64)\n",
      "loss tensor(0.8328, dtype=torch.float64)\n",
      "loss tensor(0.6624, dtype=torch.float64)\n",
      "loss tensor(0.5351, dtype=torch.float64)\n",
      "Test Epoch 54\n",
      "-------------------------------\n",
      "loss tensor(0.7032, dtype=torch.float64)\n",
      "loss tensor(1.0966, dtype=torch.float64)\n",
      "loss tensor(0.8481, dtype=torch.float64)\n",
      "loss tensor(0.6542, dtype=torch.float64)\n",
      "loss tensor(0.8489, dtype=torch.float64)\n",
      "loss tensor(1.0358, dtype=torch.float64)\n",
      "loss tensor(0.4166, dtype=torch.float64)\n",
      "loss tensor(0.8454, dtype=torch.float64)\n",
      "loss tensor(1.1968, dtype=torch.float64)\n",
      "Test Epoch 55\n",
      "-------------------------------\n",
      "loss tensor(0.7473, dtype=torch.float64)\n",
      "loss tensor(0.9311, dtype=torch.float64)\n",
      "loss tensor(0.6398, dtype=torch.float64)\n",
      "loss tensor(0.6746, dtype=torch.float64)\n",
      "loss tensor(0.7424, dtype=torch.float64)\n",
      "loss tensor(1.1845, dtype=torch.float64)\n",
      "loss tensor(0.7535, dtype=torch.float64)\n",
      "loss tensor(1.0164, dtype=torch.float64)\n",
      "loss tensor(0.6464, dtype=torch.float64)\n",
      "Test Epoch 56\n",
      "-------------------------------\n",
      "loss tensor(0.6031, dtype=torch.float64)\n",
      "loss tensor(0.9325, dtype=torch.float64)\n",
      "loss tensor(1.0597, dtype=torch.float64)\n",
      "loss tensor(0.7010, dtype=torch.float64)\n",
      "loss tensor(0.5225, dtype=torch.float64)\n",
      "loss tensor(0.8683, dtype=torch.float64)\n",
      "loss tensor(1.2478, dtype=torch.float64)\n",
      "loss tensor(0.6547, dtype=torch.float64)\n",
      "loss tensor(0.8753, dtype=torch.float64)\n",
      "Test Epoch 57\n",
      "-------------------------------\n",
      "loss tensor(0.8140, dtype=torch.float64)\n",
      "loss tensor(0.6557, dtype=torch.float64)\n",
      "loss tensor(1.0752, dtype=torch.float64)\n",
      "loss tensor(0.4841, dtype=torch.float64)\n",
      "loss tensor(1.0729, dtype=torch.float64)\n",
      "loss tensor(1.0539, dtype=torch.float64)\n",
      "loss tensor(0.7204, dtype=torch.float64)\n",
      "loss tensor(0.7419, dtype=torch.float64)\n",
      "loss tensor(0.8098, dtype=torch.float64)\n",
      "Test Epoch 58\n",
      "-------------------------------\n",
      "loss tensor(0.8615, dtype=torch.float64)\n",
      "loss tensor(0.8853, dtype=torch.float64)\n",
      "loss tensor(0.8199, dtype=torch.float64)\n",
      "loss tensor(0.7397, dtype=torch.float64)\n",
      "loss tensor(0.7543, dtype=torch.float64)\n",
      "loss tensor(0.7720, dtype=torch.float64)\n",
      "loss tensor(1.1541, dtype=torch.float64)\n",
      "loss tensor(0.6225, dtype=torch.float64)\n",
      "loss tensor(0.8302, dtype=torch.float64)\n",
      "Test Epoch 59\n",
      "-------------------------------\n",
      "loss tensor(1.1156, dtype=torch.float64)\n",
      "loss tensor(1.0454, dtype=torch.float64)\n",
      "loss tensor(0.6065, dtype=torch.float64)\n",
      "loss tensor(0.5011, dtype=torch.float64)\n",
      "loss tensor(0.6202, dtype=torch.float64)\n",
      "loss tensor(1.2291, dtype=torch.float64)\n",
      "loss tensor(0.7846, dtype=torch.float64)\n",
      "loss tensor(0.5620, dtype=torch.float64)\n",
      "loss tensor(1.1612, dtype=torch.float64)\n",
      "Test Epoch 60\n",
      "-------------------------------\n",
      "loss tensor(0.8414, dtype=torch.float64)\n",
      "loss tensor(1.0077, dtype=torch.float64)\n",
      "loss tensor(0.8043, dtype=torch.float64)\n",
      "loss tensor(0.8395, dtype=torch.float64)\n",
      "loss tensor(1.1266, dtype=torch.float64)\n",
      "loss tensor(0.8834, dtype=torch.float64)\n",
      "loss tensor(0.7452, dtype=torch.float64)\n",
      "loss tensor(0.3879, dtype=torch.float64)\n",
      "loss tensor(0.7689, dtype=torch.float64)\n",
      "Test Epoch 61\n",
      "-------------------------------\n",
      "loss tensor(1.2613, dtype=torch.float64)\n",
      "loss tensor(0.5593, dtype=torch.float64)\n",
      "loss tensor(0.5450, dtype=torch.float64)\n",
      "loss tensor(0.8727, dtype=torch.float64)\n",
      "loss tensor(1.3862, dtype=torch.float64)\n",
      "loss tensor(0.7159, dtype=torch.float64)\n",
      "loss tensor(0.6321, dtype=torch.float64)\n",
      "loss tensor(0.7564, dtype=torch.float64)\n",
      "loss tensor(0.5566, dtype=torch.float64)\n",
      "Test Epoch 62\n",
      "-------------------------------\n",
      "loss tensor(0.8695, dtype=torch.float64)\n",
      "loss tensor(1.0445, dtype=torch.float64)\n",
      "loss tensor(0.6501, dtype=torch.float64)\n",
      "loss tensor(0.8121, dtype=torch.float64)\n",
      "loss tensor(0.7957, dtype=torch.float64)\n",
      "loss tensor(0.8861, dtype=torch.float64)\n",
      "loss tensor(0.9254, dtype=torch.float64)\n",
      "loss tensor(0.8383, dtype=torch.float64)\n",
      "loss tensor(0.3443, dtype=torch.float64)\n",
      "Test Epoch 63\n",
      "-------------------------------\n",
      "loss tensor(0.6391, dtype=torch.float64)\n",
      "loss tensor(0.7255, dtype=torch.float64)\n",
      "loss tensor(0.8297, dtype=torch.float64)\n",
      "loss tensor(0.6507, dtype=torch.float64)\n",
      "loss tensor(1.1930, dtype=torch.float64)\n",
      "loss tensor(0.8324, dtype=torch.float64)\n",
      "loss tensor(0.9797, dtype=torch.float64)\n",
      "loss tensor(0.7785, dtype=torch.float64)\n",
      "loss tensor(0.7857, dtype=torch.float64)\n",
      "Test Epoch 64\n",
      "-------------------------------\n",
      "loss tensor(0.8731, dtype=torch.float64)\n",
      "loss tensor(0.8356, dtype=torch.float64)\n",
      "loss tensor(1.2282, dtype=torch.float64)\n",
      "loss tensor(0.6797, dtype=torch.float64)\n",
      "loss tensor(0.5657, dtype=torch.float64)\n",
      "loss tensor(0.4383, dtype=torch.float64)\n",
      "loss tensor(1.0138, dtype=torch.float64)\n",
      "loss tensor(0.8302, dtype=torch.float64)\n",
      "loss tensor(1.1605, dtype=torch.float64)\n",
      "Test Epoch 65\n",
      "-------------------------------\n",
      "loss tensor(0.9517, dtype=torch.float64)\n",
      "loss tensor(0.6359, dtype=torch.float64)\n",
      "loss tensor(0.7085, dtype=torch.float64)\n",
      "loss tensor(0.7354, dtype=torch.float64)\n",
      "loss tensor(0.8945, dtype=torch.float64)\n",
      "loss tensor(0.9165, dtype=torch.float64)\n",
      "loss tensor(0.9143, dtype=torch.float64)\n",
      "loss tensor(1.0321, dtype=torch.float64)\n",
      "loss tensor(0.4194, dtype=torch.float64)\n",
      "Test Epoch 66\n",
      "-------------------------------\n",
      "loss tensor(0.7073, dtype=torch.float64)\n",
      "loss tensor(0.5132, dtype=torch.float64)\n",
      "loss tensor(1.5021, dtype=torch.float64)\n",
      "loss tensor(0.6293, dtype=torch.float64)\n",
      "loss tensor(0.6790, dtype=torch.float64)\n",
      "loss tensor(0.7656, dtype=torch.float64)\n",
      "loss tensor(0.6416, dtype=torch.float64)\n",
      "loss tensor(1.2060, dtype=torch.float64)\n",
      "loss tensor(0.7505, dtype=torch.float64)\n",
      "Test Epoch 67\n",
      "-------------------------------\n",
      "loss tensor(0.7933, dtype=torch.float64)\n",
      "loss tensor(0.8703, dtype=torch.float64)\n",
      "loss tensor(0.7401, dtype=torch.float64)\n",
      "loss tensor(0.7024, dtype=torch.float64)\n",
      "loss tensor(0.8511, dtype=torch.float64)\n",
      "loss tensor(0.5784, dtype=torch.float64)\n",
      "loss tensor(1.0026, dtype=torch.float64)\n",
      "loss tensor(1.0174, dtype=torch.float64)\n",
      "loss tensor(0.9527, dtype=torch.float64)\n",
      "Test Epoch 68\n",
      "-------------------------------\n",
      "loss tensor(0.8073, dtype=torch.float64)\n",
      "loss tensor(0.6402, dtype=torch.float64)\n",
      "loss tensor(0.7327, dtype=torch.float64)\n",
      "loss tensor(0.9959, dtype=torch.float64)\n",
      "loss tensor(0.7424, dtype=torch.float64)\n",
      "loss tensor(1.0115, dtype=torch.float64)\n",
      "loss tensor(0.7098, dtype=torch.float64)\n",
      "loss tensor(0.9475, dtype=torch.float64)\n",
      "loss tensor(0.8804, dtype=torch.float64)\n",
      "Test Epoch 69\n",
      "-------------------------------\n",
      "loss tensor(0.6808, dtype=torch.float64)\n",
      "loss tensor(0.5867, dtype=torch.float64)\n",
      "loss tensor(1.0739, dtype=torch.float64)\n",
      "loss tensor(1.2984, dtype=torch.float64)\n",
      "loss tensor(0.5758, dtype=torch.float64)\n",
      "loss tensor(0.4845, dtype=torch.float64)\n",
      "loss tensor(1.1036, dtype=torch.float64)\n",
      "loss tensor(0.8228, dtype=torch.float64)\n",
      "loss tensor(0.7905, dtype=torch.float64)\n",
      "Test Epoch 70\n",
      "-------------------------------\n",
      "loss tensor(0.6743, dtype=torch.float64)\n",
      "loss tensor(0.5917, dtype=torch.float64)\n",
      "loss tensor(0.7191, dtype=torch.float64)\n",
      "loss tensor(1.2461, dtype=torch.float64)\n",
      "loss tensor(0.7411, dtype=torch.float64)\n",
      "loss tensor(0.6588, dtype=torch.float64)\n",
      "loss tensor(1.2050, dtype=torch.float64)\n",
      "loss tensor(0.9024, dtype=torch.float64)\n",
      "loss tensor(0.5347, dtype=torch.float64)\n",
      "Test Epoch 71\n",
      "-------------------------------\n",
      "loss tensor(0.9716, dtype=torch.float64)\n",
      "loss tensor(0.9235, dtype=torch.float64)\n",
      "loss tensor(0.8435, dtype=torch.float64)\n",
      "loss tensor(0.5294, dtype=torch.float64)\n",
      "loss tensor(0.6276, dtype=torch.float64)\n",
      "loss tensor(1.0451, dtype=torch.float64)\n",
      "loss tensor(0.8095, dtype=torch.float64)\n",
      "loss tensor(0.9295, dtype=torch.float64)\n",
      "loss tensor(0.6691, dtype=torch.float64)\n",
      "Test Epoch 72\n",
      "-------------------------------\n",
      "loss tensor(0.8821, dtype=torch.float64)\n",
      "loss tensor(0.7202, dtype=torch.float64)\n",
      "loss tensor(0.6231, dtype=torch.float64)\n",
      "loss tensor(0.7999, dtype=torch.float64)\n",
      "loss tensor(0.7476, dtype=torch.float64)\n",
      "loss tensor(1.4090, dtype=torch.float64)\n",
      "loss tensor(0.6495, dtype=torch.float64)\n",
      "loss tensor(0.9104, dtype=torch.float64)\n",
      "loss tensor(0.5269, dtype=torch.float64)\n",
      "Test Epoch 73\n",
      "-------------------------------\n",
      "loss tensor(0.8306, dtype=torch.float64)\n",
      "loss tensor(0.6141, dtype=torch.float64)\n",
      "loss tensor(0.4185, dtype=torch.float64)\n",
      "loss tensor(0.9464, dtype=torch.float64)\n",
      "loss tensor(1.0477, dtype=torch.float64)\n",
      "loss tensor(0.7307, dtype=torch.float64)\n",
      "loss tensor(0.9997, dtype=torch.float64)\n",
      "loss tensor(0.8018, dtype=torch.float64)\n",
      "loss tensor(1.3327, dtype=torch.float64)\n",
      "Test Epoch 74\n",
      "-------------------------------\n",
      "loss tensor(0.5017, dtype=torch.float64)\n",
      "loss tensor(0.8851, dtype=torch.float64)\n",
      "loss tensor(1.5690, dtype=torch.float64)\n",
      "loss tensor(0.9006, dtype=torch.float64)\n",
      "loss tensor(0.7369, dtype=torch.float64)\n",
      "loss tensor(0.6348, dtype=torch.float64)\n",
      "loss tensor(0.6041, dtype=torch.float64)\n",
      "loss tensor(0.8148, dtype=torch.float64)\n",
      "loss tensor(0.7439, dtype=torch.float64)\n",
      "Test Epoch 75\n",
      "-------------------------------\n",
      "loss tensor(1.3057, dtype=torch.float64)\n",
      "loss tensor(0.5456, dtype=torch.float64)\n",
      "loss tensor(0.8115, dtype=torch.float64)\n",
      "loss tensor(0.5879, dtype=torch.float64)\n",
      "loss tensor(0.8238, dtype=torch.float64)\n",
      "loss tensor(0.6336, dtype=torch.float64)\n",
      "loss tensor(0.7705, dtype=torch.float64)\n",
      "loss tensor(1.2363, dtype=torch.float64)\n",
      "loss tensor(0.5889, dtype=torch.float64)\n",
      "Test Epoch 76\n",
      "-------------------------------\n",
      "loss tensor(1.0803, dtype=torch.float64)\n",
      "loss tensor(0.7784, dtype=torch.float64)\n",
      "loss tensor(1.1111, dtype=torch.float64)\n",
      "loss tensor(0.8172, dtype=torch.float64)\n",
      "loss tensor(0.8732, dtype=torch.float64)\n",
      "loss tensor(0.5323, dtype=torch.float64)\n",
      "loss tensor(0.8032, dtype=torch.float64)\n",
      "loss tensor(0.6195, dtype=torch.float64)\n",
      "loss tensor(0.8166, dtype=torch.float64)\n",
      "Test Epoch 77\n",
      "-------------------------------\n",
      "loss tensor(0.8100, dtype=torch.float64)\n",
      "loss tensor(0.6625, dtype=torch.float64)\n",
      "loss tensor(0.9040, dtype=torch.float64)\n",
      "loss tensor(0.6626, dtype=torch.float64)\n",
      "loss tensor(0.7849, dtype=torch.float64)\n",
      "loss tensor(0.9063, dtype=torch.float64)\n",
      "loss tensor(0.8266, dtype=torch.float64)\n",
      "loss tensor(1.2178, dtype=torch.float64)\n",
      "loss tensor(0.4521, dtype=torch.float64)\n",
      "Test Epoch 78\n",
      "-------------------------------\n",
      "loss tensor(0.7576, dtype=torch.float64)\n",
      "loss tensor(0.5883, dtype=torch.float64)\n",
      "loss tensor(0.7659, dtype=torch.float64)\n",
      "loss tensor(0.8482, dtype=torch.float64)\n",
      "loss tensor(1.0732, dtype=torch.float64)\n",
      "loss tensor(0.6769, dtype=torch.float64)\n",
      "loss tensor(0.7319, dtype=torch.float64)\n",
      "loss tensor(1.2290, dtype=torch.float64)\n",
      "loss tensor(0.6889, dtype=torch.float64)\n",
      "Test Epoch 79\n",
      "-------------------------------\n",
      "loss tensor(0.6530, dtype=torch.float64)\n",
      "loss tensor(0.8811, dtype=torch.float64)\n",
      "loss tensor(0.9450, dtype=torch.float64)\n",
      "loss tensor(0.9811, dtype=torch.float64)\n",
      "loss tensor(1.0098, dtype=torch.float64)\n",
      "loss tensor(1.0400, dtype=torch.float64)\n",
      "loss tensor(0.8341, dtype=torch.float64)\n",
      "loss tensor(0.4216, dtype=torch.float64)\n",
      "loss tensor(0.4726, dtype=torch.float64)\n",
      "Test Epoch 80\n",
      "-------------------------------\n",
      "loss tensor(0.8984, dtype=torch.float64)\n",
      "loss tensor(0.7416, dtype=torch.float64)\n",
      "loss tensor(1.0129, dtype=torch.float64)\n",
      "loss tensor(0.5751, dtype=torch.float64)\n",
      "loss tensor(0.7626, dtype=torch.float64)\n",
      "loss tensor(0.9260, dtype=torch.float64)\n",
      "loss tensor(0.9012, dtype=torch.float64)\n",
      "loss tensor(0.8397, dtype=torch.float64)\n",
      "loss tensor(0.7194, dtype=torch.float64)\n",
      "Test Epoch 81\n",
      "-------------------------------\n",
      "loss tensor(1.0357, dtype=torch.float64)\n",
      "loss tensor(1.0608, dtype=torch.float64)\n",
      "loss tensor(0.5814, dtype=torch.float64)\n",
      "loss tensor(0.6026, dtype=torch.float64)\n",
      "loss tensor(1.0469, dtype=torch.float64)\n",
      "loss tensor(0.7789, dtype=torch.float64)\n",
      "loss tensor(0.8830, dtype=torch.float64)\n",
      "loss tensor(0.4329, dtype=torch.float64)\n",
      "loss tensor(1.2578, dtype=torch.float64)\n",
      "Test Epoch 82\n",
      "-------------------------------\n",
      "loss tensor(0.7508, dtype=torch.float64)\n",
      "loss tensor(0.5805, dtype=torch.float64)\n",
      "loss tensor(0.8020, dtype=torch.float64)\n",
      "loss tensor(0.9185, dtype=torch.float64)\n",
      "loss tensor(0.4804, dtype=torch.float64)\n",
      "loss tensor(0.6421, dtype=torch.float64)\n",
      "loss tensor(1.1565, dtype=torch.float64)\n",
      "loss tensor(1.1572, dtype=torch.float64)\n",
      "loss tensor(1.1075, dtype=torch.float64)\n",
      "Test Epoch 83\n",
      "-------------------------------\n",
      "loss tensor(0.6944, dtype=torch.float64)\n",
      "loss tensor(0.8357, dtype=torch.float64)\n",
      "loss tensor(0.5733, dtype=torch.float64)\n",
      "loss tensor(1.0996, dtype=torch.float64)\n",
      "loss tensor(0.9511, dtype=torch.float64)\n",
      "loss tensor(0.8867, dtype=torch.float64)\n",
      "loss tensor(0.7580, dtype=torch.float64)\n",
      "loss tensor(0.9050, dtype=torch.float64)\n",
      "loss tensor(0.6141, dtype=torch.float64)\n",
      "Test Epoch 84\n",
      "-------------------------------\n",
      "loss tensor(0.8845, dtype=torch.float64)\n",
      "loss tensor(0.7860, dtype=torch.float64)\n",
      "loss tensor(0.9952, dtype=torch.float64)\n",
      "loss tensor(0.8376, dtype=torch.float64)\n",
      "loss tensor(0.7992, dtype=torch.float64)\n",
      "loss tensor(0.6810, dtype=torch.float64)\n",
      "loss tensor(0.9539, dtype=torch.float64)\n",
      "loss tensor(0.8069, dtype=torch.float64)\n",
      "loss tensor(0.5212, dtype=torch.float64)\n",
      "Test Epoch 85\n",
      "-------------------------------\n",
      "loss tensor(0.7537, dtype=torch.float64)\n",
      "loss tensor(0.8632, dtype=torch.float64)\n",
      "loss tensor(1.0293, dtype=torch.float64)\n",
      "loss tensor(0.4674, dtype=torch.float64)\n",
      "loss tensor(0.9146, dtype=torch.float64)\n",
      "loss tensor(1.0378, dtype=torch.float64)\n",
      "loss tensor(0.6647, dtype=torch.float64)\n",
      "loss tensor(0.9099, dtype=torch.float64)\n",
      "loss tensor(0.7586, dtype=torch.float64)\n",
      "Test Epoch 86\n",
      "-------------------------------\n",
      "loss tensor(0.7112, dtype=torch.float64)\n",
      "loss tensor(0.6163, dtype=torch.float64)\n",
      "loss tensor(0.7361, dtype=torch.float64)\n",
      "loss tensor(0.7526, dtype=torch.float64)\n",
      "loss tensor(1.1485, dtype=torch.float64)\n",
      "loss tensor(1.1200, dtype=torch.float64)\n",
      "loss tensor(0.8804, dtype=torch.float64)\n",
      "loss tensor(0.6119, dtype=torch.float64)\n",
      "loss tensor(0.9039, dtype=torch.float64)\n",
      "Test Epoch 87\n",
      "-------------------------------\n",
      "loss tensor(0.9595, dtype=torch.float64)\n",
      "loss tensor(0.6493, dtype=torch.float64)\n",
      "loss tensor(0.7488, dtype=torch.float64)\n",
      "loss tensor(0.8492, dtype=torch.float64)\n",
      "loss tensor(0.6903, dtype=torch.float64)\n",
      "loss tensor(0.8189, dtype=torch.float64)\n",
      "loss tensor(0.8098, dtype=torch.float64)\n",
      "loss tensor(1.2273, dtype=torch.float64)\n",
      "loss tensor(0.5017, dtype=torch.float64)\n",
      "Test Epoch 88\n",
      "-------------------------------\n",
      "loss tensor(0.8507, dtype=torch.float64)\n",
      "loss tensor(1.2040, dtype=torch.float64)\n",
      "loss tensor(1.0524, dtype=torch.float64)\n",
      "loss tensor(0.9090, dtype=torch.float64)\n",
      "loss tensor(0.4282, dtype=torch.float64)\n",
      "loss tensor(0.7294, dtype=torch.float64)\n",
      "loss tensor(0.4595, dtype=torch.float64)\n",
      "loss tensor(1.0582, dtype=torch.float64)\n",
      "loss tensor(0.6424, dtype=torch.float64)\n",
      "Test Epoch 89\n",
      "-------------------------------\n",
      "loss tensor(1.0460, dtype=torch.float64)\n",
      "loss tensor(1.0485, dtype=torch.float64)\n",
      "loss tensor(0.8728, dtype=torch.float64)\n",
      "loss tensor(0.6942, dtype=torch.float64)\n",
      "loss tensor(0.5023, dtype=torch.float64)\n",
      "loss tensor(0.3497, dtype=torch.float64)\n",
      "loss tensor(1.0235, dtype=torch.float64)\n",
      "loss tensor(0.7597, dtype=torch.float64)\n",
      "loss tensor(1.5444, dtype=torch.float64)\n",
      "Test Epoch 90\n",
      "-------------------------------\n",
      "loss tensor(0.9330, dtype=torch.float64)\n",
      "loss tensor(0.6976, dtype=torch.float64)\n",
      "loss tensor(0.7177, dtype=torch.float64)\n",
      "loss tensor(0.8978, dtype=torch.float64)\n",
      "loss tensor(1.2867, dtype=torch.float64)\n",
      "loss tensor(0.6133, dtype=torch.float64)\n",
      "loss tensor(0.6720, dtype=torch.float64)\n",
      "loss tensor(0.7693, dtype=torch.float64)\n",
      "loss tensor(0.8798, dtype=torch.float64)\n",
      "Test Epoch 91\n",
      "-------------------------------\n",
      "loss tensor(0.5542, dtype=torch.float64)\n",
      "loss tensor(0.6851, dtype=torch.float64)\n",
      "loss tensor(1.0926, dtype=torch.float64)\n",
      "loss tensor(0.9161, dtype=torch.float64)\n",
      "loss tensor(0.8248, dtype=torch.float64)\n",
      "loss tensor(0.7504, dtype=torch.float64)\n",
      "loss tensor(0.7624, dtype=torch.float64)\n",
      "loss tensor(0.8963, dtype=torch.float64)\n",
      "loss tensor(1.1215, dtype=torch.float64)\n",
      "Test Epoch 92\n",
      "-------------------------------\n",
      "loss tensor(0.5736, dtype=torch.float64)\n",
      "loss tensor(1.0568, dtype=torch.float64)\n",
      "loss tensor(1.2368, dtype=torch.float64)\n",
      "loss tensor(0.4581, dtype=torch.float64)\n",
      "loss tensor(0.8877, dtype=torch.float64)\n",
      "loss tensor(0.7545, dtype=torch.float64)\n",
      "loss tensor(1.2656, dtype=torch.float64)\n",
      "loss tensor(0.5595, dtype=torch.float64)\n",
      "loss tensor(0.4112, dtype=torch.float64)\n",
      "Test Epoch 93\n",
      "-------------------------------\n",
      "loss tensor(0.6873, dtype=torch.float64)\n",
      "loss tensor(1.0384, dtype=torch.float64)\n",
      "loss tensor(0.4796, dtype=torch.float64)\n",
      "loss tensor(1.4548, dtype=torch.float64)\n",
      "loss tensor(0.6156, dtype=torch.float64)\n",
      "loss tensor(0.6374, dtype=torch.float64)\n",
      "loss tensor(0.8334, dtype=torch.float64)\n",
      "loss tensor(0.9178, dtype=torch.float64)\n",
      "loss tensor(0.7045, dtype=torch.float64)\n",
      "Test Epoch 94\n",
      "-------------------------------\n",
      "loss tensor(1.1633, dtype=torch.float64)\n",
      "loss tensor(0.9959, dtype=torch.float64)\n",
      "loss tensor(0.8467, dtype=torch.float64)\n",
      "loss tensor(1.1555, dtype=torch.float64)\n",
      "loss tensor(0.4400, dtype=torch.float64)\n",
      "loss tensor(0.8845, dtype=torch.float64)\n",
      "loss tensor(0.7884, dtype=torch.float64)\n",
      "loss tensor(0.5559, dtype=torch.float64)\n",
      "loss tensor(0.3254, dtype=torch.float64)\n",
      "Test Epoch 95\n",
      "-------------------------------\n",
      "loss tensor(1.2578, dtype=torch.float64)\n",
      "loss tensor(0.6497, dtype=torch.float64)\n",
      "loss tensor(0.8162, dtype=torch.float64)\n",
      "loss tensor(1.0381, dtype=torch.float64)\n",
      "loss tensor(0.4597, dtype=torch.float64)\n",
      "loss tensor(0.6563, dtype=torch.float64)\n",
      "loss tensor(0.9773, dtype=torch.float64)\n",
      "loss tensor(0.8506, dtype=torch.float64)\n",
      "loss tensor(0.6097, dtype=torch.float64)\n",
      "Test Epoch 96\n",
      "-------------------------------\n",
      "loss tensor(1.1090, dtype=torch.float64)\n",
      "loss tensor(0.7982, dtype=torch.float64)\n",
      "loss tensor(0.5355, dtype=torch.float64)\n",
      "loss tensor(0.6703, dtype=torch.float64)\n",
      "loss tensor(0.5411, dtype=torch.float64)\n",
      "loss tensor(1.0280, dtype=torch.float64)\n",
      "loss tensor(1.1647, dtype=torch.float64)\n",
      "loss tensor(0.9348, dtype=torch.float64)\n",
      "loss tensor(0.4362, dtype=torch.float64)\n",
      "Test Epoch 97\n",
      "-------------------------------\n",
      "loss tensor(0.6516, dtype=torch.float64)\n",
      "loss tensor(0.4773, dtype=torch.float64)\n",
      "loss tensor(1.0227, dtype=torch.float64)\n",
      "loss tensor(0.9137, dtype=torch.float64)\n",
      "loss tensor(0.6836, dtype=torch.float64)\n",
      "loss tensor(0.6367, dtype=torch.float64)\n",
      "loss tensor(1.1618, dtype=torch.float64)\n",
      "loss tensor(0.6077, dtype=torch.float64)\n",
      "loss tensor(1.8684, dtype=torch.float64)\n",
      "Test Epoch 98\n",
      "-------------------------------\n",
      "loss tensor(0.7445, dtype=torch.float64)\n",
      "loss tensor(0.7832, dtype=torch.float64)\n",
      "loss tensor(1.0781, dtype=torch.float64)\n",
      "loss tensor(0.7690, dtype=torch.float64)\n",
      "loss tensor(0.6264, dtype=torch.float64)\n",
      "loss tensor(0.6868, dtype=torch.float64)\n",
      "loss tensor(0.8903, dtype=torch.float64)\n",
      "loss tensor(1.0338, dtype=torch.float64)\n",
      "loss tensor(0.8237, dtype=torch.float64)\n",
      "Test Epoch 99\n",
      "-------------------------------\n",
      "loss tensor(0.4535, dtype=torch.float64)\n",
      "loss tensor(0.7210, dtype=torch.float64)\n",
      "loss tensor(0.6932, dtype=torch.float64)\n",
      "loss tensor(1.2074, dtype=torch.float64)\n",
      "loss tensor(0.4747, dtype=torch.float64)\n",
      "loss tensor(1.1641, dtype=torch.float64)\n",
      "loss tensor(0.9147, dtype=torch.float64)\n",
      "loss tensor(0.8785, dtype=torch.float64)\n",
      "loss tensor(1.0636, dtype=torch.float64)\n",
      "Test Epoch 100\n",
      "-------------------------------\n",
      "loss tensor(0.5397, dtype=torch.float64)\n",
      "loss tensor(0.5335, dtype=torch.float64)\n",
      "loss tensor(1.1529, dtype=torch.float64)\n",
      "loss tensor(1.2259, dtype=torch.float64)\n",
      "loss tensor(0.9083, dtype=torch.float64)\n",
      "loss tensor(0.8719, dtype=torch.float64)\n",
      "loss tensor(0.5688, dtype=torch.float64)\n",
      "loss tensor(0.7530, dtype=torch.float64)\n",
      "loss tensor(0.9565, dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([23.6041, 16.8647,  9.2765, 15.0698, 21.1307, 23.0000, 11.6498, 24.7704,\n",
      "         7.1093, 17.9858,  7.9630,  7.6088, 21.5392, 24.7815,  3.7456, 22.1239,\n",
      "        15.6233, 20.4642, 15.7345, 12.4237, 13.6686,  5.0994, 26.7421, 27.4197,\n",
      "        23.8186, 23.0912, 22.4803,  3.7934,  9.9579, 29.8680, 19.7472, 17.1946,\n",
      "        13.2994, 13.5956,  6.9327, 20.6775, 18.5564,  8.6147, 26.5167, 22.5161,\n",
      "        17.6422, 11.7037, 22.5898,  6.9201,  9.7730, 26.0846,  9.5345, 15.4396,\n",
      "        11.2558, 13.7099,  5.6435, 14.6315, 22.3716, 11.0937, 17.6387, 12.3417,\n",
      "        22.8820, 11.9133, 20.0360,  6.9560, 14.0282,  7.8028, 21.3565, 13.8835,\n",
      "        32.2560, 10.5937,  5.6613, 25.2585,  9.6093, 17.0488, 19.1951, 16.1436,\n",
      "        13.0347, 25.7852, 22.1738, 13.7538,  4.6023, 14.5725,  6.3443, 10.4165,\n",
      "         9.7148,  7.4759,  8.0876, 30.1381, 15.7693, 27.8631, 19.2666,  7.5997,\n",
      "        17.6930, 15.2141, 10.7475, 17.0596,  8.6132, 20.8983,  6.1177, 19.5979,\n",
      "        13.6405, 27.3246, 26.7509, 20.2212, 16.7295, 14.7033,  3.5669, 20.6363,\n",
      "        15.2371, 14.4731, 20.7892, 17.5876, 17.2125, 26.8591, 32.4855, 17.8665,\n",
      "        12.2285, 16.5739, 23.1212, 10.7289, 23.6327,  3.8814, 20.0501,  5.8525,\n",
      "        21.5151, 24.2906, 15.6014, 12.7260, 23.2801, 20.7230, 24.1975, 16.9815,\n",
      "        12.2981, 23.2827, 15.2823, 27.7355, 22.3070, 14.0102, 17.8672,  7.7367,\n",
      "        19.0843, 15.8710,  5.8253, 28.5196,  3.8905, 17.0426, 23.0054, 15.5238,\n",
      "        15.6599,  3.1778, 10.8962,  6.8261, 13.6349, 20.5447, 14.6962, 24.0360,\n",
      "        13.3845,  3.6302,  7.7889, 23.6658, 13.7379, 11.2656, 15.7946,  5.4000,\n",
      "        14.8666, 25.1618, 10.9027, 10.1573, 12.7927,  6.1830,  8.6484, 16.4093,\n",
      "        18.7107,  8.8965, 24.2748, 13.4800,  7.9818, 18.3549, 27.5966, 25.5240,\n",
      "        12.7967, 18.8439,  8.2890, 14.5180, 12.3211, 27.0809, 30.2953,  4.3827,\n",
      "        10.9520,  9.1965,  5.1781, 11.4572, 21.4677, 10.9707, 15.0878, 13.2746,\n",
      "        20.9619, 19.3986, 20.9693, 31.5219, 12.1064, 16.1299, 10.5714, 21.4696,\n",
      "        16.3598,  5.4376, 22.4807, 10.8882, 12.7836, 13.8874, 11.0782,  9.2975,\n",
      "        16.4790, 15.4312, 10.2990, 18.1014, 19.8895, 10.8081, 17.4735, 11.1296,\n",
      "        28.0305, 13.6722, 12.3329,  5.8149, 19.4759, 15.4318, 22.8327, 12.9774,\n",
      "        14.1712, 18.6101, 19.9837, 18.8954, 27.4288, 10.9798, 19.8043, 25.9622,\n",
      "        19.4716,  8.9501,  4.7248, 26.1876, 21.6608, 29.7361, 15.9567, 14.4737,\n",
      "        11.5896,  6.4949,  6.5889, 12.1736, 16.8393, 15.2706, 10.9792, 12.0125,\n",
      "         8.3916, 12.2775, 17.3565, 13.1620, 11.8052, 11.3476,  7.6791,  8.9576,\n",
      "         6.6693, 11.2859, 19.2641, 26.8133, 20.6048,  9.5947, 26.9024, 23.8258,\n",
      "        25.2003, 12.7798, 23.1950, 21.5300, 12.4319, 32.3230, 29.3042, 22.9864,\n",
      "        14.6471, 12.0465, 16.2741,  4.2776, 14.6302, 18.3575,  7.5293, 11.4653,\n",
      "        13.2304, 11.7604,  3.4710, 16.8525, 23.2133, 14.7542, 14.5171,  7.4245,\n",
      "         5.0494, 31.3904, 18.4872, 11.3489,  9.1350,  4.9064, 21.6301, 17.2354,\n",
      "        22.1048, 20.5286, 11.7641, 21.3299, 12.5130, 11.2107, 10.3906, 32.1205,\n",
      "        21.9596, 19.3471, 15.0456,  7.4405, 15.2168, 22.2809, 11.0823, 15.0731,\n",
      "        16.5232, 28.3977, 17.3920,  9.7186, 23.3953, 24.1480, 11.5242, 10.6682,\n",
      "        26.0886, 34.6099,  5.1971,  9.7156, 12.3443, 13.2840, 22.4613,  3.3695,\n",
      "         5.8241, 10.7015,  9.2610,  3.8093, 19.2648,  9.8387, 22.6267, 10.3097,\n",
      "        15.2717, 24.3215, 11.8720, 12.6621,  4.1429,  9.4822, 21.9371,  9.6606,\n",
      "         9.7126,  3.6538, 22.9930, 15.1996, 12.3442, 13.0413, 11.1489, 21.3009,\n",
      "        11.2316, 22.4491, 13.0954, 23.5951, 16.7546, 14.6683,  8.0837, 13.2961,\n",
      "        27.9916, 33.5901, 11.7291, 25.5252,  6.6968, 20.7400, 29.3724, 18.4350,\n",
      "        16.1464, 22.7019,  8.7818, 20.5911, 15.4254, 13.0450, 12.5609, 18.6104,\n",
      "         7.8289, 10.0074, 11.1553, 22.1828, 31.4530, 12.5756, 29.9218, 12.3971,\n",
      "        13.5204, 15.0057, 13.5655, 28.3467, 13.2307,  6.9434, 19.0224,  7.0212,\n",
      "        18.1846, 24.8147,  7.5030, 22.8883,  9.4857, 25.6397, 16.5889, 24.2145,\n",
      "         6.6747, 19.0668,  6.6682, 17.3854, 18.0707,  9.5827, 12.9163, 18.9837,\n",
      "        15.8539, 16.2133, 13.5244, 14.8305,  7.7177, 13.2108, 16.6427, 10.8595,\n",
      "        16.7452, 24.0950,  8.5500, 19.3760, 28.3503, 13.1102,  7.2442, 12.8056,\n",
      "         8.7101, 19.3555, 16.1658, 24.3092, 19.8393, 22.7916, 24.5917, 15.7268,\n",
      "         6.3587, 17.5785, 18.9459, 33.7389, 13.0034,  6.4204, 19.6812, 11.4810,\n",
      "        14.1992, 25.8644, 30.3066, 10.6511, 31.9697, 29.2838, 19.6293, 16.2472,\n",
      "        28.9824, 25.2161,  7.8806, 22.1177, 23.6586,  8.6260, 12.2105, 21.8120,\n",
      "        12.0853, 11.3646, 26.5079, 10.8259, 19.4073, 19.9978, 20.0149, 34.0811,\n",
      "        14.4859, 23.5356,  3.7194, 20.7931, 15.8969, 15.1078,  7.6495, 11.0057,\n",
      "        29.5996, 16.0817,  7.1444, 17.8736, 17.0458, 15.7802, 11.4339, 23.0356,\n",
      "         6.3152, 23.8801, 10.7877, 28.3169, 31.5928, 25.2652, 14.9341, 15.9409,\n",
      "         8.5687, 19.5000, 28.4818, 27.9110, 14.4328, 11.7519, 21.8533, 20.1593,\n",
      "        26.2972, 13.2454,  8.9002,  9.5981, 21.9695, 31.3962,  6.6727,  9.0345,\n",
      "        11.2279, 10.7262, 14.9038,  7.1800, 21.2184,  9.5708, 28.3399, 39.8196,\n",
      "        22.2078, 11.1325, 21.4901,  6.7525, 11.0621,  4.0104, 14.2406, 20.9415,\n",
      "        22.3183,  9.6946, 10.1015, 22.5117, 21.0441,  8.2828,  5.7059, 18.8132,\n",
      "        24.6789,  9.0400,  8.8071, 26.4992, 29.8731, 19.7262, 18.4279,  8.1155,\n",
      "        31.4138,  4.0573,  6.7137, 14.7360, 24.7660, 14.9521, 20.8579, 19.2348,\n",
      "        15.6860, 24.4429, 13.1821, 12.0596, 18.9162, 19.4805, 13.1516, 20.3766,\n",
      "        29.5993, 11.2966, 11.0772, 11.3108, 12.1861, 15.7506,  8.7540, 35.7644,\n",
      "        22.0320, 12.1896,  9.4801, 25.2866,  5.0615,  5.9170, 19.1595, 17.5540,\n",
      "         5.9324, 24.1705, 16.8575, 15.5357, 19.2616, 11.4399,  6.5099, 15.0571,\n",
      "        15.8905, 10.0863, 12.2086, 18.4957, 22.9342, 10.9017, 13.8856, 20.1440,\n",
      "        18.6285, 17.2645, 13.5207, 18.6345,  9.5526,  4.6199, 25.4076, 21.6620,\n",
      "        27.5553, 25.6784,  6.2850, 18.2576, 25.4066, 23.1051, 16.2531, 10.9738,\n",
      "        22.9753, 27.8581, 20.3467, 12.4623, 11.0188, 20.3937, 23.4827, 30.9252,\n",
      "        24.7714,  7.4226, 17.2047, 15.4547, 30.5912, 21.3131, 18.2965, 27.4305,\n",
      "        19.4060, 13.6395, 20.0207, 13.2232, 16.5511, 24.6225,  6.2005,  6.3476,\n",
      "        20.2418, 14.2842, 13.4340, 27.6574, 20.5194, 15.6976, 18.0723, 28.7313,\n",
      "        18.9892, 12.0956, 27.5993, 10.3055, 13.8935, 24.3459, 25.1674, 14.1371,\n",
      "        17.1249,  6.2200, 21.8098, 11.8613, 13.1602, 22.2543, 32.9558, 24.7583,\n",
      "        16.2439, 17.1746, 26.4526, 18.0945, 25.5270, 30.2699,  7.1666,  7.3739,\n",
      "         6.5289, 18.5066, 27.3828, 16.2577, 34.4784,  6.2891, 13.9817, 40.5172,\n",
      "         8.4315, 16.0687, 10.6870, 16.2277,  6.8698,  6.9678,  7.1724,  3.7523,\n",
      "        25.3361,  7.4011, 13.9056, 13.2854, 11.4853, 27.7114,  3.2515, 16.7629,\n",
      "        25.6112, 29.2534, 12.8075, 33.4575, 21.8617, 14.0595,  6.5943,  9.4216,\n",
      "        17.4239, 10.6927,  5.8655, 14.6309, 10.3022, 12.5082, 12.7159, 22.9074,\n",
      "        21.9826, 21.4883,  6.2361, 26.6831,  7.3494, 21.7042, 24.6286,  9.7435,\n",
      "         9.1662,  6.0247, 31.0538, 10.5781,  7.2769, 14.2604, 20.2399, 10.2114,\n",
      "        19.5099, 19.5754,  9.9862, 23.6071,  6.9625, 23.8886, 14.6702, 33.4586,\n",
      "         3.9796, 38.2587, 15.0146, 16.2427, 17.8554, 21.1583, 15.8963, 14.0614,\n",
      "        11.6298, 23.4996, 12.4039, 19.7806,  9.8139, 19.6864,  8.5985, 26.9825,\n",
      "        12.9149, 20.8859, 11.2251, 22.1586, 15.8900,  6.5473,  9.3780, 29.9206,\n",
      "         4.6732,  7.0343, 17.5006, 11.1262, 12.8555, 21.3052, 17.7263, 29.8493,\n",
      "         7.9567, 31.7296, 24.7145,  3.7882, 23.6412, 32.9478, 16.2991, 13.5322,\n",
      "        21.0535, 21.6405, 12.8291, 10.8377,  4.0134, 33.4910, 22.2175,  5.7739,\n",
      "        16.1440, 10.0628, 12.6692, 15.1360, 22.3822, 24.3680, 31.2209, 17.5699,\n",
      "        21.8305, 11.8090, 23.0051, 22.8706, 27.8556,  8.3000,  9.8171, 22.5667,\n",
      "         8.5490, 14.9777,  9.6698,  6.3205, 11.0822,  6.9689, 25.5500,  5.0797,\n",
      "        23.9156, 21.1326, 25.8046, 18.8031, 17.9798, 12.7091, 23.6918, 19.1550,\n",
      "         6.0618, 32.4684, 21.1315, 16.6098, 10.4914,  9.3602, 12.2571, 26.3091,\n",
      "        13.0109,  4.3147, 33.3387, 24.7221,  8.7778, 19.9525, 21.4427, 27.8387,\n",
      "        20.7522, 29.9870, 17.1692,  7.6077, 28.2194, 14.8097,  7.1085, 15.8517,\n",
      "        16.7389,  7.6868,  4.0333, 12.0825,  9.7932,  9.2986, 11.2259, 18.3742,\n",
      "        11.9798, 28.5392, 21.8025,  9.0628,  9.8435,  7.8849,  6.0363, 28.0692,\n",
      "        14.6660,  9.2243, 24.3576, 15.1270, 13.1384, 12.1265, 12.1306, 13.0248,\n",
      "         6.3514, 22.8631, 28.8286, 19.4677,  7.1227, 17.5961,  4.7356, 27.3273,\n",
      "        18.4562, 23.7268, 12.8655, 11.6803,  7.2499, 24.2084, 12.9507, 10.5755,\n",
      "        15.1079, 26.3790, 20.8392, 20.1878, 13.2584,  9.3656, 31.4653, 34.0464,\n",
      "        13.9154,  5.5571, 22.9494,  8.3118,  3.9183, 19.4923, 20.3999, 11.7984,\n",
      "        22.8776,  5.2297, 13.0153, 26.4761, 25.4003, 21.0986, 11.0457, 14.2362,\n",
      "        24.5601, 16.7015, 12.0694, 17.6824, 13.5956], dtype=torch.float64)\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(93, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "        # self.linear = nn.Linear(94, 1)\n",
    "        self.double()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        # logits = self.linear(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()\n",
    "print(model)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "loss_record = {\"train\": [], \"dev\": []}\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X, y\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        pred = pred.squeeze()\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss_record[\"train\"].append(loss.item())\n",
    "\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    \n",
    "def dev(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            pred = pred.squeeze()\n",
    "            loss = loss_fn(pred, y)\n",
    "\n",
    "            loss_record[\"dev\"].append(loss.item())\n",
    "\n",
    "            print(\"loss\", loss)\n",
    "\n",
    "            # test_loss += loss_fn(pred, y).item()\n",
    "            # correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    # test_loss /= num_batches\n",
    "    # correct /= size\n",
    "    # print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "def test(dataloader, model):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for X in dataloader:\n",
    "            pred = model(X)\n",
    "            pred = pred.squeeze()\n",
    "\n",
    "            preds.append(pred)\n",
    "    \n",
    "    preds = torch.cat(preds, dim=0)\n",
    "    print(preds)\n",
    "\n",
    "\n",
    "epochs = 1000\n",
    "for t in range(epochs):\n",
    "    print(f\"Train Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Test Epoch {t+1}\\n-------------------------------\")\n",
    "    dev(dev_dataloader, model, loss_fn)\n",
    "\n",
    "plot_learning_curve(loss_record, \"\")\n",
    "\n",
    "test(test_dataloader, model)\n",
    "    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 3]\n",
      " [4 5]]\n",
      "[[2 3]\n",
      " [4 5]]\n"
     ]
    }
   ],
   "source": [
    "m = np.array([[3,4],[5,6]])\n",
    "n = np.array([1,1])\n",
    "o = np.array([[1],[1]])\n",
    "\n",
    "print(m - n)\n",
    "print(m - o)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
