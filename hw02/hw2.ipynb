{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hw2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train data:  (1229932, 429)\n",
      "Size of train label:  (1229932,)\n",
      "Size of test data:  (451552, 429)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "def save_result(preds):\n",
    "    with open('test_result.csv', 'w', encoding='utf-8') as f:\n",
    "        f.write('Id,Class\\n')\n",
    "        for i, pred in enumerate(preds):\n",
    "            f.write('%d,%d\\n' % (i, pred))\n",
    "    \n",
    "    return\n",
    "\n",
    "data_root = './timit_11/'\n",
    "\n",
    "train = np.load(data_root + 'train_11.npy')\n",
    "train_label = np.load(data_root + 'train_label_11.npy')\n",
    "test = np.load(data_root + 'test_11.npy')\n",
    "\n",
    "print('Size of train data: ', train.shape)\n",
    "print('Size of train label: ', train_label.shape)\n",
    "print('Size of test data: ', test.shape)\n",
    "\n",
    "class TIMITDataset(Dataset): \n",
    "    def __init__(self, data, label, mode=\"train\"):\n",
    "        self.mode = mode\n",
    "        self.data = pd.DataFrame(data)\n",
    "        self.label = pd.DataFrame(label)\n",
    "\n",
    "        # 需要区分 train data 和 test data\n",
    "        if self.mode == \"train\":\n",
    "            indices = [i for i in range(len(self.data)) if i % 10 != 0]\n",
    "            self.data = self.data.iloc[indices, :]\n",
    "            self.label = self.label.iloc[indices]\n",
    "\n",
    "        elif self.mode == \"dev\":\n",
    "            indices = [i for i in range(len(self.data)) if i % 10 == 0]\n",
    "            self.data = self.data.iloc[indices, :]\n",
    "            self.label = self.label.iloc[indices]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == \"train\" or self.mode == \"dev\":\n",
    "            X = torch.tensor(self.data.iloc[idx], dtype=torch.float32)\n",
    "            y = torch.tensor(int(self.label.iloc[idx]), dtype=torch.int32)\n",
    "            return X, y\n",
    "        elif self.mode == \"test\":\n",
    "            return torch.tensor(self.data.iloc[idx], dtype=torch.float32)\n",
    "        \n",
    "train_data = TIMITDataset(train, train_label, mode=\"train\")\n",
    "dev_data = TIMITDataset(train, train_label, mode=\"dev\")\n",
    "test_data = TIMITDataset(test, None, mode=\"test\")\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=100, shuffle=True)\n",
    "dev_dataloader = DataLoader(dev_data, batch_size=100, shuffle=False)\n",
    "test_dataloader = DataLoader(test_data, batch_size=100, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method TIMITDataset.__len__ of <__main__.TIMITDataset object at 0x13591a870>>\n",
      "[-8.15812826e-01 -3.48689795e-01  3.95477504e-01 -1.63738783e-02\n",
      "  8.60317469e-01  1.44315893e-02  3.84072900e-01  9.11684811e-01\n",
      "  2.74616122e-01  7.21527755e-01  1.62349343e+00  1.94353950e+00\n",
      "  9.64264333e-01 -7.72595452e-03 -1.81192949e-01  5.74482083e-01\n",
      " -2.26134375e-01  4.03959244e-01  3.92566890e-01  4.68575805e-01\n",
      "  9.03372943e-01  9.12166536e-02  6.29455924e-01  6.50277019e-01\n",
      " -7.98417151e-01 -5.07435977e-01 -1.93714548e-03 -4.96793658e-01\n",
      " -1.30517155e-01  1.90668270e-01  3.52899522e-01  1.43388236e+00\n",
      "  5.82235932e-01 -4.42176044e-01  3.92167211e-01  3.82801175e-01\n",
      " -5.74597836e-01 -9.66160059e-01  8.36981013e-02 -8.17853749e-01\n",
      " -3.47191900e-01  5.85275650e-01 -1.76819056e-01  6.26596749e-01\n",
      " -2.08324268e-01  1.41138196e-01  8.14121485e-01  1.20860793e-01\n",
      "  7.84978092e-01  1.85303009e+00  2.17269802e+00  1.76232314e+00\n",
      "  4.56195819e-04 -3.57095480e-01  4.66112584e-01  4.36793976e-02\n",
      "  2.44239345e-01  6.95920229e-01  5.79180181e-01  4.36306536e-01\n",
      "  2.77873218e-01  5.59000432e-01  2.24675238e-01 -8.69635940e-01\n",
      " -2.87959218e-01  2.19163373e-02 -3.04890573e-01 -3.68410289e-01\n",
      "  6.86901152e-01  1.93446845e-01  1.22113764e+00  6.87861562e-01\n",
      " -8.80873322e-01  2.27267519e-01 -3.34653795e-01 -1.30035055e+00\n",
      " -7.05772698e-01 -4.29470271e-01 -8.19877267e-01 -5.21579981e-01\n",
      "  6.54818118e-01 -2.47517794e-01  7.76656926e-01  4.93845195e-01\n",
      "  6.38528287e-01  1.34373748e+00  1.16990530e+00  1.61719799e+00\n",
      "  1.58583629e+00  7.26012826e-01  3.88572276e-01  4.44570705e-02\n",
      " -8.08226705e-01  2.70812511e-01  5.06846428e-01 -3.04862764e-02\n",
      "  9.13899004e-01  6.55444324e-01  1.99248157e-02  1.21349134e-01\n",
      "  8.46248269e-01 -6.90799773e-01 -7.40965366e-01 -2.29644522e-01\n",
      "  1.29772574e-01 -1.11926723e+00 -3.95132959e-01  1.44303000e+00\n",
      " -1.17166817e+00 -3.43524873e-01 -3.21841627e-01 -6.46609664e-01\n",
      "  2.92410403e-01  8.45067203e-02 -1.61831212e+00  6.18455052e-01\n",
      "  6.53575718e-01 -8.21439385e-01 -5.79824984e-01  7.55298138e-01\n",
      " -1.08926654e-01  1.01349998e+00  9.15398777e-01  7.18544662e-01\n",
      "  9.52252090e-01  6.39603972e-01  1.29298186e+00  1.33980155e+00\n",
      "  7.12813556e-01  8.59630764e-01  1.09658375e-01 -1.40015316e+00\n",
      "  1.04810074e-01  8.90806198e-01 -5.54478347e-01  5.59858501e-01\n",
      "  4.76976901e-01 -4.27572317e-02  4.54809010e-01  8.39707017e-01\n",
      " -1.09382379e+00 -3.02106410e-01 -2.24989846e-01  2.37577826e-01\n",
      " -1.64795864e+00 -3.36687922e-01  1.51167357e+00 -1.72091830e+00\n",
      " -1.14630747e+00 -4.85988587e-01 -5.19942343e-01 -2.66638696e-01\n",
      " -1.06537372e-01 -7.84751356e-01  1.46447480e+00  3.82050872e-01\n",
      " -8.22048783e-01 -6.27880633e-01  7.42801547e-01 -3.90241779e-02\n",
      "  1.18365037e+00  1.21759546e+00  1.00882006e+00  1.14920890e+00\n",
      "  7.69170105e-01  1.35666311e+00  1.38351047e+00  7.51709640e-01\n",
      "  8.97786200e-01  1.88909724e-01 -1.84347391e+00 -5.92376217e-02\n",
      "  1.00558782e+00 -1.07068503e+00  2.94810176e-01  4.48351741e-01\n",
      " -2.13550150e-01  1.71433255e-01  7.12499261e-01 -7.72415757e-01\n",
      " -1.92916185e-01 -5.73165357e-01  3.06467086e-01 -1.47141361e+00\n",
      " -5.06693542e-01  4.55461711e-01 -1.67400038e+00 -1.24507964e+00\n",
      " -3.37909043e-01 -9.42163840e-02 -7.19930530e-01 -3.06202561e-01\n",
      "  7.89523959e-01  1.54539812e+00 -4.80546117e-01 -8.01864445e-01\n",
      " -7.07150519e-01  6.28714204e-01  3.89305472e-01  7.86976397e-01\n",
      "  1.08158088e+00  1.06749725e+00  5.51742494e-01  8.54246080e-01\n",
      "  9.38302457e-01  7.02012718e-01  8.34094465e-01  7.78473735e-01\n",
      "  2.46743500e-01 -2.00786448e+00 -2.59372681e-01  1.01744902e+00\n",
      " -1.21515656e+00  1.41148657e-01  2.77194500e-01 -6.88229918e-01\n",
      " -7.03711510e-01  2.29979977e-01 -6.55366600e-01  5.08279741e-01\n",
      " -7.98558593e-01  2.42155805e-01 -6.39601648e-01 -3.57569396e-01\n",
      " -5.19201696e-01 -1.06722796e+00 -8.76354277e-01 -6.17329240e-01\n",
      " -6.06594145e-01 -1.51309419e+00 -7.37058938e-01  1.06613851e+00\n",
      "  1.54067111e+00 -2.66122669e-01 -7.07047403e-01 -1.49290991e+00\n",
      "  5.77214837e-01  1.01184523e+00  2.28464417e-02  4.62869883e-01\n",
      "  5.73139429e-01  6.86030209e-01  6.59196913e-01  2.08747530e+00\n",
      " -8.50434899e-02  1.52945018e+00  1.38857460e+00  3.07250828e-01\n",
      " -1.93132412e+00 -2.75336593e-01  7.48762012e-01 -1.39199424e+00\n",
      " -2.62844265e-01 -3.80406469e-01 -9.25785244e-01 -1.07260036e+00\n",
      "  1.93184674e-01 -2.34122247e-01  8.09500456e-01 -3.79938722e-01\n",
      "  1.68058470e-01  3.26376736e-01 -1.23876520e-02 -1.41517222e+00\n",
      "  4.59585823e-02 -7.19508082e-02 -8.91312122e-01 -1.21071845e-01\n",
      " -4.21448231e-01  1.49552613e-01  1.02639949e+00 -6.27906859e-01\n",
      " -7.97479987e-01 -6.25015140e-01 -1.97806239e+00  5.62663913e-01\n",
      "  1.08202255e+00 -3.15571904e-01  1.60826400e-01  7.12553084e-01\n",
      "  8.63311768e-01  8.94219697e-01  1.68949175e+00  5.86832941e-01\n",
      "  1.82698238e+00  7.64400065e-01  3.78606379e-01 -1.93906581e+00\n",
      " -2.54375786e-01  2.99605757e-01 -1.52195930e+00 -3.10934246e-01\n",
      " -4.56030488e-01 -3.86570841e-01 -1.04673004e+00 -8.25209916e-03\n",
      "  4.21362609e-01  7.00726211e-01 -6.20774686e-01  1.31356940e-01\n",
      "  9.06182945e-01  2.47731835e-01 -1.79323173e+00  8.11256707e-01\n",
      "  6.08145833e-01 -3.43659341e-01  5.73926151e-01 -4.04250085e-01\n",
      " -6.75222874e-01  1.18168211e+00 -8.53463888e-01 -8.56958423e-03\n",
      " -5.41229427e-01 -2.12455940e+00  3.38244736e-01  6.46516800e-01\n",
      " -5.64446926e-01  4.48248535e-01  1.05095375e+00  8.40595901e-01\n",
      "  3.11493397e-01  1.61268258e+00  1.73551130e+00  1.29770410e+00\n",
      " -6.98527768e-02  4.62385297e-01 -1.74325550e+00 -9.84729901e-02\n",
      " -1.19800247e-01 -1.28110456e+00 -1.72435850e-01 -5.89624405e-01\n",
      " -1.42206907e-01 -1.27849662e+00 -3.17881048e-01  5.10424614e-01\n",
      "  3.22074503e-01 -4.75895911e-01  1.56983048e-01  1.39186656e+00\n",
      "  4.53995079e-01 -1.52112699e+00  1.43880963e+00  8.51973772e-01\n",
      "  2.16113012e-02  8.90362442e-01 -5.90594001e-02 -8.75823736e-01\n",
      "  2.90102303e-01 -1.13367736e+00  6.45637989e-01 -5.26147008e-01\n",
      " -2.06020331e+00  2.99371749e-01  6.83552027e-01 -3.23276311e-01\n",
      "  7.36325026e-01  6.55510664e-01 -1.81245700e-01 -9.77546930e-01\n",
      "  9.46608126e-01  9.21990693e-01  2.14190197e+00  2.17398033e-01\n",
      "  5.16363978e-01 -1.36978412e+00  1.74136609e-01 -4.52454537e-01\n",
      " -9.24559832e-01 -8.75526816e-02 -8.43120098e-01  3.46856117e-01\n",
      " -9.60662067e-01 -4.31923479e-01  8.75602186e-01 -3.55560482e-02\n",
      " -7.17057049e-01  1.60263851e-01  1.31419182e+00  6.40638173e-01\n",
      " -3.64474535e-01  8.50858510e-01  1.52416348e-01 -3.61995876e-01\n",
      "  7.94132769e-01  8.49550247e-01 -5.72304070e-01 -4.94386554e-01\n",
      " -8.14543962e-01  2.29587063e-01 -4.44377780e-01 -2.02960443e+00\n",
      "  5.13314545e-01  4.00894850e-01 -4.04429317e-01  5.56219280e-01\n",
      " -2.18392864e-01  2.01895729e-01  9.18949246e-02  2.01063347e+00\n",
      "  1.16167784e+00  9.79497254e-01  4.92651969e-01  4.97839808e-01\n",
      " -7.56824553e-01  3.33294928e-01 -6.37785673e-01 -3.69842678e-01\n",
      "  1.70299485e-01 -6.15268171e-01  8.54063451e-01 -4.88241553e-01\n",
      " -1.02353287e+00  2.20091596e-01 -8.97782147e-01 -9.17488098e-01\n",
      "  5.88676706e-02  5.84376633e-01  5.10970116e-01  6.05664790e-01\n",
      "  5.34149706e-01 -3.20446759e-01 -1.88363567e-02  1.45047593e+00\n",
      "  1.28690445e+00 -4.73928377e-02 -1.81417489e+00 -8.92933309e-01\n",
      "  2.31251955e-01]\n"
     ]
    }
   ],
   "source": [
    "print(train_data.__len__)\n",
    "\n",
    "# 判断变量类型\n",
    "# print(train[0])\n",
    "# print(type(train[0]))\n",
    "\n",
    "# 判断变量类型\n",
    "print(test[0])\n",
    "# print(type(test[0]))\n",
    "\n",
    "\n",
    "# print(train_label[0])\n",
    "# print(type(train_label[0]))\n",
    "# print(type(train_data))\n",
    "# print(type(train_data[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=429, out_features=1024, bias=True)\n",
      "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): Dropout(p=0.2, inplace=False)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (9): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (12): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=128, out_features=39, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(11*39, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 39)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.2, momentum=0.05)\n",
    "\n",
    "loss_record = {\"train\": [], \"dev\": []}\n",
    "pred_record = []\n",
    "target_record = []\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss_record[\"train\"].append(loss.item())\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "def dev(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "    return test_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.633367  [  100/1106938]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2c/pn5qg61n16nfl0b9y726w55r0000gn/T/ipykernel_22464/1622385451.py:48: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
      "  y = torch.tensor(int(self.label.iloc[idx]), dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.987682  [10100/1106938]\n",
      "loss: 1.840602  [20100/1106938]\n",
      "loss: 1.697019  [30100/1106938]\n",
      "loss: 1.691713  [40100/1106938]\n",
      "loss: 1.331525  [50100/1106938]\n",
      "loss: 1.502745  [60100/1106938]\n",
      "loss: 1.513005  [70100/1106938]\n",
      "loss: 1.479351  [80100/1106938]\n",
      "loss: 1.350086  [90100/1106938]\n",
      "loss: 1.545361  [100100/1106938]\n",
      "loss: 1.276307  [110100/1106938]\n",
      "loss: 1.415569  [120100/1106938]\n",
      "loss: 1.680169  [130100/1106938]\n",
      "loss: 1.280818  [140100/1106938]\n",
      "loss: 1.490186  [150100/1106938]\n",
      "loss: 1.308905  [160100/1106938]\n",
      "loss: 1.480158  [170100/1106938]\n",
      "loss: 1.148580  [180100/1106938]\n",
      "loss: 1.110358  [190100/1106938]\n",
      "loss: 1.194574  [200100/1106938]\n",
      "loss: 1.144990  [210100/1106938]\n",
      "loss: 1.401611  [220100/1106938]\n",
      "loss: 1.304969  [230100/1106938]\n",
      "loss: 1.304972  [240100/1106938]\n",
      "loss: 1.355504  [250100/1106938]\n",
      "loss: 1.147725  [260100/1106938]\n",
      "loss: 1.047630  [270100/1106938]\n",
      "loss: 1.185849  [280100/1106938]\n",
      "loss: 1.178416  [290100/1106938]\n",
      "loss: 1.356295  [300100/1106938]\n",
      "loss: 1.176982  [310100/1106938]\n",
      "loss: 0.925068  [320100/1106938]\n",
      "loss: 1.362819  [330100/1106938]\n",
      "loss: 1.224143  [340100/1106938]\n",
      "loss: 1.382155  [350100/1106938]\n",
      "loss: 1.033964  [360100/1106938]\n",
      "loss: 1.330423  [370100/1106938]\n",
      "loss: 1.304598  [380100/1106938]\n",
      "loss: 1.189220  [390100/1106938]\n",
      "loss: 1.134766  [400100/1106938]\n",
      "loss: 0.909853  [410100/1106938]\n",
      "loss: 1.260728  [420100/1106938]\n",
      "loss: 1.223830  [430100/1106938]\n",
      "loss: 1.157739  [440100/1106938]\n",
      "loss: 1.090817  [450100/1106938]\n",
      "loss: 1.149640  [460100/1106938]\n",
      "loss: 1.381820  [470100/1106938]\n",
      "loss: 1.100981  [480100/1106938]\n",
      "loss: 1.144766  [490100/1106938]\n",
      "loss: 1.123785  [500100/1106938]\n",
      "loss: 1.034420  [510100/1106938]\n",
      "loss: 1.032950  [520100/1106938]\n",
      "loss: 1.001973  [530100/1106938]\n",
      "loss: 1.042711  [540100/1106938]\n",
      "loss: 1.176330  [550100/1106938]\n",
      "loss: 1.364362  [560100/1106938]\n",
      "loss: 1.385061  [570100/1106938]\n",
      "loss: 1.307571  [580100/1106938]\n",
      "loss: 1.210347  [590100/1106938]\n",
      "loss: 1.182997  [600100/1106938]\n",
      "loss: 1.147610  [610100/1106938]\n",
      "loss: 1.068946  [620100/1106938]\n",
      "loss: 1.345246  [630100/1106938]\n",
      "loss: 1.071972  [640100/1106938]\n",
      "loss: 1.117880  [650100/1106938]\n",
      "loss: 1.094549  [660100/1106938]\n",
      "loss: 1.242572  [670100/1106938]\n",
      "loss: 1.132010  [680100/1106938]\n",
      "loss: 0.919107  [690100/1106938]\n",
      "loss: 1.119876  [700100/1106938]\n",
      "loss: 1.124035  [710100/1106938]\n",
      "loss: 1.075814  [720100/1106938]\n",
      "loss: 1.034142  [730100/1106938]\n",
      "loss: 0.988636  [740100/1106938]\n",
      "loss: 1.272131  [750100/1106938]\n",
      "loss: 1.220819  [760100/1106938]\n",
      "loss: 1.230430  [770100/1106938]\n",
      "loss: 0.987328  [780100/1106938]\n",
      "loss: 1.138414  [790100/1106938]\n",
      "loss: 1.173951  [800100/1106938]\n",
      "loss: 1.077860  [810100/1106938]\n",
      "loss: 1.195881  [820100/1106938]\n",
      "loss: 1.114281  [830100/1106938]\n",
      "loss: 1.121934  [840100/1106938]\n",
      "loss: 1.087170  [850100/1106938]\n",
      "loss: 0.968889  [860100/1106938]\n",
      "loss: 1.155271  [870100/1106938]\n",
      "loss: 1.140883  [880100/1106938]\n",
      "loss: 0.947897  [890100/1106938]\n",
      "loss: 1.061538  [900100/1106938]\n",
      "loss: 1.238340  [910100/1106938]\n",
      "loss: 1.103503  [920100/1106938]\n",
      "loss: 1.157800  [930100/1106938]\n",
      "loss: 1.016998  [940100/1106938]\n",
      "loss: 1.036999  [950100/1106938]\n",
      "loss: 1.164530  [960100/1106938]\n",
      "loss: 0.908564  [970100/1106938]\n",
      "loss: 1.074653  [980100/1106938]\n",
      "loss: 0.997947  [990100/1106938]\n",
      "loss: 0.955804  [1000100/1106938]\n",
      "loss: 1.178437  [1010100/1106938]\n",
      "loss: 1.324301  [1020100/1106938]\n",
      "loss: 1.166550  [1030100/1106938]\n",
      "loss: 1.224523  [1040100/1106938]\n",
      "loss: 1.043458  [1050100/1106938]\n",
      "loss: 0.905181  [1060100/1106938]\n",
      "loss: 0.946884  [1070100/1106938]\n",
      "loss: 1.165484  [1080100/1106938]\n",
      "loss: 1.109171  [1090100/1106938]\n",
      "loss: 0.956515  [1100100/1106938]\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 0.943312 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.112940  [  100/1106938]\n",
      "loss: 1.129266  [10100/1106938]\n",
      "loss: 1.107019  [20100/1106938]\n",
      "loss: 0.924158  [30100/1106938]\n",
      "loss: 1.055740  [40100/1106938]\n",
      "loss: 0.913327  [50100/1106938]\n",
      "loss: 0.986628  [60100/1106938]\n",
      "loss: 0.989885  [70100/1106938]\n",
      "loss: 0.964812  [80100/1106938]\n",
      "loss: 1.289499  [90100/1106938]\n",
      "loss: 0.909298  [100100/1106938]\n",
      "loss: 1.150581  [110100/1106938]\n",
      "loss: 0.898964  [120100/1106938]\n",
      "loss: 0.749666  [130100/1106938]\n",
      "loss: 1.170396  [140100/1106938]\n",
      "loss: 0.994180  [150100/1106938]\n",
      "loss: 0.858621  [160100/1106938]\n",
      "loss: 1.033293  [170100/1106938]\n",
      "loss: 1.137823  [180100/1106938]\n",
      "loss: 1.104707  [190100/1106938]\n",
      "loss: 1.117302  [200100/1106938]\n",
      "loss: 0.957850  [210100/1106938]\n",
      "loss: 1.201065  [220100/1106938]\n",
      "loss: 1.140135  [230100/1106938]\n",
      "loss: 0.834753  [240100/1106938]\n",
      "loss: 0.753837  [250100/1106938]\n",
      "loss: 0.865871  [260100/1106938]\n",
      "loss: 0.984922  [270100/1106938]\n",
      "loss: 0.908011  [280100/1106938]\n",
      "loss: 1.116905  [290100/1106938]\n",
      "loss: 1.302044  [300100/1106938]\n",
      "loss: 0.855507  [310100/1106938]\n",
      "loss: 0.889693  [320100/1106938]\n",
      "loss: 0.973220  [330100/1106938]\n",
      "loss: 0.866645  [340100/1106938]\n",
      "loss: 0.950916  [350100/1106938]\n",
      "loss: 0.808713  [360100/1106938]\n",
      "loss: 1.073298  [370100/1106938]\n",
      "loss: 0.978207  [380100/1106938]\n",
      "loss: 0.919255  [390100/1106938]\n",
      "loss: 0.793729  [400100/1106938]\n",
      "loss: 1.191940  [410100/1106938]\n",
      "loss: 0.733201  [420100/1106938]\n",
      "loss: 0.872696  [430100/1106938]\n",
      "loss: 0.862329  [440100/1106938]\n",
      "loss: 1.226486  [450100/1106938]\n",
      "loss: 0.927907  [460100/1106938]\n",
      "loss: 1.150941  [470100/1106938]\n",
      "loss: 0.908284  [480100/1106938]\n",
      "loss: 1.014550  [490100/1106938]\n",
      "loss: 0.819654  [500100/1106938]\n",
      "loss: 0.934028  [510100/1106938]\n",
      "loss: 1.030871  [520100/1106938]\n",
      "loss: 1.029417  [530100/1106938]\n",
      "loss: 1.111666  [540100/1106938]\n",
      "loss: 1.180769  [550100/1106938]\n",
      "loss: 0.854659  [560100/1106938]\n",
      "loss: 1.179557  [570100/1106938]\n",
      "loss: 1.123356  [580100/1106938]\n",
      "loss: 1.340960  [590100/1106938]\n",
      "loss: 0.969645  [600100/1106938]\n",
      "loss: 1.384888  [610100/1106938]\n",
      "loss: 0.911602  [620100/1106938]\n",
      "loss: 0.879166  [630100/1106938]\n",
      "loss: 0.804445  [640100/1106938]\n",
      "loss: 0.970184  [650100/1106938]\n",
      "loss: 1.044665  [660100/1106938]\n",
      "loss: 0.923960  [670100/1106938]\n",
      "loss: 0.970220  [680100/1106938]\n",
      "loss: 1.270060  [690100/1106938]\n",
      "loss: 1.059711  [700100/1106938]\n",
      "loss: 1.093323  [710100/1106938]\n",
      "loss: 0.952555  [720100/1106938]\n",
      "loss: 1.084288  [730100/1106938]\n",
      "loss: 1.015376  [740100/1106938]\n",
      "loss: 0.921109  [750100/1106938]\n",
      "loss: 1.002427  [760100/1106938]\n",
      "loss: 0.923043  [770100/1106938]\n",
      "loss: 1.036968  [780100/1106938]\n",
      "loss: 1.039928  [790100/1106938]\n",
      "loss: 1.056793  [800100/1106938]\n",
      "loss: 0.801554  [810100/1106938]\n",
      "loss: 1.364217  [820100/1106938]\n",
      "loss: 0.886744  [830100/1106938]\n",
      "loss: 1.087075  [840100/1106938]\n",
      "loss: 1.117807  [850100/1106938]\n",
      "loss: 0.971191  [860100/1106938]\n",
      "loss: 0.848226  [870100/1106938]\n",
      "loss: 0.857610  [880100/1106938]\n",
      "loss: 0.993531  [890100/1106938]\n",
      "loss: 0.898754  [900100/1106938]\n",
      "loss: 1.239159  [910100/1106938]\n",
      "loss: 1.009823  [920100/1106938]\n",
      "loss: 1.061217  [930100/1106938]\n",
      "loss: 1.011013  [940100/1106938]\n",
      "loss: 1.029918  [950100/1106938]\n",
      "loss: 0.846076  [960100/1106938]\n",
      "loss: 0.925845  [970100/1106938]\n",
      "loss: 0.961325  [980100/1106938]\n",
      "loss: 0.826070  [990100/1106938]\n",
      "loss: 0.831519  [1000100/1106938]\n",
      "loss: 0.922364  [1010100/1106938]\n",
      "loss: 0.810424  [1020100/1106938]\n",
      "loss: 0.813784  [1030100/1106938]\n",
      "loss: 0.883365  [1040100/1106938]\n",
      "loss: 0.934713  [1050100/1106938]\n",
      "loss: 0.759489  [1060100/1106938]\n",
      "loss: 1.034887  [1070100/1106938]\n",
      "loss: 0.902285  [1080100/1106938]\n",
      "loss: 0.849524  [1090100/1106938]\n",
      "loss: 1.166863  [1100100/1106938]\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 0.863127 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.825302  [  100/1106938]\n",
      "loss: 0.901715  [10100/1106938]\n",
      "loss: 1.003588  [20100/1106938]\n",
      "loss: 0.863622  [30100/1106938]\n",
      "loss: 0.957753  [40100/1106938]\n",
      "loss: 0.948545  [50100/1106938]\n",
      "loss: 0.991628  [60100/1106938]\n",
      "loss: 1.121500  [70100/1106938]\n",
      "loss: 0.967265  [80100/1106938]\n",
      "loss: 0.981339  [90100/1106938]\n",
      "loss: 1.119952  [100100/1106938]\n",
      "loss: 0.936120  [110100/1106938]\n",
      "loss: 0.913320  [120100/1106938]\n",
      "loss: 0.938413  [130100/1106938]\n",
      "loss: 1.173573  [140100/1106938]\n",
      "loss: 1.048099  [150100/1106938]\n",
      "loss: 0.831972  [160100/1106938]\n",
      "loss: 0.831331  [170100/1106938]\n",
      "loss: 0.746024  [180100/1106938]\n",
      "loss: 1.051353  [190100/1106938]\n",
      "loss: 0.850353  [200100/1106938]\n",
      "loss: 1.113575  [210100/1106938]\n",
      "loss: 1.118907  [220100/1106938]\n",
      "loss: 0.948167  [230100/1106938]\n",
      "loss: 1.070018  [240100/1106938]\n",
      "loss: 0.965243  [250100/1106938]\n",
      "loss: 0.943609  [260100/1106938]\n",
      "loss: 0.765480  [270100/1106938]\n",
      "loss: 1.034498  [280100/1106938]\n",
      "loss: 0.902014  [290100/1106938]\n",
      "loss: 1.140056  [300100/1106938]\n",
      "loss: 0.920861  [310100/1106938]\n",
      "loss: 0.771899  [320100/1106938]\n",
      "loss: 0.949029  [330100/1106938]\n",
      "loss: 0.997837  [340100/1106938]\n",
      "loss: 0.814110  [350100/1106938]\n",
      "loss: 1.004542  [360100/1106938]\n",
      "loss: 1.011615  [370100/1106938]\n",
      "loss: 0.751117  [380100/1106938]\n",
      "loss: 1.029667  [390100/1106938]\n",
      "loss: 0.902691  [400100/1106938]\n",
      "loss: 0.880198  [410100/1106938]\n",
      "loss: 0.733458  [420100/1106938]\n",
      "loss: 1.052939  [430100/1106938]\n",
      "loss: 0.796306  [440100/1106938]\n",
      "loss: 0.814083  [450100/1106938]\n",
      "loss: 1.137867  [460100/1106938]\n",
      "loss: 0.990549  [470100/1106938]\n",
      "loss: 0.952517  [480100/1106938]\n",
      "loss: 0.882087  [490100/1106938]\n",
      "loss: 0.958708  [500100/1106938]\n",
      "loss: 1.025796  [510100/1106938]\n",
      "loss: 0.984075  [520100/1106938]\n",
      "loss: 0.815220  [530100/1106938]\n",
      "loss: 1.103677  [540100/1106938]\n",
      "loss: 0.835616  [550100/1106938]\n",
      "loss: 0.809718  [560100/1106938]\n",
      "loss: 0.704420  [570100/1106938]\n",
      "loss: 0.728552  [580100/1106938]\n",
      "loss: 0.895884  [590100/1106938]\n",
      "loss: 0.951371  [600100/1106938]\n",
      "loss: 0.828787  [610100/1106938]\n",
      "loss: 0.927411  [620100/1106938]\n",
      "loss: 1.023870  [630100/1106938]\n",
      "loss: 0.903869  [640100/1106938]\n",
      "loss: 0.991965  [650100/1106938]\n",
      "loss: 1.001833  [660100/1106938]\n",
      "loss: 0.840058  [670100/1106938]\n",
      "loss: 1.074430  [680100/1106938]\n",
      "loss: 0.797919  [690100/1106938]\n",
      "loss: 0.979615  [700100/1106938]\n",
      "loss: 0.874282  [710100/1106938]\n",
      "loss: 0.907358  [720100/1106938]\n",
      "loss: 1.037618  [730100/1106938]\n",
      "loss: 0.945880  [740100/1106938]\n",
      "loss: 0.980480  [750100/1106938]\n",
      "loss: 1.144019  [760100/1106938]\n",
      "loss: 1.079963  [770100/1106938]\n",
      "loss: 0.958524  [780100/1106938]\n",
      "loss: 1.078645  [790100/1106938]\n",
      "loss: 0.959485  [800100/1106938]\n",
      "loss: 1.131842  [810100/1106938]\n",
      "loss: 0.967558  [820100/1106938]\n",
      "loss: 0.899733  [830100/1106938]\n",
      "loss: 0.672793  [840100/1106938]\n",
      "loss: 0.986056  [850100/1106938]\n",
      "loss: 0.940521  [860100/1106938]\n",
      "loss: 0.972991  [870100/1106938]\n",
      "loss: 0.874012  [880100/1106938]\n",
      "loss: 0.928959  [890100/1106938]\n",
      "loss: 0.878438  [900100/1106938]\n",
      "loss: 0.973699  [910100/1106938]\n",
      "loss: 0.867709  [920100/1106938]\n",
      "loss: 0.862489  [930100/1106938]\n",
      "loss: 1.073350  [940100/1106938]\n",
      "loss: 0.854303  [950100/1106938]\n",
      "loss: 0.970362  [960100/1106938]\n",
      "loss: 0.678119  [970100/1106938]\n",
      "loss: 1.046503  [980100/1106938]\n",
      "loss: 0.952046  [990100/1106938]\n",
      "loss: 0.772659  [1000100/1106938]\n",
      "loss: 0.682774  [1010100/1106938]\n",
      "loss: 0.961851  [1020100/1106938]\n",
      "loss: 0.578216  [1030100/1106938]\n",
      "loss: 0.797796  [1040100/1106938]\n",
      "loss: 1.005867  [1050100/1106938]\n",
      "loss: 0.964776  [1060100/1106938]\n",
      "loss: 0.782015  [1070100/1106938]\n",
      "loss: 0.843509  [1080100/1106938]\n",
      "loss: 1.082724  [1090100/1106938]\n",
      "loss: 0.938058  [1100100/1106938]\n",
      "Test Error: \n",
      " Accuracy: 72.8%, Avg loss: 0.820949 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.908781  [  100/1106938]\n",
      "loss: 0.768683  [10100/1106938]\n",
      "loss: 0.929886  [20100/1106938]\n",
      "loss: 1.031988  [30100/1106938]\n",
      "loss: 0.952296  [40100/1106938]\n",
      "loss: 0.933616  [50100/1106938]\n",
      "loss: 0.903389  [60100/1106938]\n",
      "loss: 0.974417  [70100/1106938]\n",
      "loss: 1.061790  [80100/1106938]\n",
      "loss: 0.789864  [90100/1106938]\n",
      "loss: 0.892702  [100100/1106938]\n",
      "loss: 0.956630  [110100/1106938]\n",
      "loss: 0.945239  [120100/1106938]\n",
      "loss: 1.137518  [130100/1106938]\n",
      "loss: 0.844654  [140100/1106938]\n",
      "loss: 0.890731  [150100/1106938]\n",
      "loss: 0.947139  [160100/1106938]\n",
      "loss: 0.823996  [170100/1106938]\n",
      "loss: 0.921697  [180100/1106938]\n",
      "loss: 0.939439  [190100/1106938]\n",
      "loss: 0.831008  [200100/1106938]\n",
      "loss: 0.775283  [210100/1106938]\n",
      "loss: 0.877548  [220100/1106938]\n",
      "loss: 0.796499  [230100/1106938]\n",
      "loss: 0.979324  [240100/1106938]\n",
      "loss: 0.772225  [250100/1106938]\n",
      "loss: 0.732781  [260100/1106938]\n",
      "loss: 1.076503  [270100/1106938]\n",
      "loss: 0.818398  [280100/1106938]\n",
      "loss: 1.022301  [290100/1106938]\n",
      "loss: 0.884681  [300100/1106938]\n",
      "loss: 0.883622  [310100/1106938]\n",
      "loss: 0.976919  [320100/1106938]\n",
      "loss: 0.700072  [330100/1106938]\n",
      "loss: 0.867975  [340100/1106938]\n",
      "loss: 0.789741  [350100/1106938]\n",
      "loss: 1.071070  [360100/1106938]\n",
      "loss: 0.887090  [370100/1106938]\n",
      "loss: 0.609625  [380100/1106938]\n",
      "loss: 1.054064  [390100/1106938]\n",
      "loss: 1.043025  [400100/1106938]\n",
      "loss: 0.785788  [410100/1106938]\n",
      "loss: 0.743982  [420100/1106938]\n",
      "loss: 1.086641  [430100/1106938]\n",
      "loss: 0.732716  [440100/1106938]\n",
      "loss: 1.105307  [450100/1106938]\n",
      "loss: 0.831999  [460100/1106938]\n",
      "loss: 0.917982  [470100/1106938]\n",
      "loss: 0.759933  [480100/1106938]\n",
      "loss: 0.952061  [490100/1106938]\n",
      "loss: 0.907830  [500100/1106938]\n",
      "loss: 0.958159  [510100/1106938]\n",
      "loss: 0.936963  [520100/1106938]\n",
      "loss: 0.883591  [530100/1106938]\n",
      "loss: 0.730485  [540100/1106938]\n",
      "loss: 1.268621  [550100/1106938]\n",
      "loss: 0.576197  [560100/1106938]\n",
      "loss: 0.853544  [570100/1106938]\n",
      "loss: 0.814896  [580100/1106938]\n",
      "loss: 0.879675  [590100/1106938]\n",
      "loss: 0.912609  [600100/1106938]\n",
      "loss: 0.836205  [610100/1106938]\n",
      "loss: 0.925075  [620100/1106938]\n",
      "loss: 0.882401  [630100/1106938]\n",
      "loss: 0.745408  [640100/1106938]\n",
      "loss: 0.821379  [650100/1106938]\n",
      "loss: 0.888355  [660100/1106938]\n",
      "loss: 0.887291  [670100/1106938]\n",
      "loss: 1.108440  [680100/1106938]\n",
      "loss: 0.927358  [690100/1106938]\n",
      "loss: 0.800531  [700100/1106938]\n",
      "loss: 1.039619  [710100/1106938]\n",
      "loss: 0.756062  [720100/1106938]\n",
      "loss: 0.961799  [730100/1106938]\n",
      "loss: 0.866584  [740100/1106938]\n",
      "loss: 0.768327  [750100/1106938]\n",
      "loss: 0.865013  [760100/1106938]\n",
      "loss: 1.030515  [770100/1106938]\n",
      "loss: 0.740394  [780100/1106938]\n",
      "loss: 0.950090  [790100/1106938]\n",
      "loss: 0.842754  [800100/1106938]\n",
      "loss: 0.735916  [810100/1106938]\n",
      "loss: 1.062402  [820100/1106938]\n",
      "loss: 1.098672  [830100/1106938]\n",
      "loss: 0.851596  [840100/1106938]\n",
      "loss: 0.940627  [850100/1106938]\n",
      "loss: 0.856285  [860100/1106938]\n",
      "loss: 0.923122  [870100/1106938]\n",
      "loss: 0.905884  [880100/1106938]\n",
      "loss: 0.866667  [890100/1106938]\n",
      "loss: 1.012291  [900100/1106938]\n",
      "loss: 0.943908  [910100/1106938]\n",
      "loss: 0.930614  [920100/1106938]\n",
      "loss: 1.081105  [930100/1106938]\n",
      "loss: 0.772666  [940100/1106938]\n",
      "loss: 0.851926  [950100/1106938]\n",
      "loss: 0.878732  [960100/1106938]\n",
      "loss: 1.045608  [970100/1106938]\n",
      "loss: 0.662007  [980100/1106938]\n",
      "loss: 1.244602  [990100/1106938]\n",
      "loss: 0.935399  [1000100/1106938]\n",
      "loss: 0.883842  [1010100/1106938]\n",
      "loss: 0.970057  [1020100/1106938]\n",
      "loss: 0.632751  [1030100/1106938]\n",
      "loss: 0.994919  [1040100/1106938]\n",
      "loss: 0.787738  [1050100/1106938]\n",
      "loss: 0.981258  [1060100/1106938]\n",
      "loss: 0.865929  [1070100/1106938]\n",
      "loss: 0.799387  [1080100/1106938]\n",
      "loss: 0.713137  [1090100/1106938]\n",
      "loss: 0.827064  [1100100/1106938]\n",
      "Test Error: \n",
      " Accuracy: 74.3%, Avg loss: 0.774016 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.889157  [  100/1106938]\n",
      "loss: 1.057767  [10100/1106938]\n",
      "loss: 0.726409  [20100/1106938]\n",
      "loss: 0.601102  [30100/1106938]\n",
      "loss: 0.654162  [40100/1106938]\n",
      "loss: 0.909543  [50100/1106938]\n",
      "loss: 0.789174  [60100/1106938]\n",
      "loss: 0.869832  [70100/1106938]\n",
      "loss: 1.001247  [80100/1106938]\n",
      "loss: 0.904302  [90100/1106938]\n",
      "loss: 0.817562  [100100/1106938]\n",
      "loss: 0.830617  [110100/1106938]\n",
      "loss: 0.809450  [120100/1106938]\n",
      "loss: 0.626981  [130100/1106938]\n",
      "loss: 0.960139  [140100/1106938]\n",
      "loss: 0.750566  [150100/1106938]\n",
      "loss: 0.745154  [160100/1106938]\n",
      "loss: 0.868504  [170100/1106938]\n",
      "loss: 0.854613  [180100/1106938]\n",
      "loss: 0.752081  [190100/1106938]\n",
      "loss: 1.042947  [200100/1106938]\n",
      "loss: 0.818959  [210100/1106938]\n",
      "loss: 0.792107  [220100/1106938]\n",
      "loss: 0.809810  [230100/1106938]\n",
      "loss: 0.990691  [240100/1106938]\n",
      "loss: 0.889607  [250100/1106938]\n",
      "loss: 0.857906  [260100/1106938]\n",
      "loss: 0.561818  [270100/1106938]\n",
      "loss: 0.911277  [280100/1106938]\n",
      "loss: 0.715482  [290100/1106938]\n",
      "loss: 0.769451  [300100/1106938]\n",
      "loss: 0.974825  [310100/1106938]\n",
      "loss: 0.914495  [320100/1106938]\n",
      "loss: 0.771967  [330100/1106938]\n",
      "loss: 0.814504  [340100/1106938]\n",
      "loss: 0.694467  [350100/1106938]\n",
      "loss: 0.922865  [360100/1106938]\n",
      "loss: 0.849232  [370100/1106938]\n",
      "loss: 0.730261  [380100/1106938]\n",
      "loss: 0.994411  [390100/1106938]\n",
      "loss: 0.903064  [400100/1106938]\n",
      "loss: 1.028948  [410100/1106938]\n",
      "loss: 0.816532  [420100/1106938]\n",
      "loss: 1.042055  [430100/1106938]\n",
      "loss: 0.876502  [440100/1106938]\n",
      "loss: 0.757262  [450100/1106938]\n",
      "loss: 0.782260  [460100/1106938]\n",
      "loss: 0.861678  [470100/1106938]\n",
      "loss: 0.912586  [480100/1106938]\n",
      "loss: 0.912582  [490100/1106938]\n",
      "loss: 0.858977  [500100/1106938]\n",
      "loss: 0.938604  [510100/1106938]\n",
      "loss: 0.863802  [520100/1106938]\n",
      "loss: 0.770627  [530100/1106938]\n",
      "loss: 0.778012  [540100/1106938]\n",
      "loss: 0.965586  [550100/1106938]\n",
      "loss: 0.855106  [560100/1106938]\n",
      "loss: 0.815639  [570100/1106938]\n",
      "loss: 0.871792  [580100/1106938]\n",
      "loss: 1.037651  [590100/1106938]\n",
      "loss: 0.898273  [600100/1106938]\n",
      "loss: 0.787457  [610100/1106938]\n",
      "loss: 0.945943  [620100/1106938]\n",
      "loss: 0.592283  [630100/1106938]\n",
      "loss: 0.839319  [640100/1106938]\n",
      "loss: 1.090224  [650100/1106938]\n",
      "loss: 0.756475  [660100/1106938]\n",
      "loss: 0.706778  [670100/1106938]\n",
      "loss: 0.781792  [680100/1106938]\n",
      "loss: 0.828779  [690100/1106938]\n",
      "loss: 0.760024  [700100/1106938]\n",
      "loss: 0.868924  [710100/1106938]\n",
      "loss: 0.886509  [720100/1106938]\n",
      "loss: 0.699339  [730100/1106938]\n",
      "loss: 0.856821  [740100/1106938]\n",
      "loss: 0.894710  [750100/1106938]\n",
      "loss: 0.796702  [760100/1106938]\n",
      "loss: 0.727426  [770100/1106938]\n",
      "loss: 0.831660  [780100/1106938]\n",
      "loss: 1.059437  [790100/1106938]\n",
      "loss: 0.696636  [800100/1106938]\n",
      "loss: 0.892437  [810100/1106938]\n",
      "loss: 0.744755  [820100/1106938]\n",
      "loss: 0.970899  [830100/1106938]\n",
      "loss: 0.759295  [840100/1106938]\n",
      "loss: 1.133864  [850100/1106938]\n",
      "loss: 0.942722  [860100/1106938]\n",
      "loss: 0.963500  [870100/1106938]\n",
      "loss: 0.676930  [880100/1106938]\n",
      "loss: 0.785747  [890100/1106938]\n",
      "loss: 0.888662  [900100/1106938]\n",
      "loss: 0.981258  [910100/1106938]\n",
      "loss: 1.048589  [920100/1106938]\n",
      "loss: 0.791208  [930100/1106938]\n",
      "loss: 0.697174  [940100/1106938]\n",
      "loss: 0.837998  [950100/1106938]\n",
      "loss: 0.894719  [960100/1106938]\n",
      "loss: 0.716628  [970100/1106938]\n",
      "loss: 0.925247  [980100/1106938]\n",
      "loss: 0.967671  [990100/1106938]\n",
      "loss: 0.592681  [1000100/1106938]\n",
      "loss: 0.935400  [1010100/1106938]\n",
      "loss: 0.892759  [1020100/1106938]\n",
      "loss: 0.714827  [1030100/1106938]\n",
      "loss: 0.755333  [1040100/1106938]\n",
      "loss: 0.906424  [1050100/1106938]\n",
      "loss: 0.834952  [1060100/1106938]\n",
      "loss: 0.850194  [1070100/1106938]\n",
      "loss: 0.814579  [1080100/1106938]\n",
      "loss: 0.840199  [1090100/1106938]\n",
      "loss: 1.032463  [1100100/1106938]\n",
      "Test Error: \n",
      " Accuracy: 74.7%, Avg loss: 0.751930 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.805745  [  100/1106938]\n",
      "loss: 0.837164  [10100/1106938]\n",
      "loss: 0.806897  [20100/1106938]\n",
      "loss: 0.851852  [30100/1106938]\n",
      "loss: 0.783865  [40100/1106938]\n",
      "loss: 0.740796  [50100/1106938]\n",
      "loss: 0.855686  [60100/1106938]\n",
      "loss: 0.860629  [70100/1106938]\n",
      "loss: 0.889493  [80100/1106938]\n",
      "loss: 0.903817  [90100/1106938]\n",
      "loss: 0.931264  [100100/1106938]\n",
      "loss: 1.039611  [110100/1106938]\n",
      "loss: 0.742301  [120100/1106938]\n",
      "loss: 1.090421  [130100/1106938]\n",
      "loss: 0.694101  [140100/1106938]\n",
      "loss: 0.657855  [150100/1106938]\n",
      "loss: 0.986716  [160100/1106938]\n",
      "loss: 0.803117  [170100/1106938]\n",
      "loss: 1.019118  [180100/1106938]\n",
      "loss: 0.729036  [190100/1106938]\n",
      "loss: 0.956152  [200100/1106938]\n",
      "loss: 0.669318  [210100/1106938]\n",
      "loss: 0.830498  [220100/1106938]\n",
      "loss: 1.057247  [230100/1106938]\n",
      "loss: 0.664266  [240100/1106938]\n",
      "loss: 0.950045  [250100/1106938]\n",
      "loss: 0.695187  [260100/1106938]\n",
      "loss: 0.805707  [270100/1106938]\n",
      "loss: 0.750308  [280100/1106938]\n",
      "loss: 0.779089  [290100/1106938]\n",
      "loss: 0.868059  [300100/1106938]\n",
      "loss: 0.832862  [310100/1106938]\n",
      "loss: 0.953593  [320100/1106938]\n",
      "loss: 0.821401  [330100/1106938]\n",
      "loss: 0.958809  [340100/1106938]\n",
      "loss: 1.018413  [350100/1106938]\n",
      "loss: 0.831192  [360100/1106938]\n",
      "loss: 0.903760  [370100/1106938]\n",
      "loss: 0.700149  [380100/1106938]\n",
      "loss: 0.703016  [390100/1106938]\n",
      "loss: 0.972454  [400100/1106938]\n",
      "loss: 0.726673  [410100/1106938]\n",
      "loss: 0.642312  [420100/1106938]\n",
      "loss: 1.092563  [430100/1106938]\n",
      "loss: 1.023644  [440100/1106938]\n",
      "loss: 0.759008  [450100/1106938]\n",
      "loss: 1.054233  [460100/1106938]\n",
      "loss: 0.752864  [470100/1106938]\n",
      "loss: 1.013471  [480100/1106938]\n",
      "loss: 0.961011  [490100/1106938]\n",
      "loss: 0.870856  [500100/1106938]\n",
      "loss: 0.649421  [510100/1106938]\n",
      "loss: 0.980651  [520100/1106938]\n",
      "loss: 0.946901  [530100/1106938]\n",
      "loss: 1.037852  [540100/1106938]\n",
      "loss: 0.845647  [550100/1106938]\n",
      "loss: 0.857232  [560100/1106938]\n",
      "loss: 0.861854  [570100/1106938]\n",
      "loss: 1.049409  [580100/1106938]\n",
      "loss: 0.598867  [590100/1106938]\n",
      "loss: 0.793610  [600100/1106938]\n",
      "loss: 0.809753  [610100/1106938]\n",
      "loss: 0.720925  [620100/1106938]\n",
      "loss: 0.727939  [630100/1106938]\n",
      "loss: 0.755129  [640100/1106938]\n",
      "loss: 0.948595  [650100/1106938]\n",
      "loss: 0.810144  [660100/1106938]\n",
      "loss: 0.869890  [670100/1106938]\n",
      "loss: 0.942973  [680100/1106938]\n",
      "loss: 0.655713  [690100/1106938]\n",
      "loss: 0.688819  [700100/1106938]\n",
      "loss: 0.958290  [710100/1106938]\n",
      "loss: 0.676627  [720100/1106938]\n",
      "loss: 0.900217  [730100/1106938]\n",
      "loss: 0.810171  [740100/1106938]\n",
      "loss: 0.762323  [750100/1106938]\n",
      "loss: 0.839514  [760100/1106938]\n",
      "loss: 0.724028  [770100/1106938]\n",
      "loss: 1.040982  [780100/1106938]\n",
      "loss: 0.852026  [790100/1106938]\n",
      "loss: 0.850785  [800100/1106938]\n",
      "loss: 0.720098  [810100/1106938]\n",
      "loss: 0.620166  [820100/1106938]\n",
      "loss: 0.574056  [830100/1106938]\n",
      "loss: 0.980437  [840100/1106938]\n",
      "loss: 0.625764  [850100/1106938]\n",
      "loss: 0.595800  [860100/1106938]\n",
      "loss: 0.725072  [870100/1106938]\n",
      "loss: 0.674974  [880100/1106938]\n",
      "loss: 0.859256  [890100/1106938]\n",
      "loss: 0.611698  [900100/1106938]\n",
      "loss: 0.647846  [910100/1106938]\n",
      "loss: 0.800084  [920100/1106938]\n",
      "loss: 1.193562  [930100/1106938]\n",
      "loss: 0.908908  [940100/1106938]\n",
      "loss: 0.855713  [950100/1106938]\n",
      "loss: 0.729092  [960100/1106938]\n",
      "loss: 0.735085  [970100/1106938]\n",
      "loss: 0.840759  [980100/1106938]\n",
      "loss: 0.872228  [990100/1106938]\n",
      "loss: 0.804510  [1000100/1106938]\n",
      "loss: 0.487864  [1010100/1106938]\n",
      "loss: 0.739267  [1020100/1106938]\n",
      "loss: 1.043136  [1030100/1106938]\n",
      "loss: 0.827979  [1040100/1106938]\n",
      "loss: 0.835597  [1050100/1106938]\n",
      "loss: 0.908674  [1060100/1106938]\n",
      "loss: 0.820504  [1070100/1106938]\n",
      "loss: 0.839540  [1080100/1106938]\n",
      "loss: 0.809366  [1090100/1106938]\n",
      "loss: 1.014827  [1100100/1106938]\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.731338 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.797015  [  100/1106938]\n",
      "loss: 0.889038  [10100/1106938]\n",
      "loss: 0.682683  [20100/1106938]\n",
      "loss: 0.787547  [30100/1106938]\n",
      "loss: 0.776149  [40100/1106938]\n",
      "loss: 0.873456  [50100/1106938]\n",
      "loss: 0.856855  [60100/1106938]\n",
      "loss: 0.619116  [70100/1106938]\n",
      "loss: 0.747387  [80100/1106938]\n",
      "loss: 0.739975  [90100/1106938]\n",
      "loss: 0.833518  [100100/1106938]\n",
      "loss: 0.804559  [110100/1106938]\n",
      "loss: 0.948578  [120100/1106938]\n",
      "loss: 0.871930  [130100/1106938]\n",
      "loss: 0.825384  [140100/1106938]\n",
      "loss: 0.980556  [150100/1106938]\n",
      "loss: 0.669246  [160100/1106938]\n",
      "loss: 0.786330  [170100/1106938]\n",
      "loss: 0.961777  [180100/1106938]\n",
      "loss: 0.795073  [190100/1106938]\n",
      "loss: 0.652964  [200100/1106938]\n",
      "loss: 0.901796  [210100/1106938]\n",
      "loss: 1.020712  [220100/1106938]\n",
      "loss: 0.867236  [230100/1106938]\n",
      "loss: 0.718744  [240100/1106938]\n",
      "loss: 0.866607  [250100/1106938]\n",
      "loss: 0.826326  [260100/1106938]\n",
      "loss: 0.848852  [270100/1106938]\n",
      "loss: 0.821097  [280100/1106938]\n",
      "loss: 0.758869  [290100/1106938]\n",
      "loss: 1.038149  [300100/1106938]\n",
      "loss: 0.756246  [310100/1106938]\n",
      "loss: 0.971899  [320100/1106938]\n",
      "loss: 0.953027  [330100/1106938]\n",
      "loss: 0.776450  [340100/1106938]\n",
      "loss: 0.844440  [350100/1106938]\n",
      "loss: 1.017727  [360100/1106938]\n",
      "loss: 0.874421  [370100/1106938]\n",
      "loss: 0.801320  [380100/1106938]\n",
      "loss: 0.671206  [390100/1106938]\n",
      "loss: 0.784791  [400100/1106938]\n",
      "loss: 0.749340  [410100/1106938]\n",
      "loss: 0.684254  [420100/1106938]\n",
      "loss: 0.758609  [430100/1106938]\n",
      "loss: 0.848608  [440100/1106938]\n",
      "loss: 0.729421  [450100/1106938]\n",
      "loss: 0.733834  [460100/1106938]\n",
      "loss: 0.857973  [470100/1106938]\n",
      "loss: 1.109574  [480100/1106938]\n",
      "loss: 0.904043  [490100/1106938]\n",
      "loss: 0.733280  [500100/1106938]\n",
      "loss: 0.742926  [510100/1106938]\n",
      "loss: 1.036695  [520100/1106938]\n",
      "loss: 0.717039  [530100/1106938]\n",
      "loss: 0.653805  [540100/1106938]\n",
      "loss: 0.616650  [550100/1106938]\n",
      "loss: 0.962646  [560100/1106938]\n",
      "loss: 0.768215  [570100/1106938]\n",
      "loss: 0.806047  [580100/1106938]\n",
      "loss: 0.955212  [590100/1106938]\n",
      "loss: 0.712016  [600100/1106938]\n",
      "loss: 0.924001  [610100/1106938]\n",
      "loss: 0.633690  [620100/1106938]\n",
      "loss: 0.610638  [630100/1106938]\n",
      "loss: 0.812254  [640100/1106938]\n",
      "loss: 0.559293  [650100/1106938]\n",
      "loss: 0.867299  [660100/1106938]\n",
      "loss: 0.746159  [670100/1106938]\n",
      "loss: 0.740249  [680100/1106938]\n",
      "loss: 0.873929  [690100/1106938]\n",
      "loss: 0.787925  [700100/1106938]\n",
      "loss: 0.695092  [710100/1106938]\n",
      "loss: 0.702873  [720100/1106938]\n",
      "loss: 0.722364  [730100/1106938]\n",
      "loss: 0.883463  [740100/1106938]\n",
      "loss: 0.688640  [750100/1106938]\n",
      "loss: 0.599961  [760100/1106938]\n",
      "loss: 1.150118  [770100/1106938]\n",
      "loss: 0.739193  [780100/1106938]\n",
      "loss: 0.772325  [790100/1106938]\n",
      "loss: 0.760222  [800100/1106938]\n",
      "loss: 0.659118  [810100/1106938]\n",
      "loss: 0.869116  [820100/1106938]\n",
      "loss: 0.725256  [830100/1106938]\n",
      "loss: 1.062898  [840100/1106938]\n",
      "loss: 1.026668  [850100/1106938]\n",
      "loss: 0.618618  [860100/1106938]\n",
      "loss: 0.642915  [870100/1106938]\n",
      "loss: 0.721787  [880100/1106938]\n",
      "loss: 0.782950  [890100/1106938]\n",
      "loss: 0.572426  [900100/1106938]\n",
      "loss: 0.856068  [910100/1106938]\n",
      "loss: 1.151127  [920100/1106938]\n",
      "loss: 0.731071  [930100/1106938]\n",
      "loss: 0.929053  [940100/1106938]\n",
      "loss: 0.570679  [950100/1106938]\n",
      "loss: 0.754927  [960100/1106938]\n",
      "loss: 0.628410  [970100/1106938]\n",
      "loss: 0.756218  [980100/1106938]\n",
      "loss: 0.768189  [990100/1106938]\n",
      "loss: 0.606574  [1000100/1106938]\n",
      "loss: 0.775926  [1010100/1106938]\n",
      "loss: 0.983342  [1020100/1106938]\n",
      "loss: 0.928900  [1030100/1106938]\n",
      "loss: 0.829105  [1040100/1106938]\n",
      "loss: 0.807379  [1050100/1106938]\n",
      "loss: 0.829407  [1060100/1106938]\n",
      "loss: 0.924378  [1070100/1106938]\n",
      "loss: 0.807368  [1080100/1106938]\n",
      "loss: 1.032735  [1090100/1106938]\n",
      "loss: 0.884650  [1100100/1106938]\n",
      "Test Error: \n",
      " Accuracy: 76.0%, Avg loss: 0.710527 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.754463  [  100/1106938]\n",
      "loss: 0.767684  [10100/1106938]\n",
      "loss: 0.802113  [20100/1106938]\n",
      "loss: 0.740056  [30100/1106938]\n",
      "loss: 0.753687  [40100/1106938]\n",
      "loss: 0.566787  [50100/1106938]\n",
      "loss: 0.730280  [60100/1106938]\n",
      "loss: 0.729890  [70100/1106938]\n",
      "loss: 0.766856  [80100/1106938]\n",
      "loss: 0.601753  [90100/1106938]\n",
      "loss: 0.748949  [100100/1106938]\n",
      "loss: 1.058641  [110100/1106938]\n",
      "loss: 0.992425  [120100/1106938]\n",
      "loss: 0.778141  [130100/1106938]\n",
      "loss: 0.728589  [140100/1106938]\n",
      "loss: 0.693367  [150100/1106938]\n",
      "loss: 0.892428  [160100/1106938]\n",
      "loss: 0.834151  [170100/1106938]\n",
      "loss: 0.783908  [180100/1106938]\n",
      "loss: 0.660364  [190100/1106938]\n",
      "loss: 0.831312  [200100/1106938]\n",
      "loss: 0.749882  [210100/1106938]\n",
      "loss: 0.899669  [220100/1106938]\n",
      "loss: 0.630188  [230100/1106938]\n",
      "loss: 1.017958  [240100/1106938]\n",
      "loss: 0.686796  [250100/1106938]\n",
      "loss: 0.798353  [260100/1106938]\n",
      "loss: 0.894089  [270100/1106938]\n",
      "loss: 0.609039  [280100/1106938]\n",
      "loss: 0.780240  [290100/1106938]\n",
      "loss: 0.989842  [300100/1106938]\n",
      "loss: 0.767007  [310100/1106938]\n",
      "loss: 1.046666  [320100/1106938]\n",
      "loss: 0.989554  [330100/1106938]\n",
      "loss: 0.756694  [340100/1106938]\n",
      "loss: 0.747100  [350100/1106938]\n",
      "loss: 0.748937  [360100/1106938]\n",
      "loss: 0.639425  [370100/1106938]\n",
      "loss: 0.931429  [380100/1106938]\n",
      "loss: 0.747058  [390100/1106938]\n",
      "loss: 0.837726  [400100/1106938]\n",
      "loss: 0.896024  [410100/1106938]\n",
      "loss: 0.647638  [420100/1106938]\n",
      "loss: 0.805698  [430100/1106938]\n",
      "loss: 0.688234  [440100/1106938]\n",
      "loss: 0.880142  [450100/1106938]\n",
      "loss: 0.828120  [460100/1106938]\n",
      "loss: 0.759061  [470100/1106938]\n",
      "loss: 0.861691  [480100/1106938]\n",
      "loss: 0.847438  [490100/1106938]\n",
      "loss: 0.875449  [500100/1106938]\n",
      "loss: 0.683689  [510100/1106938]\n",
      "loss: 0.873675  [520100/1106938]\n",
      "loss: 0.997095  [530100/1106938]\n",
      "loss: 0.853643  [540100/1106938]\n",
      "loss: 0.805987  [550100/1106938]\n",
      "loss: 0.870882  [560100/1106938]\n",
      "loss: 1.011597  [570100/1106938]\n",
      "loss: 0.901580  [580100/1106938]\n",
      "loss: 0.754968  [590100/1106938]\n",
      "loss: 1.001902  [600100/1106938]\n",
      "loss: 0.800237  [610100/1106938]\n",
      "loss: 0.752349  [620100/1106938]\n",
      "loss: 0.705365  [630100/1106938]\n",
      "loss: 0.597662  [640100/1106938]\n",
      "loss: 0.519151  [650100/1106938]\n",
      "loss: 0.594271  [660100/1106938]\n",
      "loss: 0.837407  [670100/1106938]\n",
      "loss: 0.619442  [680100/1106938]\n",
      "loss: 0.765178  [690100/1106938]\n",
      "loss: 0.898049  [700100/1106938]\n",
      "loss: 0.720717  [710100/1106938]\n",
      "loss: 0.761710  [720100/1106938]\n",
      "loss: 0.875380  [730100/1106938]\n",
      "loss: 0.956611  [740100/1106938]\n",
      "loss: 0.941993  [750100/1106938]\n",
      "loss: 0.756581  [760100/1106938]\n",
      "loss: 0.780896  [770100/1106938]\n",
      "loss: 0.827437  [780100/1106938]\n",
      "loss: 0.801613  [790100/1106938]\n",
      "loss: 0.758148  [800100/1106938]\n",
      "loss: 0.683226  [810100/1106938]\n",
      "loss: 0.655870  [820100/1106938]\n",
      "loss: 0.829372  [830100/1106938]\n",
      "loss: 0.838126  [840100/1106938]\n",
      "loss: 0.873435  [850100/1106938]\n",
      "loss: 0.776167  [860100/1106938]\n",
      "loss: 0.830272  [870100/1106938]\n",
      "loss: 0.614001  [880100/1106938]\n",
      "loss: 0.807672  [890100/1106938]\n",
      "loss: 0.848783  [900100/1106938]\n",
      "loss: 0.967449  [910100/1106938]\n",
      "loss: 0.839370  [920100/1106938]\n",
      "loss: 0.732568  [930100/1106938]\n",
      "loss: 0.735925  [940100/1106938]\n",
      "loss: 0.839283  [950100/1106938]\n",
      "loss: 0.725971  [960100/1106938]\n",
      "loss: 0.534908  [970100/1106938]\n",
      "loss: 0.985982  [980100/1106938]\n",
      "loss: 0.570243  [990100/1106938]\n",
      "loss: 0.765557  [1000100/1106938]\n",
      "loss: 1.009993  [1010100/1106938]\n",
      "loss: 0.903359  [1020100/1106938]\n",
      "loss: 0.884795  [1030100/1106938]\n",
      "loss: 0.878048  [1040100/1106938]\n",
      "loss: 0.767711  [1050100/1106938]\n",
      "loss: 0.820243  [1060100/1106938]\n",
      "loss: 0.645673  [1070100/1106938]\n",
      "loss: 0.680262  [1080100/1106938]\n",
      "loss: 0.942192  [1090100/1106938]\n",
      "loss: 0.712841  [1100100/1106938]\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.699847 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.814407  [  100/1106938]\n",
      "loss: 0.714105  [10100/1106938]\n",
      "loss: 0.969030  [20100/1106938]\n",
      "loss: 0.772332  [30100/1106938]\n",
      "loss: 0.810301  [40100/1106938]\n",
      "loss: 0.854732  [50100/1106938]\n",
      "loss: 0.737142  [60100/1106938]\n",
      "loss: 0.605221  [70100/1106938]\n",
      "loss: 0.893137  [80100/1106938]\n",
      "loss: 0.755168  [90100/1106938]\n",
      "loss: 0.776442  [100100/1106938]\n",
      "loss: 0.925513  [110100/1106938]\n",
      "loss: 0.746801  [120100/1106938]\n",
      "loss: 0.767636  [130100/1106938]\n",
      "loss: 0.632606  [140100/1106938]\n",
      "loss: 0.878251  [150100/1106938]\n",
      "loss: 0.789717  [160100/1106938]\n",
      "loss: 0.900676  [170100/1106938]\n",
      "loss: 0.733028  [180100/1106938]\n",
      "loss: 0.778099  [190100/1106938]\n",
      "loss: 0.844211  [200100/1106938]\n",
      "loss: 0.759431  [210100/1106938]\n",
      "loss: 0.794675  [220100/1106938]\n",
      "loss: 0.769916  [230100/1106938]\n",
      "loss: 1.162339  [240100/1106938]\n",
      "loss: 0.864092  [250100/1106938]\n",
      "loss: 1.000456  [260100/1106938]\n",
      "loss: 0.727671  [270100/1106938]\n",
      "loss: 0.693127  [280100/1106938]\n",
      "loss: 1.059719  [290100/1106938]\n",
      "loss: 0.886276  [300100/1106938]\n",
      "loss: 0.663225  [310100/1106938]\n",
      "loss: 0.942428  [320100/1106938]\n",
      "loss: 0.970156  [330100/1106938]\n",
      "loss: 0.650895  [340100/1106938]\n",
      "loss: 0.722516  [350100/1106938]\n",
      "loss: 0.666506  [360100/1106938]\n",
      "loss: 0.577883  [370100/1106938]\n",
      "loss: 0.633624  [380100/1106938]\n",
      "loss: 0.689485  [390100/1106938]\n",
      "loss: 0.651916  [400100/1106938]\n",
      "loss: 0.687040  [410100/1106938]\n",
      "loss: 0.693264  [420100/1106938]\n",
      "loss: 0.648195  [430100/1106938]\n",
      "loss: 1.030718  [440100/1106938]\n",
      "loss: 0.928051  [450100/1106938]\n",
      "loss: 0.748937  [460100/1106938]\n",
      "loss: 0.618709  [470100/1106938]\n",
      "loss: 0.783527  [480100/1106938]\n",
      "loss: 0.779419  [490100/1106938]\n",
      "loss: 0.708975  [500100/1106938]\n",
      "loss: 0.959548  [510100/1106938]\n",
      "loss: 0.909080  [520100/1106938]\n",
      "loss: 0.716089  [530100/1106938]\n",
      "loss: 0.990585  [540100/1106938]\n",
      "loss: 0.944134  [550100/1106938]\n",
      "loss: 0.914901  [560100/1106938]\n",
      "loss: 0.874421  [570100/1106938]\n",
      "loss: 0.856834  [580100/1106938]\n",
      "loss: 0.565648  [590100/1106938]\n",
      "loss: 0.805652  [600100/1106938]\n",
      "loss: 0.727477  [610100/1106938]\n",
      "loss: 0.607036  [620100/1106938]\n",
      "loss: 0.888242  [630100/1106938]\n",
      "loss: 0.773194  [640100/1106938]\n",
      "loss: 0.912642  [650100/1106938]\n",
      "loss: 1.018407  [660100/1106938]\n",
      "loss: 0.674014  [670100/1106938]\n",
      "loss: 0.667860  [680100/1106938]\n",
      "loss: 0.583945  [690100/1106938]\n",
      "loss: 0.732726  [700100/1106938]\n",
      "loss: 1.016166  [710100/1106938]\n",
      "loss: 0.686314  [720100/1106938]\n",
      "loss: 0.753316  [730100/1106938]\n",
      "loss: 1.044325  [740100/1106938]\n",
      "loss: 0.717765  [750100/1106938]\n",
      "loss: 0.721406  [760100/1106938]\n",
      "loss: 0.750732  [770100/1106938]\n",
      "loss: 0.652618  [780100/1106938]\n",
      "loss: 0.666172  [790100/1106938]\n",
      "loss: 0.704589  [800100/1106938]\n",
      "loss: 0.657852  [810100/1106938]\n",
      "loss: 0.805634  [820100/1106938]\n",
      "loss: 0.587602  [830100/1106938]\n",
      "loss: 0.698115  [840100/1106938]\n",
      "loss: 0.830382  [850100/1106938]\n",
      "loss: 0.807662  [860100/1106938]\n",
      "loss: 0.861970  [870100/1106938]\n",
      "loss: 1.026845  [880100/1106938]\n",
      "loss: 0.759093  [890100/1106938]\n",
      "loss: 0.840701  [900100/1106938]\n",
      "loss: 0.925679  [910100/1106938]\n",
      "loss: 0.804948  [920100/1106938]\n",
      "loss: 0.704672  [930100/1106938]\n",
      "loss: 0.818480  [940100/1106938]\n",
      "loss: 0.926128  [950100/1106938]\n",
      "loss: 0.851232  [960100/1106938]\n",
      "loss: 0.836320  [970100/1106938]\n",
      "loss: 0.785207  [980100/1106938]\n",
      "loss: 0.814060  [990100/1106938]\n",
      "loss: 0.796808  [1000100/1106938]\n",
      "loss: 0.805029  [1010100/1106938]\n",
      "loss: 0.772172  [1020100/1106938]\n",
      "loss: 0.840781  [1030100/1106938]\n",
      "loss: 0.826666  [1040100/1106938]\n",
      "loss: 0.880173  [1050100/1106938]\n",
      "loss: 0.911808  [1060100/1106938]\n",
      "loss: 0.961881  [1070100/1106938]\n",
      "loss: 0.731768  [1080100/1106938]\n",
      "loss: 0.861235  [1090100/1106938]\n",
      "loss: 0.707717  [1100100/1106938]\n",
      "Test Error: \n",
      " Accuracy: 76.8%, Avg loss: 0.685140 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.667588  [  100/1106938]\n",
      "loss: 0.715998  [10100/1106938]\n",
      "loss: 0.619743  [20100/1106938]\n",
      "loss: 0.622597  [30100/1106938]\n",
      "loss: 0.636527  [40100/1106938]\n",
      "loss: 0.954813  [50100/1106938]\n",
      "loss: 0.763752  [60100/1106938]\n",
      "loss: 0.820029  [70100/1106938]\n",
      "loss: 0.770065  [80100/1106938]\n",
      "loss: 0.785190  [90100/1106938]\n",
      "loss: 0.815135  [100100/1106938]\n",
      "loss: 0.752060  [110100/1106938]\n",
      "loss: 0.795662  [120100/1106938]\n",
      "loss: 0.598004  [130100/1106938]\n",
      "loss: 0.897657  [140100/1106938]\n",
      "loss: 0.682432  [150100/1106938]\n",
      "loss: 0.901543  [160100/1106938]\n",
      "loss: 0.733540  [170100/1106938]\n",
      "loss: 0.923060  [180100/1106938]\n",
      "loss: 0.717865  [190100/1106938]\n",
      "loss: 0.684168  [200100/1106938]\n",
      "loss: 0.586376  [210100/1106938]\n",
      "loss: 0.632233  [220100/1106938]\n",
      "loss: 1.007192  [230100/1106938]\n",
      "loss: 0.691242  [240100/1106938]\n",
      "loss: 0.833014  [250100/1106938]\n",
      "loss: 0.820551  [260100/1106938]\n",
      "loss: 0.790304  [270100/1106938]\n",
      "loss: 0.797333  [280100/1106938]\n",
      "loss: 0.742131  [290100/1106938]\n",
      "loss: 0.687289  [300100/1106938]\n",
      "loss: 0.847282  [310100/1106938]\n",
      "loss: 0.841929  [320100/1106938]\n",
      "loss: 0.755664  [330100/1106938]\n",
      "loss: 0.774598  [340100/1106938]\n",
      "loss: 0.708298  [350100/1106938]\n",
      "loss: 0.637762  [360100/1106938]\n",
      "loss: 0.608517  [370100/1106938]\n",
      "loss: 0.767666  [380100/1106938]\n",
      "loss: 0.669860  [390100/1106938]\n",
      "loss: 0.940115  [400100/1106938]\n",
      "loss: 0.723096  [410100/1106938]\n",
      "loss: 1.038532  [420100/1106938]\n",
      "loss: 0.741433  [430100/1106938]\n",
      "loss: 0.912577  [440100/1106938]\n",
      "loss: 0.703902  [450100/1106938]\n",
      "loss: 1.196673  [460100/1106938]\n",
      "loss: 0.747129  [470100/1106938]\n",
      "loss: 0.779731  [480100/1106938]\n",
      "loss: 0.749805  [490100/1106938]\n",
      "loss: 0.801580  [500100/1106938]\n",
      "loss: 0.720342  [510100/1106938]\n",
      "loss: 0.967941  [520100/1106938]\n",
      "loss: 0.762418  [530100/1106938]\n",
      "loss: 0.651597  [540100/1106938]\n",
      "loss: 0.726883  [550100/1106938]\n",
      "loss: 0.696533  [560100/1106938]\n",
      "loss: 0.818206  [570100/1106938]\n",
      "loss: 0.665202  [580100/1106938]\n",
      "loss: 0.844200  [590100/1106938]\n",
      "loss: 0.826475  [600100/1106938]\n",
      "loss: 0.762283  [610100/1106938]\n",
      "loss: 0.940587  [620100/1106938]\n",
      "loss: 0.608295  [630100/1106938]\n",
      "loss: 0.883534  [640100/1106938]\n",
      "loss: 0.749016  [650100/1106938]\n",
      "loss: 0.782415  [660100/1106938]\n",
      "loss: 0.800847  [670100/1106938]\n",
      "loss: 0.723565  [680100/1106938]\n",
      "loss: 0.890945  [690100/1106938]\n",
      "loss: 0.727414  [700100/1106938]\n",
      "loss: 0.601700  [710100/1106938]\n",
      "loss: 0.797802  [720100/1106938]\n",
      "loss: 0.677861  [730100/1106938]\n",
      "loss: 0.948591  [740100/1106938]\n",
      "loss: 0.677020  [750100/1106938]\n",
      "loss: 0.662277  [760100/1106938]\n",
      "loss: 0.864235  [770100/1106938]\n",
      "loss: 0.909049  [780100/1106938]\n",
      "loss: 0.748144  [790100/1106938]\n",
      "loss: 0.784920  [800100/1106938]\n",
      "loss: 0.597585  [810100/1106938]\n",
      "loss: 0.619714  [820100/1106938]\n",
      "loss: 0.709541  [830100/1106938]\n",
      "loss: 0.712767  [840100/1106938]\n",
      "loss: 0.589275  [850100/1106938]\n",
      "loss: 0.750914  [860100/1106938]\n",
      "loss: 0.848132  [870100/1106938]\n",
      "loss: 0.992444  [880100/1106938]\n",
      "loss: 0.877887  [890100/1106938]\n",
      "loss: 0.861734  [900100/1106938]\n",
      "loss: 0.879064  [910100/1106938]\n",
      "loss: 0.891785  [920100/1106938]\n",
      "loss: 0.679662  [930100/1106938]\n",
      "loss: 0.833513  [940100/1106938]\n",
      "loss: 0.654732  [950100/1106938]\n",
      "loss: 0.695107  [960100/1106938]\n",
      "loss: 0.599616  [970100/1106938]\n",
      "loss: 0.785030  [980100/1106938]\n",
      "loss: 0.758390  [990100/1106938]\n",
      "loss: 0.690947  [1000100/1106938]\n",
      "loss: 0.787298  [1010100/1106938]\n",
      "loss: 0.807645  [1020100/1106938]\n",
      "loss: 0.884207  [1030100/1106938]\n",
      "loss: 0.660512  [1040100/1106938]\n",
      "loss: 0.689365  [1050100/1106938]\n",
      "loss: 0.805921  [1060100/1106938]\n",
      "loss: 1.107450  [1070100/1106938]\n",
      "loss: 0.712255  [1080100/1106938]\n",
      "loss: 0.525493  [1090100/1106938]\n",
      "loss: 0.518305  [1100100/1106938]\n",
      "Test Error: \n",
      " Accuracy: 77.0%, Avg loss: 0.676577 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "min_loss = 1.0\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    dev_loss = dev(dev_dataloader, model, loss_fn)\n",
    "\n",
    "    if dev_loss < min_loss:\n",
    "        min_loss = dev_loss\n",
    "        torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([36, 36, 36,  ..., 30, 30, 25])\n"
     ]
    }
   ],
   "source": [
    "def test(dataloader, model):\n",
    "    del model\n",
    "    model = NeuralNetwork()\n",
    "    ckpt = torch.load(\"model.pth\")\n",
    "    model.load_state_dict(ckpt)\n",
    "    model.eval()\n",
    "\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for X in dataloader:\n",
    "            pred = model(X)\n",
    "            pred = pred.squeeze()\n",
    "            preds.append(pred.argmax(1))\n",
    "    \n",
    "    preds = torch.cat(preds, dim=0).long()\n",
    "    print(preds)\n",
    "    save_result(preds)\n",
    "    \n",
    "test(test_dataloader, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
