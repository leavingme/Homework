{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hw2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Size of train data:  (1229932, 429)\n",
      "Size of train label:  (1229932,)\n",
      "Size of test data:  (451552, 429)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "def save_result(preds):\n",
    "    with open('test_result.csv', 'w', encoding='utf-8') as f:\n",
    "        f.write('Id,Class\\n')\n",
    "        for i, pred in enumerate(preds):\n",
    "            f.write('%d,%d\\n' % (i, pred))\n",
    "    \n",
    "    return\n",
    "\n",
    "data_root = './timit_11/'\n",
    "\n",
    "train = np.load(data_root + 'train_11.npy')\n",
    "train_label = np.load(data_root + 'train_label_11.npy')\n",
    "test = np.load(data_root + 'test_11.npy')\n",
    "\n",
    "print('Size of train data: ', train.shape)\n",
    "print('Size of train label: ', train_label.shape)\n",
    "print('Size of test data: ', test.shape)\n",
    "\n",
    "class TIMITDataset(Dataset): \n",
    "    def __init__(self, data, label, mode=\"train\"):\n",
    "        self.mode = mode\n",
    "        self.data = pd.DataFrame(data)\n",
    "        self.label = pd.DataFrame(label)\n",
    "\n",
    "        # 需要区分 train data 和 test data\n",
    "        if self.mode == \"train\":\n",
    "            indices = [i for i in range(len(self.data)) if i % 10 != 0]\n",
    "            self.data = self.data.iloc[indices, :]\n",
    "            self.label = self.label.iloc[indices]\n",
    "\n",
    "        elif self.mode == \"dev\":\n",
    "            indices = [i for i in range(len(self.data)) if i % 10 == 0]\n",
    "            self.data = self.data.iloc[indices, :]\n",
    "            self.label = self.label.iloc[indices]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == \"train\" or self.mode == \"dev\":\n",
    "            X = torch.tensor(self.data.iloc[idx], dtype=torch.float32)\n",
    "            y = torch.tensor(int(self.label.iloc[idx]), dtype=torch.int32)\n",
    "            return X, y\n",
    "        elif self.mode == \"test\":\n",
    "            return torch.tensor(self.data.iloc[idx], dtype=torch.float32)\n",
    "        \n",
    "train_data = TIMITDataset(train, train_label, mode=\"train\")\n",
    "dev_data = TIMITDataset(train, train_label, mode=\"dev\")\n",
    "test_data = TIMITDataset(test, None, mode=\"test\")\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=100, shuffle=True)\n",
    "dev_dataloader = DataLoader(dev_data, batch_size=100, shuffle=False)\n",
    "test_dataloader = DataLoader(test_data, batch_size=100, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method TIMITDataset.__len__ of <__main__.TIMITDataset object at 0x12981ce30>>\n",
      "[-8.15812826e-01 -3.48689795e-01  3.95477504e-01 -1.63738783e-02\n",
      "  8.60317469e-01  1.44315893e-02  3.84072900e-01  9.11684811e-01\n",
      "  2.74616122e-01  7.21527755e-01  1.62349343e+00  1.94353950e+00\n",
      "  9.64264333e-01 -7.72595452e-03 -1.81192949e-01  5.74482083e-01\n",
      " -2.26134375e-01  4.03959244e-01  3.92566890e-01  4.68575805e-01\n",
      "  9.03372943e-01  9.12166536e-02  6.29455924e-01  6.50277019e-01\n",
      " -7.98417151e-01 -5.07435977e-01 -1.93714548e-03 -4.96793658e-01\n",
      " -1.30517155e-01  1.90668270e-01  3.52899522e-01  1.43388236e+00\n",
      "  5.82235932e-01 -4.42176044e-01  3.92167211e-01  3.82801175e-01\n",
      " -5.74597836e-01 -9.66160059e-01  8.36981013e-02 -8.17853749e-01\n",
      " -3.47191900e-01  5.85275650e-01 -1.76819056e-01  6.26596749e-01\n",
      " -2.08324268e-01  1.41138196e-01  8.14121485e-01  1.20860793e-01\n",
      "  7.84978092e-01  1.85303009e+00  2.17269802e+00  1.76232314e+00\n",
      "  4.56195819e-04 -3.57095480e-01  4.66112584e-01  4.36793976e-02\n",
      "  2.44239345e-01  6.95920229e-01  5.79180181e-01  4.36306536e-01\n",
      "  2.77873218e-01  5.59000432e-01  2.24675238e-01 -8.69635940e-01\n",
      " -2.87959218e-01  2.19163373e-02 -3.04890573e-01 -3.68410289e-01\n",
      "  6.86901152e-01  1.93446845e-01  1.22113764e+00  6.87861562e-01\n",
      " -8.80873322e-01  2.27267519e-01 -3.34653795e-01 -1.30035055e+00\n",
      " -7.05772698e-01 -4.29470271e-01 -8.19877267e-01 -5.21579981e-01\n",
      "  6.54818118e-01 -2.47517794e-01  7.76656926e-01  4.93845195e-01\n",
      "  6.38528287e-01  1.34373748e+00  1.16990530e+00  1.61719799e+00\n",
      "  1.58583629e+00  7.26012826e-01  3.88572276e-01  4.44570705e-02\n",
      " -8.08226705e-01  2.70812511e-01  5.06846428e-01 -3.04862764e-02\n",
      "  9.13899004e-01  6.55444324e-01  1.99248157e-02  1.21349134e-01\n",
      "  8.46248269e-01 -6.90799773e-01 -7.40965366e-01 -2.29644522e-01\n",
      "  1.29772574e-01 -1.11926723e+00 -3.95132959e-01  1.44303000e+00\n",
      " -1.17166817e+00 -3.43524873e-01 -3.21841627e-01 -6.46609664e-01\n",
      "  2.92410403e-01  8.45067203e-02 -1.61831212e+00  6.18455052e-01\n",
      "  6.53575718e-01 -8.21439385e-01 -5.79824984e-01  7.55298138e-01\n",
      " -1.08926654e-01  1.01349998e+00  9.15398777e-01  7.18544662e-01\n",
      "  9.52252090e-01  6.39603972e-01  1.29298186e+00  1.33980155e+00\n",
      "  7.12813556e-01  8.59630764e-01  1.09658375e-01 -1.40015316e+00\n",
      "  1.04810074e-01  8.90806198e-01 -5.54478347e-01  5.59858501e-01\n",
      "  4.76976901e-01 -4.27572317e-02  4.54809010e-01  8.39707017e-01\n",
      " -1.09382379e+00 -3.02106410e-01 -2.24989846e-01  2.37577826e-01\n",
      " -1.64795864e+00 -3.36687922e-01  1.51167357e+00 -1.72091830e+00\n",
      " -1.14630747e+00 -4.85988587e-01 -5.19942343e-01 -2.66638696e-01\n",
      " -1.06537372e-01 -7.84751356e-01  1.46447480e+00  3.82050872e-01\n",
      " -8.22048783e-01 -6.27880633e-01  7.42801547e-01 -3.90241779e-02\n",
      "  1.18365037e+00  1.21759546e+00  1.00882006e+00  1.14920890e+00\n",
      "  7.69170105e-01  1.35666311e+00  1.38351047e+00  7.51709640e-01\n",
      "  8.97786200e-01  1.88909724e-01 -1.84347391e+00 -5.92376217e-02\n",
      "  1.00558782e+00 -1.07068503e+00  2.94810176e-01  4.48351741e-01\n",
      " -2.13550150e-01  1.71433255e-01  7.12499261e-01 -7.72415757e-01\n",
      " -1.92916185e-01 -5.73165357e-01  3.06467086e-01 -1.47141361e+00\n",
      " -5.06693542e-01  4.55461711e-01 -1.67400038e+00 -1.24507964e+00\n",
      " -3.37909043e-01 -9.42163840e-02 -7.19930530e-01 -3.06202561e-01\n",
      "  7.89523959e-01  1.54539812e+00 -4.80546117e-01 -8.01864445e-01\n",
      " -7.07150519e-01  6.28714204e-01  3.89305472e-01  7.86976397e-01\n",
      "  1.08158088e+00  1.06749725e+00  5.51742494e-01  8.54246080e-01\n",
      "  9.38302457e-01  7.02012718e-01  8.34094465e-01  7.78473735e-01\n",
      "  2.46743500e-01 -2.00786448e+00 -2.59372681e-01  1.01744902e+00\n",
      " -1.21515656e+00  1.41148657e-01  2.77194500e-01 -6.88229918e-01\n",
      " -7.03711510e-01  2.29979977e-01 -6.55366600e-01  5.08279741e-01\n",
      " -7.98558593e-01  2.42155805e-01 -6.39601648e-01 -3.57569396e-01\n",
      " -5.19201696e-01 -1.06722796e+00 -8.76354277e-01 -6.17329240e-01\n",
      " -6.06594145e-01 -1.51309419e+00 -7.37058938e-01  1.06613851e+00\n",
      "  1.54067111e+00 -2.66122669e-01 -7.07047403e-01 -1.49290991e+00\n",
      "  5.77214837e-01  1.01184523e+00  2.28464417e-02  4.62869883e-01\n",
      "  5.73139429e-01  6.86030209e-01  6.59196913e-01  2.08747530e+00\n",
      " -8.50434899e-02  1.52945018e+00  1.38857460e+00  3.07250828e-01\n",
      " -1.93132412e+00 -2.75336593e-01  7.48762012e-01 -1.39199424e+00\n",
      " -2.62844265e-01 -3.80406469e-01 -9.25785244e-01 -1.07260036e+00\n",
      "  1.93184674e-01 -2.34122247e-01  8.09500456e-01 -3.79938722e-01\n",
      "  1.68058470e-01  3.26376736e-01 -1.23876520e-02 -1.41517222e+00\n",
      "  4.59585823e-02 -7.19508082e-02 -8.91312122e-01 -1.21071845e-01\n",
      " -4.21448231e-01  1.49552613e-01  1.02639949e+00 -6.27906859e-01\n",
      " -7.97479987e-01 -6.25015140e-01 -1.97806239e+00  5.62663913e-01\n",
      "  1.08202255e+00 -3.15571904e-01  1.60826400e-01  7.12553084e-01\n",
      "  8.63311768e-01  8.94219697e-01  1.68949175e+00  5.86832941e-01\n",
      "  1.82698238e+00  7.64400065e-01  3.78606379e-01 -1.93906581e+00\n",
      " -2.54375786e-01  2.99605757e-01 -1.52195930e+00 -3.10934246e-01\n",
      " -4.56030488e-01 -3.86570841e-01 -1.04673004e+00 -8.25209916e-03\n",
      "  4.21362609e-01  7.00726211e-01 -6.20774686e-01  1.31356940e-01\n",
      "  9.06182945e-01  2.47731835e-01 -1.79323173e+00  8.11256707e-01\n",
      "  6.08145833e-01 -3.43659341e-01  5.73926151e-01 -4.04250085e-01\n",
      " -6.75222874e-01  1.18168211e+00 -8.53463888e-01 -8.56958423e-03\n",
      " -5.41229427e-01 -2.12455940e+00  3.38244736e-01  6.46516800e-01\n",
      " -5.64446926e-01  4.48248535e-01  1.05095375e+00  8.40595901e-01\n",
      "  3.11493397e-01  1.61268258e+00  1.73551130e+00  1.29770410e+00\n",
      " -6.98527768e-02  4.62385297e-01 -1.74325550e+00 -9.84729901e-02\n",
      " -1.19800247e-01 -1.28110456e+00 -1.72435850e-01 -5.89624405e-01\n",
      " -1.42206907e-01 -1.27849662e+00 -3.17881048e-01  5.10424614e-01\n",
      "  3.22074503e-01 -4.75895911e-01  1.56983048e-01  1.39186656e+00\n",
      "  4.53995079e-01 -1.52112699e+00  1.43880963e+00  8.51973772e-01\n",
      "  2.16113012e-02  8.90362442e-01 -5.90594001e-02 -8.75823736e-01\n",
      "  2.90102303e-01 -1.13367736e+00  6.45637989e-01 -5.26147008e-01\n",
      " -2.06020331e+00  2.99371749e-01  6.83552027e-01 -3.23276311e-01\n",
      "  7.36325026e-01  6.55510664e-01 -1.81245700e-01 -9.77546930e-01\n",
      "  9.46608126e-01  9.21990693e-01  2.14190197e+00  2.17398033e-01\n",
      "  5.16363978e-01 -1.36978412e+00  1.74136609e-01 -4.52454537e-01\n",
      " -9.24559832e-01 -8.75526816e-02 -8.43120098e-01  3.46856117e-01\n",
      " -9.60662067e-01 -4.31923479e-01  8.75602186e-01 -3.55560482e-02\n",
      " -7.17057049e-01  1.60263851e-01  1.31419182e+00  6.40638173e-01\n",
      " -3.64474535e-01  8.50858510e-01  1.52416348e-01 -3.61995876e-01\n",
      "  7.94132769e-01  8.49550247e-01 -5.72304070e-01 -4.94386554e-01\n",
      " -8.14543962e-01  2.29587063e-01 -4.44377780e-01 -2.02960443e+00\n",
      "  5.13314545e-01  4.00894850e-01 -4.04429317e-01  5.56219280e-01\n",
      " -2.18392864e-01  2.01895729e-01  9.18949246e-02  2.01063347e+00\n",
      "  1.16167784e+00  9.79497254e-01  4.92651969e-01  4.97839808e-01\n",
      " -7.56824553e-01  3.33294928e-01 -6.37785673e-01 -3.69842678e-01\n",
      "  1.70299485e-01 -6.15268171e-01  8.54063451e-01 -4.88241553e-01\n",
      " -1.02353287e+00  2.20091596e-01 -8.97782147e-01 -9.17488098e-01\n",
      "  5.88676706e-02  5.84376633e-01  5.10970116e-01  6.05664790e-01\n",
      "  5.34149706e-01 -3.20446759e-01 -1.88363567e-02  1.45047593e+00\n",
      "  1.28690445e+00 -4.73928377e-02 -1.81417489e+00 -8.92933309e-01\n",
      "  2.31251955e-01]\n"
     ]
    }
   ],
   "source": [
    "print(train_data.__len__)\n",
    "\n",
    "# 判断变量类型\n",
    "# print(train[0])\n",
    "# print(type(train[0]))\n",
    "\n",
    "# 判断变量类型\n",
    "print(test[0])\n",
    "# print(type(test[0]))\n",
    "\n",
    "\n",
    "# print(train_label[0])\n",
    "# print(type(train_label[0]))\n",
    "# print(type(train_data))\n",
    "# print(type(train_data[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=429, out_features=1024, bias=True)\n",
      "    (1): Dropout(p=0.2, inplace=False)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (4): Dropout(p=0.2, inplace=False)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (7): Dropout(p=0.2, inplace=False)\n",
      "    (8): ReLU()\n",
      "    (9): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (10): Dropout(p=0.2, inplace=False)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=128, out_features=39, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(11*39, 1024),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 39)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "loss_record = {\"train\": [], \"dev\": []}\n",
    "pred_record = []\n",
    "target_record = []\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss_record[\"train\"].append(loss.item())\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "def dev(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "    return test_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2c/pn5qg61n16nfl0b9y726w55r0000gn/T/ipykernel_28071/1622385451.py:48: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
      "  y = torch.tensor(int(self.label.iloc[idx]), dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 3.656954  [  100/1106938]\n",
      "loss: 2.996101  [10100/1106938]\n",
      "loss: 2.283241  [20100/1106938]\n",
      "loss: 2.064343  [30100/1106938]\n",
      "loss: 2.170931  [40100/1106938]\n",
      "loss: 1.837096  [50100/1106938]\n",
      "loss: 1.931825  [60100/1106938]\n",
      "loss: 1.979110  [70100/1106938]\n",
      "loss: 1.823850  [80100/1106938]\n",
      "loss: 1.984595  [90100/1106938]\n",
      "loss: 1.940647  [100100/1106938]\n",
      "loss: 1.871321  [110100/1106938]\n",
      "loss: 1.734468  [120100/1106938]\n",
      "loss: 1.459019  [130100/1106938]\n",
      "loss: 1.719916  [140100/1106938]\n",
      "loss: 1.780577  [150100/1106938]\n",
      "loss: 1.646089  [160100/1106938]\n",
      "loss: 1.411963  [170100/1106938]\n",
      "loss: 1.783420  [180100/1106938]\n",
      "loss: 1.450769  [190100/1106938]\n",
      "loss: 1.754606  [200100/1106938]\n",
      "loss: 1.508600  [210100/1106938]\n",
      "loss: 1.291872  [220100/1106938]\n",
      "loss: 1.347695  [230100/1106938]\n",
      "loss: 1.309593  [240100/1106938]\n",
      "loss: 1.538297  [250100/1106938]\n",
      "loss: 1.445377  [260100/1106938]\n",
      "loss: 1.268048  [270100/1106938]\n",
      "loss: 1.492902  [280100/1106938]\n",
      "loss: 1.416960  [290100/1106938]\n",
      "loss: 1.243220  [300100/1106938]\n",
      "loss: 1.091513  [310100/1106938]\n",
      "loss: 1.380215  [320100/1106938]\n",
      "loss: 1.264820  [330100/1106938]\n",
      "loss: 1.513837  [340100/1106938]\n",
      "loss: 1.167587  [350100/1106938]\n",
      "loss: 1.455541  [360100/1106938]\n",
      "loss: 1.232980  [370100/1106938]\n",
      "loss: 1.369101  [380100/1106938]\n",
      "loss: 1.207781  [390100/1106938]\n",
      "loss: 1.544158  [400100/1106938]\n",
      "loss: 1.393000  [410100/1106938]\n",
      "loss: 1.509835  [420100/1106938]\n",
      "loss: 1.347010  [430100/1106938]\n",
      "loss: 1.340538  [440100/1106938]\n",
      "loss: 1.338819  [450100/1106938]\n",
      "loss: 1.345416  [460100/1106938]\n",
      "loss: 1.299135  [470100/1106938]\n",
      "loss: 1.368731  [480100/1106938]\n",
      "loss: 1.280998  [490100/1106938]\n",
      "loss: 0.985943  [500100/1106938]\n",
      "loss: 1.405118  [510100/1106938]\n",
      "loss: 1.522289  [520100/1106938]\n",
      "loss: 1.339731  [530100/1106938]\n",
      "loss: 1.319743  [540100/1106938]\n",
      "loss: 1.014594  [550100/1106938]\n",
      "loss: 1.194983  [560100/1106938]\n",
      "loss: 1.151724  [570100/1106938]\n",
      "loss: 1.028591  [580100/1106938]\n",
      "loss: 1.292665  [590100/1106938]\n",
      "loss: 1.540021  [600100/1106938]\n",
      "loss: 1.400980  [610100/1106938]\n",
      "loss: 1.479178  [620100/1106938]\n",
      "loss: 1.463870  [630100/1106938]\n",
      "loss: 1.323940  [640100/1106938]\n",
      "loss: 1.142367  [650100/1106938]\n",
      "loss: 1.084527  [660100/1106938]\n",
      "loss: 1.393818  [670100/1106938]\n",
      "loss: 1.080922  [680100/1106938]\n",
      "loss: 1.079506  [690100/1106938]\n",
      "loss: 1.310884  [700100/1106938]\n",
      "loss: 1.174534  [710100/1106938]\n",
      "loss: 1.358752  [720100/1106938]\n",
      "loss: 1.311193  [730100/1106938]\n",
      "loss: 1.295982  [740100/1106938]\n",
      "loss: 1.134308  [750100/1106938]\n",
      "loss: 1.239742  [760100/1106938]\n",
      "loss: 1.145270  [770100/1106938]\n",
      "loss: 1.047119  [780100/1106938]\n",
      "loss: 1.372345  [790100/1106938]\n",
      "loss: 1.462793  [800100/1106938]\n",
      "loss: 1.101223  [810100/1106938]\n",
      "loss: 1.273990  [820100/1106938]\n",
      "loss: 0.910683  [830100/1106938]\n",
      "loss: 1.204708  [840100/1106938]\n",
      "loss: 1.337756  [850100/1106938]\n",
      "loss: 1.345153  [860100/1106938]\n",
      "loss: 1.255730  [870100/1106938]\n",
      "loss: 1.497693  [880100/1106938]\n",
      "loss: 1.197042  [890100/1106938]\n",
      "loss: 1.106569  [900100/1106938]\n",
      "loss: 1.453467  [910100/1106938]\n",
      "loss: 1.170204  [920100/1106938]\n",
      "loss: 1.406980  [930100/1106938]\n",
      "loss: 1.138669  [940100/1106938]\n",
      "loss: 1.130722  [950100/1106938]\n",
      "loss: 1.308846  [960100/1106938]\n",
      "loss: 1.199161  [970100/1106938]\n",
      "loss: 1.156771  [980100/1106938]\n",
      "loss: 1.174097  [990100/1106938]\n",
      "loss: 1.143171  [1000100/1106938]\n",
      "loss: 1.072736  [1010100/1106938]\n",
      "loss: 1.266204  [1020100/1106938]\n",
      "loss: 0.931315  [1030100/1106938]\n",
      "loss: 1.145180  [1040100/1106938]\n",
      "loss: 1.269260  [1050100/1106938]\n",
      "loss: 1.166504  [1060100/1106938]\n",
      "loss: 0.944652  [1070100/1106938]\n",
      "loss: 1.262216  [1080100/1106938]\n",
      "loss: 1.196623  [1090100/1106938]\n",
      "loss: 1.000887  [1100100/1106938]\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 1.049068 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.256899  [  100/1106938]\n",
      "loss: 1.316990  [10100/1106938]\n",
      "loss: 1.159124  [20100/1106938]\n",
      "loss: 1.546551  [30100/1106938]\n",
      "loss: 1.323369  [40100/1106938]\n",
      "loss: 1.234162  [50100/1106938]\n",
      "loss: 1.039358  [60100/1106938]\n",
      "loss: 0.881523  [70100/1106938]\n",
      "loss: 1.173242  [80100/1106938]\n",
      "loss: 0.996898  [90100/1106938]\n",
      "loss: 1.286410  [100100/1106938]\n",
      "loss: 1.183918  [110100/1106938]\n",
      "loss: 1.400977  [120100/1106938]\n",
      "loss: 1.324656  [130100/1106938]\n",
      "loss: 1.001711  [140100/1106938]\n",
      "loss: 1.325552  [150100/1106938]\n",
      "loss: 1.208467  [160100/1106938]\n",
      "loss: 0.820963  [170100/1106938]\n",
      "loss: 1.129802  [180100/1106938]\n",
      "loss: 0.988196  [190100/1106938]\n",
      "loss: 1.090785  [200100/1106938]\n",
      "loss: 1.245607  [210100/1106938]\n",
      "loss: 1.024979  [220100/1106938]\n",
      "loss: 1.019150  [230100/1106938]\n",
      "loss: 1.048867  [240100/1106938]\n",
      "loss: 1.326284  [250100/1106938]\n",
      "loss: 1.143195  [260100/1106938]\n",
      "loss: 1.561985  [270100/1106938]\n",
      "loss: 1.168123  [280100/1106938]\n",
      "loss: 0.976193  [290100/1106938]\n",
      "loss: 0.990807  [300100/1106938]\n",
      "loss: 1.163926  [310100/1106938]\n",
      "loss: 1.230065  [320100/1106938]\n",
      "loss: 1.117861  [330100/1106938]\n",
      "loss: 1.167820  [340100/1106938]\n",
      "loss: 1.097871  [350100/1106938]\n",
      "loss: 0.939509  [360100/1106938]\n",
      "loss: 1.121601  [370100/1106938]\n",
      "loss: 0.911384  [380100/1106938]\n",
      "loss: 1.221451  [390100/1106938]\n",
      "loss: 0.956233  [400100/1106938]\n",
      "loss: 1.091404  [410100/1106938]\n",
      "loss: 1.063203  [420100/1106938]\n",
      "loss: 1.136738  [430100/1106938]\n",
      "loss: 1.129685  [440100/1106938]\n",
      "loss: 0.970861  [450100/1106938]\n",
      "loss: 1.253544  [460100/1106938]\n",
      "loss: 1.098740  [470100/1106938]\n",
      "loss: 1.260022  [480100/1106938]\n",
      "loss: 1.140074  [490100/1106938]\n",
      "loss: 0.986661  [500100/1106938]\n",
      "loss: 1.237248  [510100/1106938]\n",
      "loss: 1.234308  [520100/1106938]\n",
      "loss: 0.896627  [530100/1106938]\n",
      "loss: 1.043771  [540100/1106938]\n",
      "loss: 0.995161  [550100/1106938]\n",
      "loss: 1.138718  [560100/1106938]\n",
      "loss: 1.091975  [570100/1106938]\n",
      "loss: 0.992419  [580100/1106938]\n",
      "loss: 1.049341  [590100/1106938]\n",
      "loss: 1.265287  [600100/1106938]\n",
      "loss: 1.099685  [610100/1106938]\n",
      "loss: 1.156249  [620100/1106938]\n",
      "loss: 1.006823  [630100/1106938]\n",
      "loss: 1.050785  [640100/1106938]\n",
      "loss: 0.792513  [650100/1106938]\n",
      "loss: 1.093485  [660100/1106938]\n",
      "loss: 1.186331  [670100/1106938]\n",
      "loss: 1.145621  [680100/1106938]\n",
      "loss: 1.099110  [690100/1106938]\n",
      "loss: 1.086498  [700100/1106938]\n",
      "loss: 1.072510  [710100/1106938]\n",
      "loss: 1.123001  [720100/1106938]\n",
      "loss: 1.126978  [730100/1106938]\n",
      "loss: 1.236718  [740100/1106938]\n",
      "loss: 0.952157  [750100/1106938]\n",
      "loss: 0.870738  [760100/1106938]\n",
      "loss: 0.857505  [770100/1106938]\n",
      "loss: 1.289079  [780100/1106938]\n",
      "loss: 1.039400  [790100/1106938]\n",
      "loss: 1.040839  [800100/1106938]\n",
      "loss: 0.898012  [810100/1106938]\n",
      "loss: 0.907082  [820100/1106938]\n",
      "loss: 1.009281  [830100/1106938]\n",
      "loss: 1.003480  [840100/1106938]\n",
      "loss: 1.079527  [850100/1106938]\n",
      "loss: 1.128013  [860100/1106938]\n",
      "loss: 1.094421  [870100/1106938]\n",
      "loss: 1.006120  [880100/1106938]\n",
      "loss: 0.906056  [890100/1106938]\n",
      "loss: 1.089901  [900100/1106938]\n",
      "loss: 1.098958  [910100/1106938]\n",
      "loss: 0.985024  [920100/1106938]\n",
      "loss: 1.016243  [930100/1106938]\n",
      "loss: 1.205721  [940100/1106938]\n",
      "loss: 1.120096  [950100/1106938]\n",
      "loss: 0.805815  [960100/1106938]\n",
      "loss: 0.972170  [970100/1106938]\n",
      "loss: 1.170217  [980100/1106938]\n",
      "loss: 1.032307  [990100/1106938]\n",
      "loss: 0.988298  [1000100/1106938]\n",
      "loss: 1.053333  [1010100/1106938]\n",
      "loss: 1.228056  [1020100/1106938]\n",
      "loss: 1.291588  [1030100/1106938]\n",
      "loss: 1.112117  [1040100/1106938]\n",
      "loss: 0.997956  [1050100/1106938]\n",
      "loss: 1.015655  [1060100/1106938]\n",
      "loss: 1.395053  [1070100/1106938]\n",
      "loss: 1.044372  [1080100/1106938]\n",
      "loss: 1.057200  [1090100/1106938]\n",
      "loss: 1.036388  [1100100/1106938]\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 0.936560 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.186016  [  100/1106938]\n",
      "loss: 1.130916  [10100/1106938]\n",
      "loss: 0.934050  [20100/1106938]\n",
      "loss: 1.263200  [30100/1106938]\n",
      "loss: 1.114646  [40100/1106938]\n",
      "loss: 0.984321  [50100/1106938]\n",
      "loss: 1.233294  [60100/1106938]\n",
      "loss: 0.885366  [70100/1106938]\n",
      "loss: 1.274589  [80100/1106938]\n",
      "loss: 1.380396  [90100/1106938]\n",
      "loss: 0.798902  [100100/1106938]\n",
      "loss: 1.072311  [110100/1106938]\n",
      "loss: 0.988013  [120100/1106938]\n",
      "loss: 1.056130  [130100/1106938]\n",
      "loss: 0.863185  [140100/1106938]\n",
      "loss: 1.004509  [150100/1106938]\n",
      "loss: 0.912989  [160100/1106938]\n",
      "loss: 0.964142  [170100/1106938]\n",
      "loss: 0.993964  [180100/1106938]\n",
      "loss: 0.946896  [190100/1106938]\n",
      "loss: 1.206932  [200100/1106938]\n",
      "loss: 0.773864  [210100/1106938]\n",
      "loss: 1.160140  [220100/1106938]\n",
      "loss: 0.999300  [230100/1106938]\n",
      "loss: 1.135385  [240100/1106938]\n",
      "loss: 0.905924  [250100/1106938]\n",
      "loss: 1.137196  [260100/1106938]\n",
      "loss: 0.898242  [270100/1106938]\n",
      "loss: 0.981827  [280100/1106938]\n",
      "loss: 1.000301  [290100/1106938]\n",
      "loss: 0.940724  [300100/1106938]\n",
      "loss: 0.879490  [310100/1106938]\n",
      "loss: 1.017290  [320100/1106938]\n",
      "loss: 0.934085  [330100/1106938]\n",
      "loss: 0.890457  [340100/1106938]\n",
      "loss: 0.928046  [350100/1106938]\n",
      "loss: 1.090090  [360100/1106938]\n",
      "loss: 1.012838  [370100/1106938]\n",
      "loss: 0.915555  [380100/1106938]\n",
      "loss: 1.062440  [390100/1106938]\n",
      "loss: 1.025537  [400100/1106938]\n",
      "loss: 1.061110  [410100/1106938]\n",
      "loss: 0.916211  [420100/1106938]\n",
      "loss: 0.785335  [430100/1106938]\n",
      "loss: 1.143882  [440100/1106938]\n",
      "loss: 1.034686  [450100/1106938]\n",
      "loss: 1.094477  [460100/1106938]\n",
      "loss: 0.932659  [470100/1106938]\n",
      "loss: 0.894672  [480100/1106938]\n",
      "loss: 0.918363  [490100/1106938]\n",
      "loss: 0.917481  [500100/1106938]\n",
      "loss: 1.181881  [510100/1106938]\n",
      "loss: 0.871420  [520100/1106938]\n",
      "loss: 1.003106  [530100/1106938]\n",
      "loss: 1.046347  [540100/1106938]\n",
      "loss: 0.862003  [550100/1106938]\n",
      "loss: 1.156874  [560100/1106938]\n",
      "loss: 1.127005  [570100/1106938]\n",
      "loss: 1.135787  [580100/1106938]\n",
      "loss: 1.107975  [590100/1106938]\n",
      "loss: 1.086501  [600100/1106938]\n",
      "loss: 0.957586  [610100/1106938]\n",
      "loss: 0.925921  [620100/1106938]\n",
      "loss: 1.294878  [630100/1106938]\n",
      "loss: 0.910478  [640100/1106938]\n",
      "loss: 1.177612  [650100/1106938]\n",
      "loss: 1.044345  [660100/1106938]\n",
      "loss: 1.089627  [670100/1106938]\n",
      "loss: 1.004174  [680100/1106938]\n",
      "loss: 0.982848  [690100/1106938]\n",
      "loss: 0.944519  [700100/1106938]\n",
      "loss: 0.919879  [710100/1106938]\n",
      "loss: 1.051978  [720100/1106938]\n",
      "loss: 1.007528  [730100/1106938]\n",
      "loss: 1.058032  [740100/1106938]\n",
      "loss: 1.219274  [750100/1106938]\n",
      "loss: 0.966241  [760100/1106938]\n",
      "loss: 0.908085  [770100/1106938]\n",
      "loss: 0.958424  [780100/1106938]\n",
      "loss: 0.810134  [790100/1106938]\n",
      "loss: 1.203512  [800100/1106938]\n",
      "loss: 0.869454  [810100/1106938]\n",
      "loss: 1.254041  [820100/1106938]\n",
      "loss: 0.902227  [830100/1106938]\n",
      "loss: 0.921718  [840100/1106938]\n",
      "loss: 1.015759  [850100/1106938]\n",
      "loss: 0.914451  [860100/1106938]\n",
      "loss: 1.040483  [870100/1106938]\n",
      "loss: 1.322005  [880100/1106938]\n",
      "loss: 1.156247  [890100/1106938]\n",
      "loss: 0.912390  [900100/1106938]\n",
      "loss: 1.278686  [910100/1106938]\n",
      "loss: 1.198768  [920100/1106938]\n",
      "loss: 1.098347  [930100/1106938]\n",
      "loss: 0.852310  [940100/1106938]\n",
      "loss: 1.165450  [950100/1106938]\n",
      "loss: 1.012962  [960100/1106938]\n",
      "loss: 1.339980  [970100/1106938]\n",
      "loss: 0.822924  [980100/1106938]\n",
      "loss: 0.788871  [990100/1106938]\n",
      "loss: 1.217206  [1000100/1106938]\n",
      "loss: 1.068096  [1010100/1106938]\n",
      "loss: 0.960541  [1020100/1106938]\n",
      "loss: 0.806653  [1030100/1106938]\n",
      "loss: 1.236846  [1040100/1106938]\n",
      "loss: 0.873847  [1050100/1106938]\n",
      "loss: 1.017314  [1060100/1106938]\n",
      "loss: 1.239665  [1070100/1106938]\n",
      "loss: 1.040400  [1080100/1106938]\n",
      "loss: 0.796741  [1090100/1106938]\n",
      "loss: 1.001475  [1100100/1106938]\n",
      "Test Error: \n",
      " Accuracy: 71.3%, Avg loss: 0.879555 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.979005  [  100/1106938]\n",
      "loss: 1.052966  [10100/1106938]\n",
      "loss: 1.160250  [20100/1106938]\n",
      "loss: 0.986340  [30100/1106938]\n",
      "loss: 0.891373  [40100/1106938]\n",
      "loss: 0.752479  [50100/1106938]\n",
      "loss: 0.994704  [60100/1106938]\n",
      "loss: 0.881272  [70100/1106938]\n",
      "loss: 1.279882  [80100/1106938]\n",
      "loss: 0.627635  [90100/1106938]\n",
      "loss: 0.946706  [100100/1106938]\n",
      "loss: 1.068777  [110100/1106938]\n",
      "loss: 0.944725  [120100/1106938]\n",
      "loss: 0.992988  [130100/1106938]\n",
      "loss: 0.871002  [140100/1106938]\n",
      "loss: 1.001476  [150100/1106938]\n",
      "loss: 0.967078  [160100/1106938]\n",
      "loss: 1.166558  [170100/1106938]\n",
      "loss: 1.016718  [180100/1106938]\n",
      "loss: 0.864753  [190100/1106938]\n",
      "loss: 1.135675  [200100/1106938]\n",
      "loss: 0.815371  [210100/1106938]\n",
      "loss: 0.988752  [220100/1106938]\n",
      "loss: 1.238019  [230100/1106938]\n",
      "loss: 1.165289  [240100/1106938]\n",
      "loss: 1.103239  [250100/1106938]\n",
      "loss: 1.146936  [260100/1106938]\n",
      "loss: 0.843777  [270100/1106938]\n",
      "loss: 0.914993  [280100/1106938]\n",
      "loss: 0.978907  [290100/1106938]\n",
      "loss: 1.224315  [300100/1106938]\n",
      "loss: 0.809661  [310100/1106938]\n",
      "loss: 1.038855  [320100/1106938]\n",
      "loss: 0.898762  [330100/1106938]\n",
      "loss: 0.996223  [340100/1106938]\n",
      "loss: 0.952536  [350100/1106938]\n",
      "loss: 0.969292  [360100/1106938]\n",
      "loss: 1.097833  [370100/1106938]\n",
      "loss: 0.925147  [380100/1106938]\n",
      "loss: 1.028000  [390100/1106938]\n",
      "loss: 1.037299  [400100/1106938]\n",
      "loss: 0.941750  [410100/1106938]\n",
      "loss: 0.917813  [420100/1106938]\n",
      "loss: 0.838088  [430100/1106938]\n",
      "loss: 0.931219  [440100/1106938]\n",
      "loss: 1.009533  [450100/1106938]\n",
      "loss: 0.908550  [460100/1106938]\n",
      "loss: 1.045703  [470100/1106938]\n",
      "loss: 1.022488  [480100/1106938]\n",
      "loss: 1.047360  [490100/1106938]\n",
      "loss: 1.301155  [500100/1106938]\n",
      "loss: 1.074484  [510100/1106938]\n",
      "loss: 0.950378  [520100/1106938]\n",
      "loss: 1.307007  [530100/1106938]\n",
      "loss: 0.813150  [540100/1106938]\n",
      "loss: 1.037649  [550100/1106938]\n",
      "loss: 0.925222  [560100/1106938]\n",
      "loss: 1.020676  [570100/1106938]\n",
      "loss: 0.930792  [580100/1106938]\n",
      "loss: 1.163680  [590100/1106938]\n",
      "loss: 0.918311  [600100/1106938]\n",
      "loss: 1.008945  [610100/1106938]\n",
      "loss: 0.857689  [620100/1106938]\n",
      "loss: 1.292761  [630100/1106938]\n",
      "loss: 1.005996  [640100/1106938]\n",
      "loss: 1.218775  [650100/1106938]\n",
      "loss: 1.047709  [660100/1106938]\n",
      "loss: 1.085781  [670100/1106938]\n",
      "loss: 0.985096  [680100/1106938]\n",
      "loss: 1.000952  [690100/1106938]\n",
      "loss: 1.021105  [700100/1106938]\n",
      "loss: 0.942858  [710100/1106938]\n",
      "loss: 0.921483  [720100/1106938]\n",
      "loss: 1.081905  [730100/1106938]\n",
      "loss: 1.008226  [740100/1106938]\n",
      "loss: 1.271581  [750100/1106938]\n",
      "loss: 1.045792  [760100/1106938]\n",
      "loss: 0.954124  [770100/1106938]\n",
      "loss: 0.988776  [780100/1106938]\n",
      "loss: 1.083494  [790100/1106938]\n",
      "loss: 1.303440  [800100/1106938]\n",
      "loss: 0.934532  [810100/1106938]\n",
      "loss: 0.909579  [820100/1106938]\n",
      "loss: 1.061661  [830100/1106938]\n",
      "loss: 0.837772  [840100/1106938]\n",
      "loss: 1.063976  [850100/1106938]\n",
      "loss: 0.816427  [860100/1106938]\n",
      "loss: 0.949991  [870100/1106938]\n",
      "loss: 0.920727  [880100/1106938]\n",
      "loss: 0.806383  [890100/1106938]\n",
      "loss: 0.880592  [900100/1106938]\n",
      "loss: 0.890593  [910100/1106938]\n",
      "loss: 0.903275  [920100/1106938]\n",
      "loss: 1.140718  [930100/1106938]\n",
      "loss: 0.903653  [940100/1106938]\n",
      "loss: 0.707070  [950100/1106938]\n",
      "loss: 0.790653  [960100/1106938]\n",
      "loss: 0.854294  [970100/1106938]\n",
      "loss: 0.671273  [980100/1106938]\n",
      "loss: 0.978106  [990100/1106938]\n",
      "loss: 0.987657  [1000100/1106938]\n",
      "loss: 0.773754  [1010100/1106938]\n",
      "loss: 0.913402  [1020100/1106938]\n",
      "loss: 0.874556  [1030100/1106938]\n",
      "loss: 1.213469  [1040100/1106938]\n",
      "loss: 0.897973  [1050100/1106938]\n",
      "loss: 1.052136  [1060100/1106938]\n",
      "loss: 0.816387  [1070100/1106938]\n",
      "loss: 1.091013  [1080100/1106938]\n",
      "loss: 0.921948  [1090100/1106938]\n",
      "loss: 0.967962  [1100100/1106938]\n",
      "Test Error: \n",
      " Accuracy: 72.5%, Avg loss: 0.835402 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.923583  [  100/1106938]\n",
      "loss: 0.815797  [10100/1106938]\n",
      "loss: 0.958925  [20100/1106938]\n",
      "loss: 0.758537  [30100/1106938]\n",
      "loss: 0.786756  [40100/1106938]\n",
      "loss: 0.905174  [50100/1106938]\n",
      "loss: 0.937602  [60100/1106938]\n",
      "loss: 1.013562  [70100/1106938]\n",
      "loss: 1.045171  [80100/1106938]\n",
      "loss: 0.854481  [90100/1106938]\n",
      "loss: 0.972825  [100100/1106938]\n",
      "loss: 0.876387  [110100/1106938]\n",
      "loss: 1.129682  [120100/1106938]\n",
      "loss: 0.824372  [130100/1106938]\n",
      "loss: 0.763907  [140100/1106938]\n",
      "loss: 0.865202  [150100/1106938]\n",
      "loss: 1.015157  [160100/1106938]\n",
      "loss: 1.052296  [170100/1106938]\n",
      "loss: 1.200869  [180100/1106938]\n",
      "loss: 0.955312  [190100/1106938]\n",
      "loss: 0.885999  [200100/1106938]\n",
      "loss: 0.898698  [210100/1106938]\n",
      "loss: 0.826098  [220100/1106938]\n",
      "loss: 0.809198  [230100/1106938]\n",
      "loss: 0.772998  [240100/1106938]\n",
      "loss: 0.884429  [250100/1106938]\n",
      "loss: 0.971653  [260100/1106938]\n",
      "loss: 0.986458  [270100/1106938]\n",
      "loss: 1.257762  [280100/1106938]\n",
      "loss: 0.852528  [290100/1106938]\n",
      "loss: 0.985676  [300100/1106938]\n",
      "loss: 0.870801  [310100/1106938]\n",
      "loss: 0.990375  [320100/1106938]\n",
      "loss: 0.951192  [330100/1106938]\n",
      "loss: 0.864315  [340100/1106938]\n",
      "loss: 0.850489  [350100/1106938]\n",
      "loss: 0.776203  [360100/1106938]\n",
      "loss: 0.996871  [370100/1106938]\n",
      "loss: 0.754581  [380100/1106938]\n",
      "loss: 0.920486  [390100/1106938]\n",
      "loss: 0.830026  [400100/1106938]\n",
      "loss: 0.734559  [410100/1106938]\n",
      "loss: 0.991022  [420100/1106938]\n",
      "loss: 0.761026  [430100/1106938]\n",
      "loss: 0.976495  [440100/1106938]\n",
      "loss: 1.029078  [450100/1106938]\n",
      "loss: 0.654500  [460100/1106938]\n",
      "loss: 0.973118  [470100/1106938]\n",
      "loss: 1.005967  [480100/1106938]\n",
      "loss: 0.879081  [490100/1106938]\n",
      "loss: 1.179157  [500100/1106938]\n",
      "loss: 1.001236  [510100/1106938]\n",
      "loss: 1.265623  [520100/1106938]\n",
      "loss: 0.974751  [530100/1106938]\n",
      "loss: 0.938118  [540100/1106938]\n",
      "loss: 0.765877  [550100/1106938]\n",
      "loss: 0.944508  [560100/1106938]\n",
      "loss: 1.083706  [570100/1106938]\n",
      "loss: 0.858520  [580100/1106938]\n",
      "loss: 0.700887  [590100/1106938]\n",
      "loss: 0.803005  [600100/1106938]\n",
      "loss: 0.894802  [610100/1106938]\n",
      "loss: 0.884062  [620100/1106938]\n",
      "loss: 1.022177  [630100/1106938]\n",
      "loss: 1.053761  [640100/1106938]\n",
      "loss: 0.969173  [650100/1106938]\n",
      "loss: 1.023169  [660100/1106938]\n",
      "loss: 0.827916  [670100/1106938]\n",
      "loss: 0.873945  [680100/1106938]\n",
      "loss: 0.989730  [690100/1106938]\n",
      "loss: 1.015974  [700100/1106938]\n",
      "loss: 0.964775  [710100/1106938]\n",
      "loss: 1.120806  [720100/1106938]\n",
      "loss: 0.977139  [730100/1106938]\n",
      "loss: 0.961890  [740100/1106938]\n",
      "loss: 0.886491  [750100/1106938]\n",
      "loss: 1.080479  [760100/1106938]\n",
      "loss: 1.035685  [770100/1106938]\n",
      "loss: 0.929127  [780100/1106938]\n",
      "loss: 1.049282  [790100/1106938]\n",
      "loss: 0.856517  [800100/1106938]\n",
      "loss: 0.642136  [810100/1106938]\n",
      "loss: 0.703457  [820100/1106938]\n",
      "loss: 0.990660  [830100/1106938]\n",
      "loss: 1.138087  [840100/1106938]\n",
      "loss: 0.832582  [850100/1106938]\n",
      "loss: 1.181419  [860100/1106938]\n",
      "loss: 0.973103  [870100/1106938]\n",
      "loss: 1.083175  [880100/1106938]\n",
      "loss: 0.738498  [890100/1106938]\n",
      "loss: 0.883212  [900100/1106938]\n",
      "loss: 0.927789  [910100/1106938]\n",
      "loss: 1.069755  [920100/1106938]\n",
      "loss: 1.088207  [930100/1106938]\n",
      "loss: 1.091888  [940100/1106938]\n",
      "loss: 1.062957  [950100/1106938]\n",
      "loss: 0.837120  [960100/1106938]\n",
      "loss: 0.819454  [970100/1106938]\n",
      "loss: 1.107885  [980100/1106938]\n",
      "loss: 0.889687  [990100/1106938]\n",
      "loss: 0.860074  [1000100/1106938]\n",
      "loss: 1.018775  [1010100/1106938]\n",
      "loss: 0.903134  [1020100/1106938]\n",
      "loss: 1.040397  [1030100/1106938]\n",
      "loss: 0.903376  [1040100/1106938]\n",
      "loss: 1.100403  [1050100/1106938]\n",
      "loss: 1.100524  [1060100/1106938]\n",
      "loss: 0.911807  [1070100/1106938]\n",
      "loss: 1.099712  [1080100/1106938]\n",
      "loss: 0.933436  [1090100/1106938]\n",
      "loss: 1.141497  [1100100/1106938]\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 0.808313 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.029211  [  100/1106938]\n",
      "loss: 1.143426  [10100/1106938]\n",
      "loss: 0.947008  [20100/1106938]\n",
      "loss: 1.036892  [30100/1106938]\n",
      "loss: 1.165879  [40100/1106938]\n",
      "loss: 0.802520  [50100/1106938]\n",
      "loss: 1.025651  [60100/1106938]\n",
      "loss: 1.119384  [70100/1106938]\n",
      "loss: 0.838126  [80100/1106938]\n",
      "loss: 0.925435  [90100/1106938]\n",
      "loss: 0.835870  [100100/1106938]\n",
      "loss: 0.848090  [110100/1106938]\n",
      "loss: 1.205747  [120100/1106938]\n",
      "loss: 0.897706  [130100/1106938]\n",
      "loss: 1.180126  [140100/1106938]\n",
      "loss: 1.224828  [150100/1106938]\n",
      "loss: 0.932896  [160100/1106938]\n",
      "loss: 0.871985  [170100/1106938]\n",
      "loss: 1.039601  [180100/1106938]\n",
      "loss: 0.822296  [190100/1106938]\n",
      "loss: 0.827253  [200100/1106938]\n",
      "loss: 0.994125  [210100/1106938]\n",
      "loss: 0.796032  [220100/1106938]\n",
      "loss: 0.946807  [230100/1106938]\n",
      "loss: 0.801136  [240100/1106938]\n",
      "loss: 0.918352  [250100/1106938]\n",
      "loss: 0.754106  [260100/1106938]\n",
      "loss: 0.795319  [270100/1106938]\n",
      "loss: 0.865205  [280100/1106938]\n",
      "loss: 0.949646  [290100/1106938]\n",
      "loss: 0.759302  [300100/1106938]\n",
      "loss: 0.981114  [310100/1106938]\n",
      "loss: 0.956554  [320100/1106938]\n",
      "loss: 0.786309  [330100/1106938]\n",
      "loss: 0.886038  [340100/1106938]\n",
      "loss: 0.978332  [350100/1106938]\n",
      "loss: 0.651878  [360100/1106938]\n",
      "loss: 0.944176  [370100/1106938]\n",
      "loss: 0.980401  [380100/1106938]\n",
      "loss: 1.087146  [390100/1106938]\n",
      "loss: 1.055607  [400100/1106938]\n",
      "loss: 0.702096  [410100/1106938]\n",
      "loss: 0.907227  [420100/1106938]\n",
      "loss: 0.629740  [430100/1106938]\n",
      "loss: 0.945603  [440100/1106938]\n",
      "loss: 0.907057  [450100/1106938]\n",
      "loss: 0.803484  [460100/1106938]\n",
      "loss: 0.777055  [470100/1106938]\n",
      "loss: 0.902928  [480100/1106938]\n",
      "loss: 0.889181  [490100/1106938]\n",
      "loss: 0.826333  [500100/1106938]\n",
      "loss: 1.015378  [510100/1106938]\n",
      "loss: 0.907461  [520100/1106938]\n",
      "loss: 0.947377  [530100/1106938]\n",
      "loss: 0.875365  [540100/1106938]\n",
      "loss: 0.930523  [550100/1106938]\n",
      "loss: 0.913996  [560100/1106938]\n",
      "loss: 0.724898  [570100/1106938]\n",
      "loss: 0.766555  [580100/1106938]\n",
      "loss: 0.775278  [590100/1106938]\n",
      "loss: 1.154676  [600100/1106938]\n",
      "loss: 1.112275  [610100/1106938]\n",
      "loss: 0.731439  [620100/1106938]\n",
      "loss: 0.876173  [630100/1106938]\n",
      "loss: 0.836055  [640100/1106938]\n",
      "loss: 1.023936  [650100/1106938]\n",
      "loss: 1.054709  [660100/1106938]\n",
      "loss: 0.991018  [670100/1106938]\n",
      "loss: 0.808690  [680100/1106938]\n",
      "loss: 0.855143  [690100/1106938]\n",
      "loss: 1.006423  [700100/1106938]\n",
      "loss: 0.667916  [710100/1106938]\n",
      "loss: 1.258793  [720100/1106938]\n",
      "loss: 1.096032  [730100/1106938]\n",
      "loss: 1.174964  [740100/1106938]\n",
      "loss: 0.866353  [750100/1106938]\n",
      "loss: 1.064041  [760100/1106938]\n",
      "loss: 0.907200  [770100/1106938]\n",
      "loss: 0.801311  [780100/1106938]\n",
      "loss: 1.025032  [790100/1106938]\n",
      "loss: 0.785088  [800100/1106938]\n",
      "loss: 0.890313  [810100/1106938]\n",
      "loss: 1.143617  [820100/1106938]\n",
      "loss: 0.710374  [830100/1106938]\n",
      "loss: 0.914747  [840100/1106938]\n",
      "loss: 1.041511  [850100/1106938]\n",
      "loss: 0.803910  [860100/1106938]\n",
      "loss: 0.875530  [870100/1106938]\n",
      "loss: 0.975143  [880100/1106938]\n",
      "loss: 1.034167  [890100/1106938]\n",
      "loss: 0.845097  [900100/1106938]\n",
      "loss: 0.960813  [910100/1106938]\n",
      "loss: 0.986435  [920100/1106938]\n",
      "loss: 0.904131  [930100/1106938]\n",
      "loss: 0.763343  [940100/1106938]\n",
      "loss: 0.906559  [950100/1106938]\n",
      "loss: 1.024872  [960100/1106938]\n",
      "loss: 1.059949  [970100/1106938]\n",
      "loss: 0.862461  [980100/1106938]\n",
      "loss: 0.920017  [990100/1106938]\n",
      "loss: 0.937762  [1000100/1106938]\n",
      "loss: 1.028930  [1010100/1106938]\n",
      "loss: 1.089337  [1020100/1106938]\n",
      "loss: 1.037954  [1030100/1106938]\n",
      "loss: 0.887802  [1040100/1106938]\n",
      "loss: 0.819181  [1050100/1106938]\n",
      "loss: 0.908066  [1060100/1106938]\n",
      "loss: 0.849422  [1070100/1106938]\n",
      "loss: 1.000500  [1080100/1106938]\n",
      "loss: 1.015814  [1090100/1106938]\n",
      "loss: 1.099048  [1100100/1106938]\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 0.804623 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.962421  [  100/1106938]\n",
      "loss: 0.862240  [10100/1106938]\n",
      "loss: 1.016590  [20100/1106938]\n",
      "loss: 0.650224  [30100/1106938]\n",
      "loss: 0.864154  [40100/1106938]\n",
      "loss: 1.179617  [50100/1106938]\n",
      "loss: 0.862415  [60100/1106938]\n",
      "loss: 0.723749  [70100/1106938]\n",
      "loss: 0.796134  [80100/1106938]\n",
      "loss: 0.888025  [90100/1106938]\n",
      "loss: 0.766593  [100100/1106938]\n",
      "loss: 0.882175  [110100/1106938]\n",
      "loss: 0.817327  [120100/1106938]\n",
      "loss: 0.928947  [130100/1106938]\n",
      "loss: 0.738165  [140100/1106938]\n",
      "loss: 1.126996  [150100/1106938]\n",
      "loss: 0.806677  [160100/1106938]\n",
      "loss: 0.809587  [170100/1106938]\n",
      "loss: 1.041206  [180100/1106938]\n",
      "loss: 0.842201  [190100/1106938]\n",
      "loss: 0.923292  [200100/1106938]\n",
      "loss: 1.110250  [210100/1106938]\n",
      "loss: 1.008785  [220100/1106938]\n",
      "loss: 0.975825  [230100/1106938]\n",
      "loss: 0.617166  [240100/1106938]\n",
      "loss: 0.913931  [250100/1106938]\n",
      "loss: 0.873166  [260100/1106938]\n",
      "loss: 1.000385  [270100/1106938]\n",
      "loss: 0.878898  [280100/1106938]\n",
      "loss: 0.911287  [290100/1106938]\n",
      "loss: 1.169457  [300100/1106938]\n",
      "loss: 0.826931  [310100/1106938]\n",
      "loss: 0.832018  [320100/1106938]\n",
      "loss: 1.056059  [330100/1106938]\n",
      "loss: 0.929979  [340100/1106938]\n",
      "loss: 0.893768  [350100/1106938]\n",
      "loss: 0.893233  [360100/1106938]\n",
      "loss: 1.097926  [370100/1106938]\n",
      "loss: 0.845014  [380100/1106938]\n",
      "loss: 0.877723  [390100/1106938]\n",
      "loss: 0.990240  [400100/1106938]\n",
      "loss: 1.003025  [410100/1106938]\n",
      "loss: 1.026006  [420100/1106938]\n",
      "loss: 0.943762  [430100/1106938]\n",
      "loss: 0.896233  [440100/1106938]\n",
      "loss: 0.853738  [450100/1106938]\n",
      "loss: 1.018525  [460100/1106938]\n",
      "loss: 0.876647  [470100/1106938]\n",
      "loss: 0.870019  [480100/1106938]\n",
      "loss: 1.041643  [490100/1106938]\n",
      "loss: 0.792006  [500100/1106938]\n",
      "loss: 0.838088  [510100/1106938]\n",
      "loss: 1.049900  [520100/1106938]\n",
      "loss: 0.945371  [530100/1106938]\n",
      "loss: 1.020979  [540100/1106938]\n",
      "loss: 0.870064  [550100/1106938]\n",
      "loss: 0.902923  [560100/1106938]\n",
      "loss: 1.105604  [570100/1106938]\n",
      "loss: 1.064045  [580100/1106938]\n",
      "loss: 0.838023  [590100/1106938]\n",
      "loss: 0.636194  [600100/1106938]\n",
      "loss: 1.120324  [610100/1106938]\n",
      "loss: 0.987415  [620100/1106938]\n",
      "loss: 0.825248  [630100/1106938]\n",
      "loss: 0.916903  [640100/1106938]\n",
      "loss: 0.983379  [650100/1106938]\n",
      "loss: 0.799807  [660100/1106938]\n",
      "loss: 0.851687  [670100/1106938]\n",
      "loss: 0.829943  [680100/1106938]\n",
      "loss: 0.765650  [690100/1106938]\n",
      "loss: 0.984728  [700100/1106938]\n",
      "loss: 0.872254  [710100/1106938]\n",
      "loss: 1.033833  [720100/1106938]\n",
      "loss: 1.014683  [730100/1106938]\n",
      "loss: 0.782186  [740100/1106938]\n",
      "loss: 0.965915  [750100/1106938]\n",
      "loss: 0.646723  [760100/1106938]\n",
      "loss: 0.949541  [770100/1106938]\n",
      "loss: 1.120920  [780100/1106938]\n",
      "loss: 0.850868  [790100/1106938]\n",
      "loss: 0.877535  [800100/1106938]\n",
      "loss: 1.115143  [810100/1106938]\n",
      "loss: 0.822528  [820100/1106938]\n",
      "loss: 0.865470  [830100/1106938]\n",
      "loss: 1.083102  [840100/1106938]\n",
      "loss: 1.014817  [850100/1106938]\n",
      "loss: 0.881984  [860100/1106938]\n",
      "loss: 0.617415  [870100/1106938]\n",
      "loss: 1.038578  [880100/1106938]\n",
      "loss: 0.808847  [890100/1106938]\n",
      "loss: 0.803170  [900100/1106938]\n",
      "loss: 0.966906  [910100/1106938]\n",
      "loss: 0.901999  [920100/1106938]\n",
      "loss: 1.012399  [930100/1106938]\n",
      "loss: 0.813041  [940100/1106938]\n",
      "loss: 1.080919  [950100/1106938]\n",
      "loss: 0.639586  [960100/1106938]\n",
      "loss: 0.668599  [970100/1106938]\n",
      "loss: 1.140922  [980100/1106938]\n",
      "loss: 1.151984  [990100/1106938]\n",
      "loss: 0.696480  [1000100/1106938]\n",
      "loss: 0.988039  [1010100/1106938]\n",
      "loss: 0.762016  [1020100/1106938]\n",
      "loss: 0.797405  [1030100/1106938]\n",
      "loss: 0.792717  [1040100/1106938]\n",
      "loss: 0.842847  [1050100/1106938]\n",
      "loss: 1.075914  [1060100/1106938]\n",
      "loss: 1.072778  [1070100/1106938]\n",
      "loss: 1.041763  [1080100/1106938]\n",
      "loss: 0.903507  [1090100/1106938]\n",
      "loss: 0.985502  [1100100/1106938]\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.777230 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.003278  [  100/1106938]\n",
      "loss: 0.960564  [10100/1106938]\n",
      "loss: 1.031586  [20100/1106938]\n",
      "loss: 1.046032  [30100/1106938]\n",
      "loss: 0.957377  [40100/1106938]\n",
      "loss: 0.923013  [50100/1106938]\n",
      "loss: 0.935761  [60100/1106938]\n",
      "loss: 0.929973  [70100/1106938]\n",
      "loss: 1.065605  [80100/1106938]\n",
      "loss: 0.870429  [90100/1106938]\n",
      "loss: 0.886463  [100100/1106938]\n",
      "loss: 0.790597  [110100/1106938]\n",
      "loss: 0.790417  [120100/1106938]\n",
      "loss: 0.809811  [130100/1106938]\n",
      "loss: 1.047655  [140100/1106938]\n",
      "loss: 1.025041  [150100/1106938]\n",
      "loss: 0.727745  [160100/1106938]\n",
      "loss: 0.809392  [170100/1106938]\n",
      "loss: 0.928150  [180100/1106938]\n",
      "loss: 1.008867  [190100/1106938]\n",
      "loss: 1.005700  [200100/1106938]\n",
      "loss: 1.110995  [210100/1106938]\n",
      "loss: 0.721538  [220100/1106938]\n",
      "loss: 0.674188  [230100/1106938]\n",
      "loss: 1.208983  [240100/1106938]\n",
      "loss: 0.886377  [250100/1106938]\n",
      "loss: 0.645400  [260100/1106938]\n",
      "loss: 0.700916  [270100/1106938]\n",
      "loss: 0.782706  [280100/1106938]\n",
      "loss: 0.754518  [290100/1106938]\n",
      "loss: 1.016216  [300100/1106938]\n",
      "loss: 0.819395  [310100/1106938]\n",
      "loss: 0.879836  [320100/1106938]\n",
      "loss: 0.957460  [330100/1106938]\n",
      "loss: 0.985758  [340100/1106938]\n",
      "loss: 0.855576  [350100/1106938]\n",
      "loss: 1.148986  [360100/1106938]\n",
      "loss: 0.850247  [370100/1106938]\n",
      "loss: 0.994500  [380100/1106938]\n",
      "loss: 0.881473  [390100/1106938]\n",
      "loss: 0.866527  [400100/1106938]\n",
      "loss: 0.733050  [410100/1106938]\n",
      "loss: 0.819139  [420100/1106938]\n",
      "loss: 0.978091  [430100/1106938]\n",
      "loss: 0.813270  [440100/1106938]\n",
      "loss: 0.822051  [450100/1106938]\n",
      "loss: 1.086204  [460100/1106938]\n",
      "loss: 0.776740  [470100/1106938]\n",
      "loss: 1.014049  [480100/1106938]\n",
      "loss: 0.977139  [490100/1106938]\n",
      "loss: 0.774060  [500100/1106938]\n",
      "loss: 0.822326  [510100/1106938]\n",
      "loss: 0.693295  [520100/1106938]\n",
      "loss: 0.827666  [530100/1106938]\n",
      "loss: 0.857795  [540100/1106938]\n",
      "loss: 0.804960  [550100/1106938]\n",
      "loss: 0.832614  [560100/1106938]\n",
      "loss: 0.926140  [570100/1106938]\n",
      "loss: 0.851009  [580100/1106938]\n",
      "loss: 1.101169  [590100/1106938]\n",
      "loss: 1.158373  [600100/1106938]\n",
      "loss: 0.830910  [610100/1106938]\n",
      "loss: 0.999550  [620100/1106938]\n",
      "loss: 0.757204  [630100/1106938]\n",
      "loss: 0.886645  [640100/1106938]\n",
      "loss: 1.020456  [650100/1106938]\n",
      "loss: 0.682328  [660100/1106938]\n",
      "loss: 0.830122  [670100/1106938]\n",
      "loss: 1.047733  [680100/1106938]\n",
      "loss: 1.185313  [690100/1106938]\n",
      "loss: 1.073494  [700100/1106938]\n",
      "loss: 0.723829  [710100/1106938]\n",
      "loss: 0.913705  [720100/1106938]\n",
      "loss: 0.896075  [730100/1106938]\n",
      "loss: 0.954017  [740100/1106938]\n",
      "loss: 0.812958  [750100/1106938]\n",
      "loss: 1.142953  [760100/1106938]\n",
      "loss: 1.094261  [770100/1106938]\n",
      "loss: 0.794145  [780100/1106938]\n",
      "loss: 0.752110  [790100/1106938]\n",
      "loss: 1.037867  [800100/1106938]\n",
      "loss: 1.018967  [810100/1106938]\n",
      "loss: 0.878859  [820100/1106938]\n",
      "loss: 0.798124  [830100/1106938]\n",
      "loss: 0.928339  [840100/1106938]\n",
      "loss: 0.863945  [850100/1106938]\n",
      "loss: 0.759371  [860100/1106938]\n",
      "loss: 0.744534  [870100/1106938]\n",
      "loss: 0.979275  [880100/1106938]\n",
      "loss: 0.836597  [890100/1106938]\n",
      "loss: 1.271274  [900100/1106938]\n",
      "loss: 0.702654  [910100/1106938]\n",
      "loss: 1.017798  [920100/1106938]\n",
      "loss: 1.094861  [930100/1106938]\n",
      "loss: 1.073742  [940100/1106938]\n",
      "loss: 0.815023  [950100/1106938]\n",
      "loss: 0.702065  [960100/1106938]\n",
      "loss: 0.841000  [970100/1106938]\n",
      "loss: 0.846244  [980100/1106938]\n",
      "loss: 0.709142  [990100/1106938]\n",
      "loss: 0.738415  [1000100/1106938]\n",
      "loss: 0.924823  [1010100/1106938]\n",
      "loss: 0.990077  [1020100/1106938]\n",
      "loss: 0.810329  [1030100/1106938]\n",
      "loss: 0.669350  [1040100/1106938]\n",
      "loss: 1.142364  [1050100/1106938]\n",
      "loss: 0.758193  [1060100/1106938]\n",
      "loss: 0.812213  [1070100/1106938]\n",
      "loss: 0.976432  [1080100/1106938]\n",
      "loss: 0.875028  [1090100/1106938]\n",
      "loss: 0.887939  [1100100/1106938]\n",
      "Test Error: \n",
      " Accuracy: 74.4%, Avg loss: 0.765958 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.799363  [  100/1106938]\n",
      "loss: 0.870383  [10100/1106938]\n",
      "loss: 1.075030  [20100/1106938]\n",
      "loss: 0.989289  [30100/1106938]\n",
      "loss: 0.829730  [40100/1106938]\n",
      "loss: 0.895346  [50100/1106938]\n",
      "loss: 0.836921  [60100/1106938]\n",
      "loss: 0.827347  [70100/1106938]\n",
      "loss: 1.022393  [80100/1106938]\n",
      "loss: 0.831096  [90100/1106938]\n",
      "loss: 1.042534  [100100/1106938]\n",
      "loss: 0.966668  [110100/1106938]\n",
      "loss: 1.061168  [120100/1106938]\n",
      "loss: 0.966150  [130100/1106938]\n",
      "loss: 0.941710  [140100/1106938]\n",
      "loss: 0.900184  [150100/1106938]\n",
      "loss: 0.853705  [160100/1106938]\n",
      "loss: 1.034356  [170100/1106938]\n",
      "loss: 1.014427  [180100/1106938]\n",
      "loss: 0.957472  [190100/1106938]\n",
      "loss: 0.941317  [200100/1106938]\n",
      "loss: 1.046963  [210100/1106938]\n",
      "loss: 0.784134  [220100/1106938]\n",
      "loss: 0.890339  [230100/1106938]\n",
      "loss: 0.772807  [240100/1106938]\n",
      "loss: 0.809333  [250100/1106938]\n",
      "loss: 1.087108  [260100/1106938]\n",
      "loss: 0.967802  [270100/1106938]\n",
      "loss: 1.010616  [280100/1106938]\n",
      "loss: 0.842843  [290100/1106938]\n",
      "loss: 0.730348  [300100/1106938]\n",
      "loss: 0.965356  [310100/1106938]\n",
      "loss: 0.957912  [320100/1106938]\n",
      "loss: 1.014165  [330100/1106938]\n",
      "loss: 1.036726  [340100/1106938]\n",
      "loss: 1.219634  [350100/1106938]\n",
      "loss: 0.666996  [360100/1106938]\n",
      "loss: 0.826035  [370100/1106938]\n",
      "loss: 1.045223  [380100/1106938]\n",
      "loss: 0.848846  [390100/1106938]\n",
      "loss: 0.909157  [400100/1106938]\n",
      "loss: 0.896365  [410100/1106938]\n",
      "loss: 0.690986  [420100/1106938]\n",
      "loss: 0.821398  [430100/1106938]\n",
      "loss: 0.917388  [440100/1106938]\n",
      "loss: 0.736808  [450100/1106938]\n",
      "loss: 0.841427  [460100/1106938]\n",
      "loss: 0.579002  [470100/1106938]\n",
      "loss: 0.745671  [480100/1106938]\n",
      "loss: 0.937082  [490100/1106938]\n",
      "loss: 1.120623  [500100/1106938]\n",
      "loss: 0.811638  [510100/1106938]\n",
      "loss: 0.839820  [520100/1106938]\n",
      "loss: 0.823770  [530100/1106938]\n",
      "loss: 0.660359  [540100/1106938]\n",
      "loss: 1.208772  [550100/1106938]\n",
      "loss: 0.676443  [560100/1106938]\n",
      "loss: 0.706026  [570100/1106938]\n",
      "loss: 0.725047  [580100/1106938]\n",
      "loss: 0.814709  [590100/1106938]\n",
      "loss: 0.999968  [600100/1106938]\n",
      "loss: 0.994865  [610100/1106938]\n",
      "loss: 0.873344  [620100/1106938]\n",
      "loss: 0.968339  [630100/1106938]\n",
      "loss: 1.099262  [640100/1106938]\n",
      "loss: 0.839127  [650100/1106938]\n",
      "loss: 0.770797  [660100/1106938]\n",
      "loss: 0.949765  [670100/1106938]\n",
      "loss: 0.630630  [680100/1106938]\n",
      "loss: 0.792568  [690100/1106938]\n",
      "loss: 0.600584  [700100/1106938]\n",
      "loss: 0.796194  [710100/1106938]\n",
      "loss: 0.736412  [720100/1106938]\n",
      "loss: 0.802514  [730100/1106938]\n",
      "loss: 0.894160  [740100/1106938]\n",
      "loss: 0.880643  [750100/1106938]\n",
      "loss: 0.931988  [760100/1106938]\n",
      "loss: 0.845759  [770100/1106938]\n",
      "loss: 1.054717  [780100/1106938]\n",
      "loss: 0.780323  [790100/1106938]\n",
      "loss: 0.840814  [800100/1106938]\n",
      "loss: 0.848922  [810100/1106938]\n",
      "loss: 0.953006  [820100/1106938]\n",
      "loss: 1.126427  [830100/1106938]\n",
      "loss: 0.771669  [840100/1106938]\n",
      "loss: 0.945325  [850100/1106938]\n",
      "loss: 0.840562  [860100/1106938]\n",
      "loss: 0.970866  [870100/1106938]\n",
      "loss: 0.748065  [880100/1106938]\n",
      "loss: 1.005448  [890100/1106938]\n",
      "loss: 0.748649  [900100/1106938]\n",
      "loss: 0.943756  [910100/1106938]\n",
      "loss: 0.795899  [920100/1106938]\n",
      "loss: 0.918373  [930100/1106938]\n",
      "loss: 0.862545  [940100/1106938]\n",
      "loss: 1.076844  [950100/1106938]\n",
      "loss: 0.930751  [960100/1106938]\n",
      "loss: 0.889665  [970100/1106938]\n",
      "loss: 0.832697  [980100/1106938]\n",
      "loss: 0.958448  [990100/1106938]\n",
      "loss: 1.016003  [1000100/1106938]\n",
      "loss: 1.268954  [1010100/1106938]\n",
      "loss: 0.814331  [1020100/1106938]\n",
      "loss: 1.010349  [1030100/1106938]\n",
      "loss: 0.987430  [1040100/1106938]\n",
      "loss: 0.701741  [1050100/1106938]\n",
      "loss: 0.928406  [1060100/1106938]\n",
      "loss: 1.021908  [1070100/1106938]\n",
      "loss: 0.804615  [1080100/1106938]\n",
      "loss: 0.854748  [1090100/1106938]\n",
      "loss: 0.696657  [1100100/1106938]\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 0.759556 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.020481  [  100/1106938]\n",
      "loss: 0.729329  [10100/1106938]\n",
      "loss: 0.816505  [20100/1106938]\n",
      "loss: 0.952051  [30100/1106938]\n",
      "loss: 0.816871  [40100/1106938]\n",
      "loss: 0.907774  [50100/1106938]\n",
      "loss: 0.693982  [60100/1106938]\n",
      "loss: 0.980285  [70100/1106938]\n",
      "loss: 0.771279  [80100/1106938]\n",
      "loss: 0.706494  [90100/1106938]\n",
      "loss: 0.956625  [100100/1106938]\n",
      "loss: 0.704604  [110100/1106938]\n",
      "loss: 0.914561  [120100/1106938]\n",
      "loss: 0.880581  [130100/1106938]\n",
      "loss: 0.930405  [140100/1106938]\n",
      "loss: 0.777276  [150100/1106938]\n",
      "loss: 0.757207  [160100/1106938]\n",
      "loss: 0.783598  [170100/1106938]\n",
      "loss: 0.810558  [180100/1106938]\n",
      "loss: 0.714820  [190100/1106938]\n",
      "loss: 1.116311  [200100/1106938]\n",
      "loss: 0.896902  [210100/1106938]\n",
      "loss: 0.893400  [220100/1106938]\n",
      "loss: 1.064422  [230100/1106938]\n",
      "loss: 0.784526  [240100/1106938]\n",
      "loss: 0.769069  [250100/1106938]\n",
      "loss: 0.916138  [260100/1106938]\n",
      "loss: 0.900407  [270100/1106938]\n",
      "loss: 1.307154  [280100/1106938]\n",
      "loss: 0.810629  [290100/1106938]\n",
      "loss: 1.124043  [300100/1106938]\n",
      "loss: 0.988422  [310100/1106938]\n",
      "loss: 1.039651  [320100/1106938]\n",
      "loss: 1.020245  [330100/1106938]\n",
      "loss: 0.631629  [340100/1106938]\n",
      "loss: 0.719673  [350100/1106938]\n",
      "loss: 0.947087  [360100/1106938]\n",
      "loss: 0.973884  [370100/1106938]\n",
      "loss: 0.737775  [380100/1106938]\n",
      "loss: 0.902761  [390100/1106938]\n",
      "loss: 0.963129  [400100/1106938]\n",
      "loss: 0.945469  [410100/1106938]\n",
      "loss: 0.685227  [420100/1106938]\n",
      "loss: 1.012850  [430100/1106938]\n",
      "loss: 0.796637  [440100/1106938]\n",
      "loss: 1.016035  [450100/1106938]\n",
      "loss: 0.984669  [460100/1106938]\n",
      "loss: 0.980235  [470100/1106938]\n",
      "loss: 0.839045  [480100/1106938]\n",
      "loss: 0.986414  [490100/1106938]\n",
      "loss: 1.090423  [500100/1106938]\n",
      "loss: 0.793311  [510100/1106938]\n",
      "loss: 0.952288  [520100/1106938]\n",
      "loss: 0.982002  [530100/1106938]\n",
      "loss: 0.899860  [540100/1106938]\n",
      "loss: 1.035258  [550100/1106938]\n",
      "loss: 0.922261  [560100/1106938]\n",
      "loss: 0.919979  [570100/1106938]\n",
      "loss: 0.974034  [580100/1106938]\n",
      "loss: 0.900479  [590100/1106938]\n",
      "loss: 0.872522  [600100/1106938]\n",
      "loss: 0.888963  [610100/1106938]\n",
      "loss: 0.972171  [620100/1106938]\n",
      "loss: 0.917701  [630100/1106938]\n",
      "loss: 0.950124  [640100/1106938]\n",
      "loss: 1.127605  [650100/1106938]\n",
      "loss: 0.775938  [660100/1106938]\n",
      "loss: 0.886792  [670100/1106938]\n",
      "loss: 1.069199  [680100/1106938]\n",
      "loss: 0.611871  [690100/1106938]\n",
      "loss: 1.003680  [700100/1106938]\n",
      "loss: 0.937145  [710100/1106938]\n",
      "loss: 0.698736  [720100/1106938]\n",
      "loss: 0.839246  [730100/1106938]\n",
      "loss: 0.802141  [740100/1106938]\n",
      "loss: 0.883935  [750100/1106938]\n",
      "loss: 1.095612  [760100/1106938]\n",
      "loss: 0.776470  [770100/1106938]\n",
      "loss: 0.960191  [780100/1106938]\n",
      "loss: 0.807128  [790100/1106938]\n",
      "loss: 0.821926  [800100/1106938]\n",
      "loss: 0.916735  [810100/1106938]\n",
      "loss: 0.945358  [820100/1106938]\n",
      "loss: 0.823721  [830100/1106938]\n",
      "loss: 0.889825  [840100/1106938]\n",
      "loss: 0.732835  [850100/1106938]\n",
      "loss: 0.831700  [860100/1106938]\n",
      "loss: 0.830553  [870100/1106938]\n",
      "loss: 0.727780  [880100/1106938]\n",
      "loss: 0.955875  [890100/1106938]\n",
      "loss: 0.789608  [900100/1106938]\n",
      "loss: 1.018983  [910100/1106938]\n",
      "loss: 0.959098  [920100/1106938]\n",
      "loss: 0.826045  [930100/1106938]\n",
      "loss: 0.831028  [940100/1106938]\n",
      "loss: 1.023974  [950100/1106938]\n",
      "loss: 0.814299  [960100/1106938]\n",
      "loss: 0.869905  [970100/1106938]\n",
      "loss: 1.042962  [980100/1106938]\n",
      "loss: 0.802323  [990100/1106938]\n",
      "loss: 1.053648  [1000100/1106938]\n",
      "loss: 0.786800  [1010100/1106938]\n",
      "loss: 0.677656  [1020100/1106938]\n",
      "loss: 0.877780  [1030100/1106938]\n",
      "loss: 0.880994  [1040100/1106938]\n",
      "loss: 0.913857  [1050100/1106938]\n",
      "loss: 0.822274  [1060100/1106938]\n",
      "loss: 0.821548  [1070100/1106938]\n",
      "loss: 0.955036  [1080100/1106938]\n",
      "loss: 0.753593  [1090100/1106938]\n",
      "loss: 0.699871  [1100100/1106938]\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 0.753833 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "min_loss = 1.0\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    dev_loss = dev(dev_dataloader, model, loss_fn)\n",
    "\n",
    "    if dev_loss < min_loss:\n",
    "        min_loss = dev_loss\n",
    "        torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([36, 36, 36,  ..., 25, 29, 25])\n"
     ]
    }
   ],
   "source": [
    "def test(dataloader, model):\n",
    "    del model\n",
    "    model = NeuralNetwork()\n",
    "    ckpt = torch.load(\"model.pth\")\n",
    "    model.load_state_dict(ckpt)\n",
    "    model.eval()\n",
    "\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for X in dataloader:\n",
    "            pred = model(X)\n",
    "            pred = pred.squeeze()\n",
    "            preds.append(pred.argmax(1))\n",
    "    \n",
    "    preds = torch.cat(preds, dim=0).long()\n",
    "    print(preds)\n",
    "    save_result(preds)\n",
    "    \n",
    "test(test_dataloader, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
