{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hw2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train data:  (1229932, 429)\n",
      "Size of train label:  (1229932,)\n",
      "Size of test data:  (451552, 429)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "def save_result(preds):\n",
    "    with open('test_result.csv', 'w', encoding='utf-8') as f:\n",
    "        f.write('Id,Class\\n')\n",
    "        for i, pred in enumerate(preds):\n",
    "            f.write('%d,%d\\n' % (i, pred))\n",
    "    \n",
    "    return\n",
    "\n",
    "data_root = './timit_11/'\n",
    "\n",
    "train = np.load(data_root + 'train_11.npy')\n",
    "train_label = np.load(data_root + 'train_label_11.npy')\n",
    "test = np.load(data_root + 'test_11.npy')\n",
    "\n",
    "print('Size of train data: ', train.shape)\n",
    "print('Size of train label: ', train_label.shape)\n",
    "print('Size of test data: ', test.shape)\n",
    "\n",
    "class TIMITDataset(Dataset): \n",
    "    def __init__(self, data, label, mode=\"train\"):\n",
    "        self.mode = mode\n",
    "        self.data = pd.DataFrame(data)\n",
    "        self.label = pd.DataFrame(label)\n",
    "\n",
    "        # 需要区分 train data 和 test data\n",
    "        if self.mode == \"train\":\n",
    "            indices = [i for i in range(len(self.data)) if i % 10 != 0]\n",
    "            self.data = self.data.iloc[indices, :]\n",
    "            self.label = self.label.iloc[indices]\n",
    "\n",
    "        elif self.mode == \"dev\":\n",
    "            indices = [i for i in range(len(self.data)) if i % 10 == 0]\n",
    "            self.data = self.data.iloc[indices, :]\n",
    "            self.label = self.label.iloc[indices]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == \"train\" or self.mode == \"dev\":\n",
    "            X = torch.tensor(self.data.iloc[idx], dtype=torch.float32)\n",
    "            y = torch.tensor(int(self.label.iloc[idx]), dtype=torch.int32)\n",
    "            return X, y\n",
    "        elif self.mode == \"test\":\n",
    "            return torch.tensor(self.data.iloc[idx], dtype=torch.float32)\n",
    "        \n",
    "train_data = TIMITDataset(train, train_label, mode=\"train\")\n",
    "dev_data = TIMITDataset(train, train_label, mode=\"dev\")\n",
    "test_data = TIMITDataset(test, None, mode=\"test\")\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=100, shuffle=True)\n",
    "dev_dataloader = DataLoader(dev_data, batch_size=100, shuffle=False)\n",
    "test_dataloader = DataLoader(test_data, batch_size=100, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method TIMITDataset.__len__ of <__main__.TIMITDataset object at 0x136667ec0>>\n",
      "[-8.15812826e-01 -3.48689795e-01  3.95477504e-01 -1.63738783e-02\n",
      "  8.60317469e-01  1.44315893e-02  3.84072900e-01  9.11684811e-01\n",
      "  2.74616122e-01  7.21527755e-01  1.62349343e+00  1.94353950e+00\n",
      "  9.64264333e-01 -7.72595452e-03 -1.81192949e-01  5.74482083e-01\n",
      " -2.26134375e-01  4.03959244e-01  3.92566890e-01  4.68575805e-01\n",
      "  9.03372943e-01  9.12166536e-02  6.29455924e-01  6.50277019e-01\n",
      " -7.98417151e-01 -5.07435977e-01 -1.93714548e-03 -4.96793658e-01\n",
      " -1.30517155e-01  1.90668270e-01  3.52899522e-01  1.43388236e+00\n",
      "  5.82235932e-01 -4.42176044e-01  3.92167211e-01  3.82801175e-01\n",
      " -5.74597836e-01 -9.66160059e-01  8.36981013e-02 -8.17853749e-01\n",
      " -3.47191900e-01  5.85275650e-01 -1.76819056e-01  6.26596749e-01\n",
      " -2.08324268e-01  1.41138196e-01  8.14121485e-01  1.20860793e-01\n",
      "  7.84978092e-01  1.85303009e+00  2.17269802e+00  1.76232314e+00\n",
      "  4.56195819e-04 -3.57095480e-01  4.66112584e-01  4.36793976e-02\n",
      "  2.44239345e-01  6.95920229e-01  5.79180181e-01  4.36306536e-01\n",
      "  2.77873218e-01  5.59000432e-01  2.24675238e-01 -8.69635940e-01\n",
      " -2.87959218e-01  2.19163373e-02 -3.04890573e-01 -3.68410289e-01\n",
      "  6.86901152e-01  1.93446845e-01  1.22113764e+00  6.87861562e-01\n",
      " -8.80873322e-01  2.27267519e-01 -3.34653795e-01 -1.30035055e+00\n",
      " -7.05772698e-01 -4.29470271e-01 -8.19877267e-01 -5.21579981e-01\n",
      "  6.54818118e-01 -2.47517794e-01  7.76656926e-01  4.93845195e-01\n",
      "  6.38528287e-01  1.34373748e+00  1.16990530e+00  1.61719799e+00\n",
      "  1.58583629e+00  7.26012826e-01  3.88572276e-01  4.44570705e-02\n",
      " -8.08226705e-01  2.70812511e-01  5.06846428e-01 -3.04862764e-02\n",
      "  9.13899004e-01  6.55444324e-01  1.99248157e-02  1.21349134e-01\n",
      "  8.46248269e-01 -6.90799773e-01 -7.40965366e-01 -2.29644522e-01\n",
      "  1.29772574e-01 -1.11926723e+00 -3.95132959e-01  1.44303000e+00\n",
      " -1.17166817e+00 -3.43524873e-01 -3.21841627e-01 -6.46609664e-01\n",
      "  2.92410403e-01  8.45067203e-02 -1.61831212e+00  6.18455052e-01\n",
      "  6.53575718e-01 -8.21439385e-01 -5.79824984e-01  7.55298138e-01\n",
      " -1.08926654e-01  1.01349998e+00  9.15398777e-01  7.18544662e-01\n",
      "  9.52252090e-01  6.39603972e-01  1.29298186e+00  1.33980155e+00\n",
      "  7.12813556e-01  8.59630764e-01  1.09658375e-01 -1.40015316e+00\n",
      "  1.04810074e-01  8.90806198e-01 -5.54478347e-01  5.59858501e-01\n",
      "  4.76976901e-01 -4.27572317e-02  4.54809010e-01  8.39707017e-01\n",
      " -1.09382379e+00 -3.02106410e-01 -2.24989846e-01  2.37577826e-01\n",
      " -1.64795864e+00 -3.36687922e-01  1.51167357e+00 -1.72091830e+00\n",
      " -1.14630747e+00 -4.85988587e-01 -5.19942343e-01 -2.66638696e-01\n",
      " -1.06537372e-01 -7.84751356e-01  1.46447480e+00  3.82050872e-01\n",
      " -8.22048783e-01 -6.27880633e-01  7.42801547e-01 -3.90241779e-02\n",
      "  1.18365037e+00  1.21759546e+00  1.00882006e+00  1.14920890e+00\n",
      "  7.69170105e-01  1.35666311e+00  1.38351047e+00  7.51709640e-01\n",
      "  8.97786200e-01  1.88909724e-01 -1.84347391e+00 -5.92376217e-02\n",
      "  1.00558782e+00 -1.07068503e+00  2.94810176e-01  4.48351741e-01\n",
      " -2.13550150e-01  1.71433255e-01  7.12499261e-01 -7.72415757e-01\n",
      " -1.92916185e-01 -5.73165357e-01  3.06467086e-01 -1.47141361e+00\n",
      " -5.06693542e-01  4.55461711e-01 -1.67400038e+00 -1.24507964e+00\n",
      " -3.37909043e-01 -9.42163840e-02 -7.19930530e-01 -3.06202561e-01\n",
      "  7.89523959e-01  1.54539812e+00 -4.80546117e-01 -8.01864445e-01\n",
      " -7.07150519e-01  6.28714204e-01  3.89305472e-01  7.86976397e-01\n",
      "  1.08158088e+00  1.06749725e+00  5.51742494e-01  8.54246080e-01\n",
      "  9.38302457e-01  7.02012718e-01  8.34094465e-01  7.78473735e-01\n",
      "  2.46743500e-01 -2.00786448e+00 -2.59372681e-01  1.01744902e+00\n",
      " -1.21515656e+00  1.41148657e-01  2.77194500e-01 -6.88229918e-01\n",
      " -7.03711510e-01  2.29979977e-01 -6.55366600e-01  5.08279741e-01\n",
      " -7.98558593e-01  2.42155805e-01 -6.39601648e-01 -3.57569396e-01\n",
      " -5.19201696e-01 -1.06722796e+00 -8.76354277e-01 -6.17329240e-01\n",
      " -6.06594145e-01 -1.51309419e+00 -7.37058938e-01  1.06613851e+00\n",
      "  1.54067111e+00 -2.66122669e-01 -7.07047403e-01 -1.49290991e+00\n",
      "  5.77214837e-01  1.01184523e+00  2.28464417e-02  4.62869883e-01\n",
      "  5.73139429e-01  6.86030209e-01  6.59196913e-01  2.08747530e+00\n",
      " -8.50434899e-02  1.52945018e+00  1.38857460e+00  3.07250828e-01\n",
      " -1.93132412e+00 -2.75336593e-01  7.48762012e-01 -1.39199424e+00\n",
      " -2.62844265e-01 -3.80406469e-01 -9.25785244e-01 -1.07260036e+00\n",
      "  1.93184674e-01 -2.34122247e-01  8.09500456e-01 -3.79938722e-01\n",
      "  1.68058470e-01  3.26376736e-01 -1.23876520e-02 -1.41517222e+00\n",
      "  4.59585823e-02 -7.19508082e-02 -8.91312122e-01 -1.21071845e-01\n",
      " -4.21448231e-01  1.49552613e-01  1.02639949e+00 -6.27906859e-01\n",
      " -7.97479987e-01 -6.25015140e-01 -1.97806239e+00  5.62663913e-01\n",
      "  1.08202255e+00 -3.15571904e-01  1.60826400e-01  7.12553084e-01\n",
      "  8.63311768e-01  8.94219697e-01  1.68949175e+00  5.86832941e-01\n",
      "  1.82698238e+00  7.64400065e-01  3.78606379e-01 -1.93906581e+00\n",
      " -2.54375786e-01  2.99605757e-01 -1.52195930e+00 -3.10934246e-01\n",
      " -4.56030488e-01 -3.86570841e-01 -1.04673004e+00 -8.25209916e-03\n",
      "  4.21362609e-01  7.00726211e-01 -6.20774686e-01  1.31356940e-01\n",
      "  9.06182945e-01  2.47731835e-01 -1.79323173e+00  8.11256707e-01\n",
      "  6.08145833e-01 -3.43659341e-01  5.73926151e-01 -4.04250085e-01\n",
      " -6.75222874e-01  1.18168211e+00 -8.53463888e-01 -8.56958423e-03\n",
      " -5.41229427e-01 -2.12455940e+00  3.38244736e-01  6.46516800e-01\n",
      " -5.64446926e-01  4.48248535e-01  1.05095375e+00  8.40595901e-01\n",
      "  3.11493397e-01  1.61268258e+00  1.73551130e+00  1.29770410e+00\n",
      " -6.98527768e-02  4.62385297e-01 -1.74325550e+00 -9.84729901e-02\n",
      " -1.19800247e-01 -1.28110456e+00 -1.72435850e-01 -5.89624405e-01\n",
      " -1.42206907e-01 -1.27849662e+00 -3.17881048e-01  5.10424614e-01\n",
      "  3.22074503e-01 -4.75895911e-01  1.56983048e-01  1.39186656e+00\n",
      "  4.53995079e-01 -1.52112699e+00  1.43880963e+00  8.51973772e-01\n",
      "  2.16113012e-02  8.90362442e-01 -5.90594001e-02 -8.75823736e-01\n",
      "  2.90102303e-01 -1.13367736e+00  6.45637989e-01 -5.26147008e-01\n",
      " -2.06020331e+00  2.99371749e-01  6.83552027e-01 -3.23276311e-01\n",
      "  7.36325026e-01  6.55510664e-01 -1.81245700e-01 -9.77546930e-01\n",
      "  9.46608126e-01  9.21990693e-01  2.14190197e+00  2.17398033e-01\n",
      "  5.16363978e-01 -1.36978412e+00  1.74136609e-01 -4.52454537e-01\n",
      " -9.24559832e-01 -8.75526816e-02 -8.43120098e-01  3.46856117e-01\n",
      " -9.60662067e-01 -4.31923479e-01  8.75602186e-01 -3.55560482e-02\n",
      " -7.17057049e-01  1.60263851e-01  1.31419182e+00  6.40638173e-01\n",
      " -3.64474535e-01  8.50858510e-01  1.52416348e-01 -3.61995876e-01\n",
      "  7.94132769e-01  8.49550247e-01 -5.72304070e-01 -4.94386554e-01\n",
      " -8.14543962e-01  2.29587063e-01 -4.44377780e-01 -2.02960443e+00\n",
      "  5.13314545e-01  4.00894850e-01 -4.04429317e-01  5.56219280e-01\n",
      " -2.18392864e-01  2.01895729e-01  9.18949246e-02  2.01063347e+00\n",
      "  1.16167784e+00  9.79497254e-01  4.92651969e-01  4.97839808e-01\n",
      " -7.56824553e-01  3.33294928e-01 -6.37785673e-01 -3.69842678e-01\n",
      "  1.70299485e-01 -6.15268171e-01  8.54063451e-01 -4.88241553e-01\n",
      " -1.02353287e+00  2.20091596e-01 -8.97782147e-01 -9.17488098e-01\n",
      "  5.88676706e-02  5.84376633e-01  5.10970116e-01  6.05664790e-01\n",
      "  5.34149706e-01 -3.20446759e-01 -1.88363567e-02  1.45047593e+00\n",
      "  1.28690445e+00 -4.73928377e-02 -1.81417489e+00 -8.92933309e-01\n",
      "  2.31251955e-01]\n"
     ]
    }
   ],
   "source": [
    "print(train_data.__len__)\n",
    "\n",
    "# 判断变量类型\n",
    "# print(train[0])\n",
    "# print(type(train[0]))\n",
    "\n",
    "# 判断变量类型\n",
    "print(test[0])\n",
    "# print(type(test[0]))\n",
    "\n",
    "\n",
    "# print(train_label[0])\n",
    "# print(type(train_label[0]))\n",
    "# print(type(train_data))\n",
    "# print(type(train_data[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=429, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=128, out_features=39, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(11*39, 1024),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 39)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "loss_record = {\"train\": [], \"dev\": []}\n",
    "pred_record = []\n",
    "target_record = []\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss_record[\"train\"].append(loss.item())\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "def dev(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "    return test_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2c/pn5qg61n16nfl0b9y726w55r0000gn/T/ipykernel_83724/1622385451.py:48: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
      "  y = torch.tensor(int(self.label.iloc[idx]), dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 3.687816  [  100/1106938]\n",
      "loss: 2.831595  [10100/1106938]\n",
      "loss: 2.321041  [20100/1106938]\n",
      "loss: 1.978735  [30100/1106938]\n",
      "loss: 1.956048  [40100/1106938]\n",
      "loss: 1.759165  [50100/1106938]\n",
      "loss: 1.538223  [60100/1106938]\n",
      "loss: 1.622286  [70100/1106938]\n",
      "loss: 1.228981  [80100/1106938]\n",
      "loss: 1.430620  [90100/1106938]\n",
      "loss: 1.584050  [100100/1106938]\n",
      "loss: 1.469232  [110100/1106938]\n",
      "loss: 1.303987  [120100/1106938]\n",
      "loss: 1.619686  [130100/1106938]\n",
      "loss: 1.571481  [140100/1106938]\n",
      "loss: 1.712508  [150100/1106938]\n",
      "loss: 1.271112  [160100/1106938]\n",
      "loss: 1.315387  [170100/1106938]\n",
      "loss: 1.397047  [180100/1106938]\n",
      "loss: 1.624878  [190100/1106938]\n",
      "loss: 1.219437  [200100/1106938]\n",
      "loss: 1.284557  [210100/1106938]\n",
      "loss: 1.443118  [220100/1106938]\n",
      "loss: 1.159790  [230100/1106938]\n",
      "loss: 1.508383  [240100/1106938]\n",
      "loss: 1.392748  [250100/1106938]\n",
      "loss: 1.149363  [260100/1106938]\n",
      "loss: 1.207536  [270100/1106938]\n",
      "loss: 1.420834  [280100/1106938]\n",
      "loss: 1.160326  [290100/1106938]\n",
      "loss: 1.180812  [300100/1106938]\n",
      "loss: 1.227500  [310100/1106938]\n",
      "loss: 1.203225  [320100/1106938]\n",
      "loss: 1.228171  [330100/1106938]\n",
      "loss: 1.331985  [340100/1106938]\n",
      "loss: 1.374177  [350100/1106938]\n",
      "loss: 1.316018  [360100/1106938]\n",
      "loss: 1.258571  [370100/1106938]\n",
      "loss: 0.985575  [380100/1106938]\n",
      "loss: 1.145275  [390100/1106938]\n",
      "loss: 1.123585  [400100/1106938]\n",
      "loss: 1.355196  [410100/1106938]\n",
      "loss: 1.146520  [420100/1106938]\n",
      "loss: 1.236911  [430100/1106938]\n",
      "loss: 1.115779  [440100/1106938]\n",
      "loss: 1.246254  [450100/1106938]\n",
      "loss: 1.405688  [460100/1106938]\n",
      "loss: 1.216122  [470100/1106938]\n",
      "loss: 1.184297  [480100/1106938]\n",
      "loss: 1.006715  [490100/1106938]\n",
      "loss: 1.173116  [500100/1106938]\n",
      "loss: 1.229900  [510100/1106938]\n",
      "loss: 1.294344  [520100/1106938]\n",
      "loss: 1.290451  [530100/1106938]\n",
      "loss: 1.185181  [540100/1106938]\n",
      "loss: 1.009118  [550100/1106938]\n",
      "loss: 1.223890  [560100/1106938]\n",
      "loss: 1.202361  [570100/1106938]\n",
      "loss: 1.226643  [580100/1106938]\n",
      "loss: 0.890857  [590100/1106938]\n",
      "loss: 0.837661  [600100/1106938]\n",
      "loss: 1.299226  [610100/1106938]\n",
      "loss: 1.000901  [620100/1106938]\n",
      "loss: 1.128879  [630100/1106938]\n",
      "loss: 1.314444  [640100/1106938]\n",
      "loss: 1.242700  [650100/1106938]\n",
      "loss: 0.930010  [660100/1106938]\n",
      "loss: 1.180557  [670100/1106938]\n",
      "loss: 1.069389  [680100/1106938]\n",
      "loss: 1.050179  [690100/1106938]\n",
      "loss: 1.185858  [700100/1106938]\n",
      "loss: 1.048915  [710100/1106938]\n",
      "loss: 1.241927  [720100/1106938]\n",
      "loss: 1.220671  [730100/1106938]\n",
      "loss: 1.057707  [740100/1106938]\n",
      "loss: 0.951382  [750100/1106938]\n",
      "loss: 0.917151  [760100/1106938]\n",
      "loss: 0.920121  [770100/1106938]\n",
      "loss: 0.992270  [780100/1106938]\n",
      "loss: 0.907421  [790100/1106938]\n",
      "loss: 0.950778  [800100/1106938]\n",
      "loss: 1.118462  [810100/1106938]\n",
      "loss: 0.866279  [820100/1106938]\n",
      "loss: 0.949217  [830100/1106938]\n",
      "loss: 0.853791  [840100/1106938]\n",
      "loss: 0.928098  [850100/1106938]\n",
      "loss: 1.257059  [860100/1106938]\n",
      "loss: 0.811771  [870100/1106938]\n",
      "loss: 1.045076  [880100/1106938]\n",
      "loss: 0.981464  [890100/1106938]\n",
      "loss: 0.961738  [900100/1106938]\n",
      "loss: 0.920918  [910100/1106938]\n",
      "loss: 0.955886  [920100/1106938]\n",
      "loss: 0.946098  [930100/1106938]\n",
      "loss: 1.008479  [940100/1106938]\n",
      "loss: 1.018003  [950100/1106938]\n",
      "loss: 1.016850  [960100/1106938]\n",
      "loss: 0.865221  [970100/1106938]\n",
      "loss: 1.208391  [980100/1106938]\n",
      "loss: 1.156040  [990100/1106938]\n",
      "loss: 0.861986  [1000100/1106938]\n",
      "loss: 1.052569  [1010100/1106938]\n",
      "loss: 0.838375  [1020100/1106938]\n",
      "loss: 0.989486  [1030100/1106938]\n",
      "loss: 0.993178  [1040100/1106938]\n",
      "loss: 1.093572  [1050100/1106938]\n",
      "loss: 1.052026  [1060100/1106938]\n",
      "loss: 0.717686  [1070100/1106938]\n",
      "loss: 0.972877  [1080100/1106938]\n",
      "loss: 1.024256  [1090100/1106938]\n",
      "loss: 0.840957  [1100100/1106938]\n",
      "Test Error: \n",
      " Accuracy: 68.2%, Avg loss: 0.986541 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.831047  [  100/1106938]\n",
      "loss: 0.677283  [10100/1106938]\n",
      "loss: 0.872780  [20100/1106938]\n",
      "loss: 1.142432  [30100/1106938]\n",
      "loss: 0.864234  [40100/1106938]\n",
      "loss: 0.993120  [50100/1106938]\n",
      "loss: 1.167870  [60100/1106938]\n",
      "loss: 0.891707  [70100/1106938]\n",
      "loss: 0.916527  [80100/1106938]\n",
      "loss: 1.017213  [90100/1106938]\n",
      "loss: 1.159388  [100100/1106938]\n",
      "loss: 0.987272  [110100/1106938]\n",
      "loss: 0.721289  [120100/1106938]\n",
      "loss: 0.952970  [130100/1106938]\n",
      "loss: 0.996111  [140100/1106938]\n",
      "loss: 0.725065  [150100/1106938]\n",
      "loss: 0.762757  [160100/1106938]\n",
      "loss: 0.846263  [170100/1106938]\n",
      "loss: 0.852558  [180100/1106938]\n",
      "loss: 1.242429  [190100/1106938]\n",
      "loss: 0.864845  [200100/1106938]\n",
      "loss: 0.827971  [210100/1106938]\n",
      "loss: 1.052612  [220100/1106938]\n",
      "loss: 1.085938  [230100/1106938]\n",
      "loss: 1.023768  [240100/1106938]\n",
      "loss: 1.066582  [250100/1106938]\n",
      "loss: 0.862835  [260100/1106938]\n",
      "loss: 0.688546  [270100/1106938]\n",
      "loss: 0.839302  [280100/1106938]\n",
      "loss: 0.895508  [290100/1106938]\n",
      "loss: 0.911120  [300100/1106938]\n",
      "loss: 0.591955  [310100/1106938]\n",
      "loss: 0.896975  [320100/1106938]\n",
      "loss: 0.917442  [330100/1106938]\n",
      "loss: 0.938841  [340100/1106938]\n",
      "loss: 1.019115  [350100/1106938]\n",
      "loss: 0.760760  [360100/1106938]\n",
      "loss: 0.844156  [370100/1106938]\n",
      "loss: 0.952011  [380100/1106938]\n",
      "loss: 0.956098  [390100/1106938]\n",
      "loss: 0.955254  [400100/1106938]\n",
      "loss: 0.850364  [410100/1106938]\n",
      "loss: 0.847382  [420100/1106938]\n",
      "loss: 0.984606  [430100/1106938]\n",
      "loss: 1.128602  [440100/1106938]\n",
      "loss: 0.850441  [450100/1106938]\n",
      "loss: 0.968914  [460100/1106938]\n",
      "loss: 0.736388  [470100/1106938]\n",
      "loss: 0.998182  [480100/1106938]\n",
      "loss: 0.888517  [490100/1106938]\n",
      "loss: 0.968787  [500100/1106938]\n",
      "loss: 0.828194  [510100/1106938]\n",
      "loss: 0.862306  [520100/1106938]\n",
      "loss: 0.601158  [530100/1106938]\n",
      "loss: 0.778306  [540100/1106938]\n",
      "loss: 0.941860  [550100/1106938]\n",
      "loss: 0.903651  [560100/1106938]\n",
      "loss: 0.807488  [570100/1106938]\n",
      "loss: 0.788074  [580100/1106938]\n",
      "loss: 0.865769  [590100/1106938]\n",
      "loss: 0.905713  [600100/1106938]\n",
      "loss: 0.871698  [610100/1106938]\n",
      "loss: 0.656322  [620100/1106938]\n",
      "loss: 0.803965  [630100/1106938]\n",
      "loss: 0.746955  [640100/1106938]\n",
      "loss: 1.079110  [650100/1106938]\n",
      "loss: 0.975254  [660100/1106938]\n",
      "loss: 0.709611  [670100/1106938]\n",
      "loss: 1.029700  [680100/1106938]\n",
      "loss: 0.913951  [690100/1106938]\n",
      "loss: 0.860609  [700100/1106938]\n",
      "loss: 1.252818  [710100/1106938]\n",
      "loss: 1.004222  [720100/1106938]\n",
      "loss: 0.951709  [730100/1106938]\n",
      "loss: 0.976680  [740100/1106938]\n",
      "loss: 0.760488  [750100/1106938]\n",
      "loss: 0.966072  [760100/1106938]\n",
      "loss: 0.786403  [770100/1106938]\n",
      "loss: 0.894994  [780100/1106938]\n",
      "loss: 1.009174  [790100/1106938]\n",
      "loss: 1.023734  [800100/1106938]\n",
      "loss: 0.815538  [810100/1106938]\n",
      "loss: 1.196732  [820100/1106938]\n",
      "loss: 0.919746  [830100/1106938]\n",
      "loss: 0.811131  [840100/1106938]\n",
      "loss: 0.892851  [850100/1106938]\n",
      "loss: 0.668259  [860100/1106938]\n",
      "loss: 0.795127  [870100/1106938]\n",
      "loss: 0.926335  [880100/1106938]\n",
      "loss: 0.782765  [890100/1106938]\n",
      "loss: 0.886556  [900100/1106938]\n",
      "loss: 0.820654  [910100/1106938]\n",
      "loss: 0.797946  [920100/1106938]\n",
      "loss: 0.849754  [930100/1106938]\n",
      "loss: 0.834738  [940100/1106938]\n",
      "loss: 0.727536  [950100/1106938]\n",
      "loss: 0.649528  [960100/1106938]\n",
      "loss: 0.920256  [970100/1106938]\n",
      "loss: 0.983324  [980100/1106938]\n",
      "loss: 0.871925  [990100/1106938]\n",
      "loss: 0.951536  [1000100/1106938]\n",
      "loss: 0.922347  [1010100/1106938]\n",
      "loss: 0.761858  [1020100/1106938]\n",
      "loss: 0.996203  [1030100/1106938]\n",
      "loss: 0.758844  [1040100/1106938]\n",
      "loss: 0.919616  [1050100/1106938]\n",
      "loss: 0.996902  [1060100/1106938]\n",
      "loss: 0.677527  [1070100/1106938]\n",
      "loss: 0.773581  [1080100/1106938]\n",
      "loss: 1.038143  [1090100/1106938]\n",
      "loss: 0.951396  [1100100/1106938]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.892378 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.742176  [  100/1106938]\n",
      "loss: 0.701000  [10100/1106938]\n",
      "loss: 0.748862  [20100/1106938]\n",
      "loss: 0.923573  [30100/1106938]\n",
      "loss: 0.763198  [40100/1106938]\n",
      "loss: 0.908034  [50100/1106938]\n",
      "loss: 0.682277  [60100/1106938]\n",
      "loss: 0.645266  [70100/1106938]\n",
      "loss: 1.016244  [80100/1106938]\n",
      "loss: 0.926015  [90100/1106938]\n",
      "loss: 0.791142  [100100/1106938]\n",
      "loss: 0.608950  [110100/1106938]\n",
      "loss: 0.788310  [120100/1106938]\n",
      "loss: 0.741384  [130100/1106938]\n",
      "loss: 0.884486  [140100/1106938]\n",
      "loss: 0.712926  [150100/1106938]\n",
      "loss: 0.703792  [160100/1106938]\n",
      "loss: 0.590575  [170100/1106938]\n",
      "loss: 0.816797  [180100/1106938]\n",
      "loss: 0.814500  [190100/1106938]\n",
      "loss: 0.828647  [200100/1106938]\n",
      "loss: 0.886547  [210100/1106938]\n",
      "loss: 0.661856  [220100/1106938]\n",
      "loss: 0.909062  [230100/1106938]\n",
      "loss: 0.915604  [240100/1106938]\n",
      "loss: 0.683954  [250100/1106938]\n",
      "loss: 0.692038  [260100/1106938]\n",
      "loss: 0.860773  [270100/1106938]\n",
      "loss: 0.772239  [280100/1106938]\n",
      "loss: 0.791975  [290100/1106938]\n",
      "loss: 0.776719  [300100/1106938]\n",
      "loss: 0.657977  [310100/1106938]\n",
      "loss: 0.755863  [320100/1106938]\n",
      "loss: 1.003253  [330100/1106938]\n",
      "loss: 0.984003  [340100/1106938]\n",
      "loss: 0.861234  [350100/1106938]\n",
      "loss: 0.851817  [360100/1106938]\n",
      "loss: 0.780589  [370100/1106938]\n",
      "loss: 1.137261  [380100/1106938]\n",
      "loss: 0.919173  [390100/1106938]\n",
      "loss: 0.706686  [400100/1106938]\n",
      "loss: 0.864518  [410100/1106938]\n",
      "loss: 0.793078  [420100/1106938]\n",
      "loss: 0.967307  [430100/1106938]\n",
      "loss: 0.796255  [440100/1106938]\n",
      "loss: 0.670107  [450100/1106938]\n",
      "loss: 0.741435  [460100/1106938]\n",
      "loss: 0.862675  [470100/1106938]\n",
      "loss: 0.664544  [480100/1106938]\n",
      "loss: 0.713485  [490100/1106938]\n",
      "loss: 0.805106  [500100/1106938]\n",
      "loss: 0.966142  [510100/1106938]\n",
      "loss: 0.903025  [520100/1106938]\n",
      "loss: 0.655071  [530100/1106938]\n",
      "loss: 0.805092  [540100/1106938]\n",
      "loss: 0.895018  [550100/1106938]\n",
      "loss: 0.958508  [560100/1106938]\n",
      "loss: 0.761236  [570100/1106938]\n",
      "loss: 0.838148  [580100/1106938]\n",
      "loss: 0.864831  [590100/1106938]\n",
      "loss: 0.740472  [600100/1106938]\n",
      "loss: 0.852655  [610100/1106938]\n",
      "loss: 0.866908  [620100/1106938]\n",
      "loss: 1.086936  [630100/1106938]\n",
      "loss: 0.755368  [640100/1106938]\n",
      "loss: 0.824960  [650100/1106938]\n",
      "loss: 0.808085  [660100/1106938]\n",
      "loss: 0.914666  [670100/1106938]\n",
      "loss: 0.703733  [680100/1106938]\n",
      "loss: 0.692909  [690100/1106938]\n",
      "loss: 0.870961  [700100/1106938]\n",
      "loss: 0.972991  [710100/1106938]\n",
      "loss: 0.766083  [720100/1106938]\n",
      "loss: 0.711180  [730100/1106938]\n",
      "loss: 0.826733  [740100/1106938]\n",
      "loss: 0.829097  [750100/1106938]\n",
      "loss: 0.822146  [760100/1106938]\n",
      "loss: 0.782007  [770100/1106938]\n",
      "loss: 0.731437  [780100/1106938]\n",
      "loss: 0.921300  [790100/1106938]\n",
      "loss: 0.580102  [800100/1106938]\n",
      "loss: 0.737690  [810100/1106938]\n",
      "loss: 0.605496  [820100/1106938]\n",
      "loss: 0.830042  [830100/1106938]\n",
      "loss: 0.683770  [840100/1106938]\n",
      "loss: 0.967655  [850100/1106938]\n",
      "loss: 0.817456  [860100/1106938]\n",
      "loss: 0.900726  [870100/1106938]\n",
      "loss: 0.935034  [880100/1106938]\n",
      "loss: 0.724063  [890100/1106938]\n",
      "loss: 0.907768  [900100/1106938]\n",
      "loss: 0.950572  [910100/1106938]\n",
      "loss: 0.768014  [920100/1106938]\n",
      "loss: 0.671096  [930100/1106938]\n",
      "loss: 0.850017  [940100/1106938]\n",
      "loss: 0.953410  [950100/1106938]\n",
      "loss: 0.703308  [960100/1106938]\n",
      "loss: 0.645952  [970100/1106938]\n",
      "loss: 0.671197  [980100/1106938]\n",
      "loss: 0.779375  [990100/1106938]\n",
      "loss: 0.815750  [1000100/1106938]\n",
      "loss: 0.703234  [1010100/1106938]\n",
      "loss: 0.620473  [1020100/1106938]\n",
      "loss: 1.023710  [1030100/1106938]\n",
      "loss: 0.766212  [1040100/1106938]\n",
      "loss: 0.971414  [1050100/1106938]\n",
      "loss: 1.050967  [1060100/1106938]\n",
      "loss: 0.729035  [1070100/1106938]\n",
      "loss: 0.697797  [1080100/1106938]\n",
      "loss: 0.785207  [1090100/1106938]\n",
      "loss: 0.897373  [1100100/1106938]\n",
      "Test Error: \n",
      " Accuracy: 72.9%, Avg loss: 0.813838 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.738477  [  100/1106938]\n",
      "loss: 0.924224  [10100/1106938]\n",
      "loss: 0.790940  [20100/1106938]\n",
      "loss: 0.787438  [30100/1106938]\n",
      "loss: 0.656011  [40100/1106938]\n",
      "loss: 0.740348  [50100/1106938]\n",
      "loss: 0.677709  [60100/1106938]\n",
      "loss: 0.595807  [70100/1106938]\n",
      "loss: 0.920415  [80100/1106938]\n",
      "loss: 0.645997  [90100/1106938]\n",
      "loss: 0.906660  [100100/1106938]\n",
      "loss: 0.633084  [110100/1106938]\n",
      "loss: 0.883721  [120100/1106938]\n",
      "loss: 0.696508  [130100/1106938]\n",
      "loss: 0.851875  [140100/1106938]\n",
      "loss: 0.831582  [150100/1106938]\n",
      "loss: 0.619816  [160100/1106938]\n",
      "loss: 0.617146  [170100/1106938]\n",
      "loss: 0.646806  [180100/1106938]\n",
      "loss: 0.744645  [190100/1106938]\n",
      "loss: 0.757436  [200100/1106938]\n",
      "loss: 0.704467  [210100/1106938]\n",
      "loss: 0.731235  [220100/1106938]\n",
      "loss: 0.584831  [230100/1106938]\n",
      "loss: 0.852387  [240100/1106938]\n",
      "loss: 0.614428  [250100/1106938]\n",
      "loss: 0.776576  [260100/1106938]\n",
      "loss: 0.839488  [270100/1106938]\n",
      "loss: 0.883530  [280100/1106938]\n",
      "loss: 0.863991  [290100/1106938]\n",
      "loss: 0.592472  [300100/1106938]\n",
      "loss: 0.705094  [310100/1106938]\n",
      "loss: 0.790687  [320100/1106938]\n",
      "loss: 0.665662  [330100/1106938]\n",
      "loss: 0.798583  [340100/1106938]\n",
      "loss: 0.843763  [350100/1106938]\n",
      "loss: 0.718113  [360100/1106938]\n",
      "loss: 0.753706  [370100/1106938]\n",
      "loss: 0.814700  [380100/1106938]\n",
      "loss: 0.769532  [390100/1106938]\n",
      "loss: 0.770726  [400100/1106938]\n",
      "loss: 0.717315  [410100/1106938]\n",
      "loss: 0.619900  [420100/1106938]\n",
      "loss: 0.676258  [430100/1106938]\n",
      "loss: 0.777413  [440100/1106938]\n",
      "loss: 0.812034  [450100/1106938]\n",
      "loss: 0.717164  [460100/1106938]\n",
      "loss: 0.732670  [470100/1106938]\n",
      "loss: 0.556119  [480100/1106938]\n",
      "loss: 0.519266  [490100/1106938]\n",
      "loss: 0.708622  [500100/1106938]\n",
      "loss: 0.658248  [510100/1106938]\n",
      "loss: 0.896191  [520100/1106938]\n",
      "loss: 0.821046  [530100/1106938]\n",
      "loss: 0.822848  [540100/1106938]\n",
      "loss: 0.640889  [550100/1106938]\n",
      "loss: 0.890479  [560100/1106938]\n",
      "loss: 0.496974  [570100/1106938]\n",
      "loss: 0.670110  [580100/1106938]\n",
      "loss: 0.824547  [590100/1106938]\n",
      "loss: 0.709554  [600100/1106938]\n",
      "loss: 0.676589  [610100/1106938]\n",
      "loss: 0.598430  [620100/1106938]\n",
      "loss: 0.778408  [630100/1106938]\n",
      "loss: 0.622935  [640100/1106938]\n",
      "loss: 0.884165  [650100/1106938]\n",
      "loss: 0.894994  [660100/1106938]\n",
      "loss: 0.650387  [670100/1106938]\n",
      "loss: 0.743750  [680100/1106938]\n",
      "loss: 0.700617  [690100/1106938]\n",
      "loss: 0.892169  [700100/1106938]\n",
      "loss: 0.783997  [710100/1106938]\n",
      "loss: 0.788295  [720100/1106938]\n",
      "loss: 0.706719  [730100/1106938]\n",
      "loss: 0.769207  [740100/1106938]\n",
      "loss: 0.643614  [750100/1106938]\n",
      "loss: 0.673207  [760100/1106938]\n",
      "loss: 0.465080  [770100/1106938]\n",
      "loss: 0.682184  [780100/1106938]\n",
      "loss: 0.479028  [790100/1106938]\n",
      "loss: 0.767390  [800100/1106938]\n",
      "loss: 0.540592  [810100/1106938]\n",
      "loss: 0.580196  [820100/1106938]\n",
      "loss: 0.657656  [830100/1106938]\n",
      "loss: 0.641162  [840100/1106938]\n",
      "loss: 0.661376  [850100/1106938]\n",
      "loss: 0.733801  [860100/1106938]\n",
      "loss: 0.650162  [870100/1106938]\n",
      "loss: 0.717927  [880100/1106938]\n",
      "loss: 0.452956  [890100/1106938]\n",
      "loss: 0.812866  [900100/1106938]\n",
      "loss: 0.822032  [910100/1106938]\n",
      "loss: 0.616937  [920100/1106938]\n",
      "loss: 0.623180  [930100/1106938]\n",
      "loss: 0.864267  [940100/1106938]\n",
      "loss: 0.621529  [950100/1106938]\n",
      "loss: 0.646999  [960100/1106938]\n",
      "loss: 0.565357  [970100/1106938]\n",
      "loss: 0.703138  [980100/1106938]\n",
      "loss: 0.787580  [990100/1106938]\n",
      "loss: 0.904958  [1000100/1106938]\n",
      "loss: 0.808344  [1010100/1106938]\n",
      "loss: 0.861818  [1020100/1106938]\n",
      "loss: 0.385767  [1030100/1106938]\n",
      "loss: 0.840196  [1040100/1106938]\n",
      "loss: 0.653262  [1050100/1106938]\n",
      "loss: 0.908757  [1060100/1106938]\n",
      "loss: 0.546032  [1070100/1106938]\n",
      "loss: 0.965297  [1080100/1106938]\n",
      "loss: 0.819292  [1090100/1106938]\n",
      "loss: 0.773898  [1100100/1106938]\n",
      "Test Error: \n",
      " Accuracy: 74.3%, Avg loss: 0.773359 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.707124  [  100/1106938]\n",
      "loss: 0.691644  [10100/1106938]\n",
      "loss: 0.767641  [20100/1106938]\n",
      "loss: 0.658878  [30100/1106938]\n",
      "loss: 0.642201  [40100/1106938]\n",
      "loss: 0.654651  [50100/1106938]\n",
      "loss: 0.583577  [60100/1106938]\n",
      "loss: 0.688989  [70100/1106938]\n",
      "loss: 0.628475  [80100/1106938]\n",
      "loss: 0.651678  [90100/1106938]\n",
      "loss: 0.869050  [100100/1106938]\n",
      "loss: 0.838032  [110100/1106938]\n",
      "loss: 0.696837  [120100/1106938]\n",
      "loss: 0.528457  [130100/1106938]\n",
      "loss: 0.719975  [140100/1106938]\n",
      "loss: 0.757680  [150100/1106938]\n",
      "loss: 0.699344  [160100/1106938]\n",
      "loss: 0.680412  [170100/1106938]\n",
      "loss: 0.750779  [180100/1106938]\n",
      "loss: 0.572473  [190100/1106938]\n",
      "loss: 0.531004  [200100/1106938]\n",
      "loss: 0.691508  [210100/1106938]\n",
      "loss: 0.517640  [220100/1106938]\n",
      "loss: 0.681237  [230100/1106938]\n",
      "loss: 0.633375  [240100/1106938]\n",
      "loss: 0.716420  [250100/1106938]\n",
      "loss: 0.587297  [260100/1106938]\n",
      "loss: 0.582281  [270100/1106938]\n",
      "loss: 0.951470  [280100/1106938]\n",
      "loss: 0.744073  [290100/1106938]\n",
      "loss: 0.887906  [300100/1106938]\n",
      "loss: 0.506182  [310100/1106938]\n",
      "loss: 0.647804  [320100/1106938]\n",
      "loss: 0.604931  [330100/1106938]\n",
      "loss: 0.633959  [340100/1106938]\n",
      "loss: 0.517958  [350100/1106938]\n",
      "loss: 0.564663  [360100/1106938]\n",
      "loss: 0.530174  [370100/1106938]\n",
      "loss: 0.511619  [380100/1106938]\n",
      "loss: 0.879389  [390100/1106938]\n",
      "loss: 0.592145  [400100/1106938]\n",
      "loss: 0.505508  [410100/1106938]\n",
      "loss: 0.874935  [420100/1106938]\n",
      "loss: 0.621128  [430100/1106938]\n",
      "loss: 0.644743  [440100/1106938]\n",
      "loss: 0.565379  [450100/1106938]\n",
      "loss: 0.727352  [460100/1106938]\n",
      "loss: 0.642015  [470100/1106938]\n",
      "loss: 0.684368  [480100/1106938]\n",
      "loss: 0.715182  [490100/1106938]\n",
      "loss: 0.653559  [500100/1106938]\n",
      "loss: 0.617013  [510100/1106938]\n",
      "loss: 0.521701  [520100/1106938]\n",
      "loss: 0.560730  [530100/1106938]\n",
      "loss: 0.674242  [540100/1106938]\n",
      "loss: 0.698722  [550100/1106938]\n",
      "loss: 0.746371  [560100/1106938]\n",
      "loss: 0.717372  [570100/1106938]\n",
      "loss: 0.819501  [580100/1106938]\n",
      "loss: 0.703936  [590100/1106938]\n",
      "loss: 0.713339  [600100/1106938]\n",
      "loss: 0.677858  [610100/1106938]\n",
      "loss: 0.540440  [620100/1106938]\n",
      "loss: 0.757740  [630100/1106938]\n",
      "loss: 0.689072  [640100/1106938]\n",
      "loss: 0.511234  [650100/1106938]\n",
      "loss: 0.833296  [660100/1106938]\n",
      "loss: 0.598248  [670100/1106938]\n",
      "loss: 0.544679  [680100/1106938]\n",
      "loss: 0.632630  [690100/1106938]\n",
      "loss: 0.694794  [700100/1106938]\n",
      "loss: 0.807241  [710100/1106938]\n",
      "loss: 0.499654  [720100/1106938]\n",
      "loss: 0.689193  [730100/1106938]\n",
      "loss: 0.875821  [740100/1106938]\n",
      "loss: 0.514111  [750100/1106938]\n",
      "loss: 0.605211  [760100/1106938]\n",
      "loss: 0.454029  [770100/1106938]\n",
      "loss: 0.674430  [780100/1106938]\n",
      "loss: 0.711720  [790100/1106938]\n",
      "loss: 0.807848  [800100/1106938]\n",
      "loss: 0.748269  [810100/1106938]\n",
      "loss: 0.886533  [820100/1106938]\n",
      "loss: 0.705365  [830100/1106938]\n",
      "loss: 0.624786  [840100/1106938]\n",
      "loss: 0.642057  [850100/1106938]\n",
      "loss: 0.632248  [860100/1106938]\n",
      "loss: 0.596193  [870100/1106938]\n",
      "loss: 0.537893  [880100/1106938]\n",
      "loss: 0.715217  [890100/1106938]\n",
      "loss: 0.519823  [900100/1106938]\n",
      "loss: 0.747030  [910100/1106938]\n",
      "loss: 0.619716  [920100/1106938]\n",
      "loss: 0.754316  [930100/1106938]\n",
      "loss: 0.655175  [940100/1106938]\n",
      "loss: 0.588454  [950100/1106938]\n",
      "loss: 0.510618  [960100/1106938]\n",
      "loss: 0.676951  [970100/1106938]\n",
      "loss: 0.764917  [980100/1106938]\n",
      "loss: 0.618133  [990100/1106938]\n",
      "loss: 0.881695  [1000100/1106938]\n",
      "loss: 0.561566  [1010100/1106938]\n",
      "loss: 0.699896  [1020100/1106938]\n",
      "loss: 0.637023  [1030100/1106938]\n",
      "loss: 0.683217  [1040100/1106938]\n",
      "loss: 0.487127  [1050100/1106938]\n",
      "loss: 0.551099  [1060100/1106938]\n",
      "loss: 0.596972  [1070100/1106938]\n",
      "loss: 0.655201  [1080100/1106938]\n",
      "loss: 0.731908  [1090100/1106938]\n",
      "loss: 0.677177  [1100100/1106938]\n",
      "Test Error: \n",
      " Accuracy: 75.5%, Avg loss: 0.737925 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.682762  [  100/1106938]\n",
      "loss: 0.555549  [10100/1106938]\n",
      "loss: 0.582078  [20100/1106938]\n",
      "loss: 0.553829  [30100/1106938]\n",
      "loss: 0.463192  [40100/1106938]\n",
      "loss: 0.532713  [50100/1106938]\n",
      "loss: 0.783764  [60100/1106938]\n",
      "loss: 0.516637  [70100/1106938]\n",
      "loss: 0.626275  [80100/1106938]\n",
      "loss: 0.745694  [90100/1106938]\n",
      "loss: 0.580938  [100100/1106938]\n",
      "loss: 0.740156  [110100/1106938]\n",
      "loss: 0.636459  [120100/1106938]\n",
      "loss: 0.658267  [130100/1106938]\n",
      "loss: 0.478953  [140100/1106938]\n",
      "loss: 0.626973  [150100/1106938]\n",
      "loss: 0.758834  [160100/1106938]\n",
      "loss: 0.619029  [170100/1106938]\n",
      "loss: 0.547507  [180100/1106938]\n",
      "loss: 0.721347  [190100/1106938]\n",
      "loss: 0.556170  [200100/1106938]\n",
      "loss: 0.655566  [210100/1106938]\n",
      "loss: 0.835507  [220100/1106938]\n",
      "loss: 0.713560  [230100/1106938]\n",
      "loss: 0.630153  [240100/1106938]\n",
      "loss: 0.700659  [250100/1106938]\n",
      "loss: 0.542746  [260100/1106938]\n",
      "loss: 0.616153  [270100/1106938]\n",
      "loss: 0.561006  [280100/1106938]\n",
      "loss: 0.613862  [290100/1106938]\n",
      "loss: 0.467713  [300100/1106938]\n",
      "loss: 0.851166  [310100/1106938]\n",
      "loss: 0.668244  [320100/1106938]\n",
      "loss: 0.614748  [330100/1106938]\n",
      "loss: 0.449664  [340100/1106938]\n",
      "loss: 0.620153  [350100/1106938]\n",
      "loss: 0.627872  [360100/1106938]\n",
      "loss: 0.797753  [370100/1106938]\n",
      "loss: 0.540405  [380100/1106938]\n",
      "loss: 0.658382  [390100/1106938]\n",
      "loss: 0.654263  [400100/1106938]\n",
      "loss: 0.561236  [410100/1106938]\n",
      "loss: 0.847279  [420100/1106938]\n",
      "loss: 0.679556  [430100/1106938]\n",
      "loss: 0.626102  [440100/1106938]\n",
      "loss: 0.656094  [450100/1106938]\n",
      "loss: 0.594292  [460100/1106938]\n",
      "loss: 0.770960  [470100/1106938]\n",
      "loss: 0.497890  [480100/1106938]\n",
      "loss: 0.529206  [490100/1106938]\n",
      "loss: 0.432657  [500100/1106938]\n",
      "loss: 0.394735  [510100/1106938]\n",
      "loss: 0.591593  [520100/1106938]\n",
      "loss: 0.586916  [530100/1106938]\n",
      "loss: 0.572529  [540100/1106938]\n",
      "loss: 0.749294  [550100/1106938]\n",
      "loss: 0.702592  [560100/1106938]\n",
      "loss: 0.795695  [570100/1106938]\n",
      "loss: 0.711197  [580100/1106938]\n",
      "loss: 0.722928  [590100/1106938]\n",
      "loss: 0.600918  [600100/1106938]\n",
      "loss: 0.577420  [610100/1106938]\n",
      "loss: 0.502162  [620100/1106938]\n",
      "loss: 0.679326  [630100/1106938]\n",
      "loss: 0.674279  [640100/1106938]\n",
      "loss: 0.418695  [650100/1106938]\n",
      "loss: 0.570284  [660100/1106938]\n",
      "loss: 0.693184  [670100/1106938]\n",
      "loss: 0.764093  [680100/1106938]\n",
      "loss: 0.649587  [690100/1106938]\n",
      "loss: 0.632034  [700100/1106938]\n",
      "loss: 0.632977  [710100/1106938]\n",
      "loss: 0.602084  [720100/1106938]\n",
      "loss: 0.697219  [730100/1106938]\n",
      "loss: 0.680110  [740100/1106938]\n",
      "loss: 0.607253  [750100/1106938]\n",
      "loss: 0.722369  [760100/1106938]\n",
      "loss: 0.477354  [770100/1106938]\n",
      "loss: 0.528372  [780100/1106938]\n",
      "loss: 0.488729  [790100/1106938]\n",
      "loss: 0.721983  [800100/1106938]\n",
      "loss: 0.608423  [810100/1106938]\n",
      "loss: 0.812144  [820100/1106938]\n",
      "loss: 0.557705  [830100/1106938]\n",
      "loss: 0.563634  [840100/1106938]\n",
      "loss: 0.605932  [850100/1106938]\n",
      "loss: 0.689664  [860100/1106938]\n",
      "loss: 0.543850  [870100/1106938]\n",
      "loss: 0.737686  [880100/1106938]\n",
      "loss: 0.591321  [890100/1106938]\n",
      "loss: 0.769465  [900100/1106938]\n",
      "loss: 0.691223  [910100/1106938]\n",
      "loss: 0.540104  [920100/1106938]\n",
      "loss: 0.547478  [930100/1106938]\n",
      "loss: 0.817557  [940100/1106938]\n",
      "loss: 0.718295  [950100/1106938]\n",
      "loss: 0.689111  [960100/1106938]\n",
      "loss: 0.627417  [970100/1106938]\n",
      "loss: 0.724584  [980100/1106938]\n",
      "loss: 0.658609  [990100/1106938]\n",
      "loss: 0.882316  [1000100/1106938]\n",
      "loss: 0.652657  [1010100/1106938]\n",
      "loss: 0.511215  [1020100/1106938]\n",
      "loss: 0.842615  [1030100/1106938]\n",
      "loss: 0.507142  [1040100/1106938]\n",
      "loss: 0.827339  [1050100/1106938]\n",
      "loss: 0.589456  [1060100/1106938]\n",
      "loss: 0.855275  [1070100/1106938]\n",
      "loss: 0.700175  [1080100/1106938]\n",
      "loss: 0.577486  [1090100/1106938]\n",
      "loss: 0.680367  [1100100/1106938]\n",
      "Test Error: \n",
      " Accuracy: 75.5%, Avg loss: 0.732929 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.526619  [  100/1106938]\n",
      "loss: 0.568095  [10100/1106938]\n",
      "loss: 0.598974  [20100/1106938]\n",
      "loss: 0.367445  [30100/1106938]\n",
      "loss: 0.631220  [40100/1106938]\n",
      "loss: 0.618317  [50100/1106938]\n",
      "loss: 0.555577  [60100/1106938]\n",
      "loss: 0.445381  [70100/1106938]\n",
      "loss: 0.567988  [80100/1106938]\n",
      "loss: 0.545140  [90100/1106938]\n",
      "loss: 0.743636  [100100/1106938]\n",
      "loss: 0.574795  [110100/1106938]\n",
      "loss: 0.680088  [120100/1106938]\n",
      "loss: 0.572850  [130100/1106938]\n",
      "loss: 0.476184  [140100/1106938]\n",
      "loss: 0.473243  [150100/1106938]\n",
      "loss: 0.641530  [160100/1106938]\n",
      "loss: 0.698996  [170100/1106938]\n",
      "loss: 0.733813  [180100/1106938]\n",
      "loss: 0.564884  [190100/1106938]\n",
      "loss: 0.599526  [200100/1106938]\n",
      "loss: 0.663800  [210100/1106938]\n",
      "loss: 0.418029  [220100/1106938]\n",
      "loss: 0.666530  [230100/1106938]\n",
      "loss: 0.657412  [240100/1106938]\n",
      "loss: 0.412910  [250100/1106938]\n",
      "loss: 0.864235  [260100/1106938]\n",
      "loss: 0.843029  [270100/1106938]\n",
      "loss: 0.527886  [280100/1106938]\n",
      "loss: 0.439498  [290100/1106938]\n",
      "loss: 0.427346  [300100/1106938]\n",
      "loss: 0.696437  [310100/1106938]\n",
      "loss: 0.628192  [320100/1106938]\n",
      "loss: 0.630902  [330100/1106938]\n",
      "loss: 0.617989  [340100/1106938]\n",
      "loss: 0.515266  [350100/1106938]\n",
      "loss: 0.777515  [360100/1106938]\n",
      "loss: 0.495146  [370100/1106938]\n",
      "loss: 0.602028  [380100/1106938]\n",
      "loss: 0.522113  [390100/1106938]\n",
      "loss: 0.696493  [400100/1106938]\n",
      "loss: 0.517411  [410100/1106938]\n",
      "loss: 0.749061  [420100/1106938]\n",
      "loss: 0.563938  [430100/1106938]\n",
      "loss: 0.668336  [440100/1106938]\n",
      "loss: 0.595398  [450100/1106938]\n",
      "loss: 0.592320  [460100/1106938]\n",
      "loss: 0.527426  [470100/1106938]\n",
      "loss: 0.513088  [480100/1106938]\n",
      "loss: 0.760024  [490100/1106938]\n",
      "loss: 0.548606  [500100/1106938]\n",
      "loss: 0.580682  [510100/1106938]\n",
      "loss: 0.584378  [520100/1106938]\n",
      "loss: 0.808245  [530100/1106938]\n",
      "loss: 0.657665  [540100/1106938]\n",
      "loss: 0.570389  [550100/1106938]\n",
      "loss: 0.549393  [560100/1106938]\n",
      "loss: 0.650150  [570100/1106938]\n",
      "loss: 0.547016  [580100/1106938]\n",
      "loss: 0.591034  [590100/1106938]\n",
      "loss: 0.464776  [600100/1106938]\n",
      "loss: 0.739648  [610100/1106938]\n",
      "loss: 0.657850  [620100/1106938]\n",
      "loss: 0.565930  [630100/1106938]\n",
      "loss: 0.577959  [640100/1106938]\n",
      "loss: 0.747412  [650100/1106938]\n",
      "loss: 0.520118  [660100/1106938]\n",
      "loss: 0.480554  [670100/1106938]\n",
      "loss: 0.616059  [680100/1106938]\n",
      "loss: 0.655863  [690100/1106938]\n",
      "loss: 0.580865  [700100/1106938]\n",
      "loss: 0.710067  [710100/1106938]\n",
      "loss: 0.618200  [720100/1106938]\n",
      "loss: 0.637416  [730100/1106938]\n",
      "loss: 0.470488  [740100/1106938]\n",
      "loss: 0.513369  [750100/1106938]\n",
      "loss: 0.704119  [760100/1106938]\n",
      "loss: 0.640447  [770100/1106938]\n",
      "loss: 0.569428  [780100/1106938]\n",
      "loss: 0.625442  [790100/1106938]\n",
      "loss: 0.446319  [800100/1106938]\n",
      "loss: 0.442300  [810100/1106938]\n",
      "loss: 0.688183  [820100/1106938]\n",
      "loss: 0.650065  [830100/1106938]\n",
      "loss: 0.738239  [840100/1106938]\n",
      "loss: 0.591494  [850100/1106938]\n",
      "loss: 0.835418  [860100/1106938]\n",
      "loss: 0.493948  [870100/1106938]\n",
      "loss: 0.696986  [880100/1106938]\n",
      "loss: 0.670041  [890100/1106938]\n",
      "loss: 0.670252  [900100/1106938]\n",
      "loss: 0.530697  [910100/1106938]\n",
      "loss: 0.488799  [920100/1106938]\n",
      "loss: 0.513783  [930100/1106938]\n",
      "loss: 0.782191  [940100/1106938]\n",
      "loss: 0.661818  [950100/1106938]\n",
      "loss: 0.508562  [960100/1106938]\n",
      "loss: 0.591407  [970100/1106938]\n",
      "loss: 0.621572  [980100/1106938]\n",
      "loss: 0.685873  [990100/1106938]\n",
      "loss: 0.561426  [1000100/1106938]\n",
      "loss: 0.817989  [1010100/1106938]\n",
      "loss: 0.712345  [1020100/1106938]\n",
      "loss: 0.546802  [1030100/1106938]\n",
      "loss: 0.514752  [1040100/1106938]\n",
      "loss: 0.586820  [1050100/1106938]\n",
      "loss: 0.557324  [1060100/1106938]\n",
      "loss: 0.571685  [1070100/1106938]\n",
      "loss: 0.460775  [1080100/1106938]\n",
      "loss: 0.607232  [1090100/1106938]\n",
      "loss: 0.631711  [1100100/1106938]\n",
      "Test Error: \n",
      " Accuracy: 76.5%, Avg loss: 0.701507 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.567035  [  100/1106938]\n",
      "loss: 0.462961  [10100/1106938]\n",
      "loss: 0.410412  [20100/1106938]\n",
      "loss: 0.395182  [30100/1106938]\n",
      "loss: 0.729265  [40100/1106938]\n",
      "loss: 0.419227  [50100/1106938]\n",
      "loss: 0.641714  [60100/1106938]\n",
      "loss: 0.387774  [70100/1106938]\n",
      "loss: 0.450549  [80100/1106938]\n",
      "loss: 0.639864  [90100/1106938]\n",
      "loss: 0.571496  [100100/1106938]\n",
      "loss: 0.493206  [110100/1106938]\n",
      "loss: 0.615802  [120100/1106938]\n",
      "loss: 0.551928  [130100/1106938]\n",
      "loss: 0.512558  [140100/1106938]\n",
      "loss: 0.655556  [150100/1106938]\n",
      "loss: 0.523443  [160100/1106938]\n",
      "loss: 0.656573  [170100/1106938]\n",
      "loss: 0.638010  [180100/1106938]\n",
      "loss: 0.776872  [190100/1106938]\n",
      "loss: 0.471445  [200100/1106938]\n",
      "loss: 0.529649  [210100/1106938]\n",
      "loss: 0.463045  [220100/1106938]\n",
      "loss: 0.592887  [230100/1106938]\n",
      "loss: 0.455206  [240100/1106938]\n",
      "loss: 0.718955  [250100/1106938]\n",
      "loss: 0.467935  [260100/1106938]\n",
      "loss: 0.421933  [270100/1106938]\n",
      "loss: 0.514631  [280100/1106938]\n",
      "loss: 0.506444  [290100/1106938]\n",
      "loss: 0.623385  [300100/1106938]\n",
      "loss: 0.543149  [310100/1106938]\n",
      "loss: 0.471886  [320100/1106938]\n",
      "loss: 0.505063  [330100/1106938]\n",
      "loss: 0.397712  [340100/1106938]\n",
      "loss: 0.484989  [350100/1106938]\n",
      "loss: 0.474926  [360100/1106938]\n",
      "loss: 0.756879  [370100/1106938]\n",
      "loss: 0.479750  [380100/1106938]\n",
      "loss: 0.737719  [390100/1106938]\n",
      "loss: 0.542746  [400100/1106938]\n",
      "loss: 0.558727  [410100/1106938]\n",
      "loss: 0.629500  [420100/1106938]\n",
      "loss: 0.657159  [430100/1106938]\n",
      "loss: 0.589710  [440100/1106938]\n",
      "loss: 0.273499  [450100/1106938]\n",
      "loss: 0.581873  [460100/1106938]\n",
      "loss: 0.599400  [470100/1106938]\n",
      "loss: 0.476466  [480100/1106938]\n",
      "loss: 0.553844  [490100/1106938]\n",
      "loss: 0.484503  [500100/1106938]\n",
      "loss: 0.527871  [510100/1106938]\n",
      "loss: 0.417074  [520100/1106938]\n",
      "loss: 0.441497  [530100/1106938]\n",
      "loss: 0.537713  [540100/1106938]\n",
      "loss: 0.555106  [550100/1106938]\n",
      "loss: 0.479081  [560100/1106938]\n",
      "loss: 0.564690  [570100/1106938]\n",
      "loss: 0.766423  [580100/1106938]\n",
      "loss: 0.737695  [590100/1106938]\n",
      "loss: 0.638157  [600100/1106938]\n",
      "loss: 0.536957  [610100/1106938]\n",
      "loss: 0.536844  [620100/1106938]\n",
      "loss: 0.783105  [630100/1106938]\n",
      "loss: 0.465923  [640100/1106938]\n",
      "loss: 0.626545  [650100/1106938]\n",
      "loss: 0.578273  [660100/1106938]\n",
      "loss: 0.530272  [670100/1106938]\n",
      "loss: 0.390684  [680100/1106938]\n",
      "loss: 0.536938  [690100/1106938]\n",
      "loss: 0.567793  [700100/1106938]\n",
      "loss: 0.550854  [710100/1106938]\n",
      "loss: 0.576123  [720100/1106938]\n",
      "loss: 0.576477  [730100/1106938]\n",
      "loss: 0.456965  [740100/1106938]\n",
      "loss: 0.692169  [750100/1106938]\n",
      "loss: 0.733513  [760100/1106938]\n",
      "loss: 0.460800  [770100/1106938]\n",
      "loss: 0.631588  [780100/1106938]\n",
      "loss: 0.691638  [790100/1106938]\n",
      "loss: 0.418727  [800100/1106938]\n",
      "loss: 0.504314  [810100/1106938]\n",
      "loss: 0.768897  [820100/1106938]\n",
      "loss: 0.839760  [830100/1106938]\n",
      "loss: 0.757510  [840100/1106938]\n",
      "loss: 0.522395  [850100/1106938]\n",
      "loss: 0.488614  [860100/1106938]\n",
      "loss: 0.836460  [870100/1106938]\n",
      "loss: 0.515919  [880100/1106938]\n",
      "loss: 0.694705  [890100/1106938]\n",
      "loss: 0.730251  [900100/1106938]\n",
      "loss: 0.567723  [910100/1106938]\n",
      "loss: 0.587264  [920100/1106938]\n",
      "loss: 0.658705  [930100/1106938]\n",
      "loss: 0.556516  [940100/1106938]\n",
      "loss: 0.553132  [950100/1106938]\n",
      "loss: 0.681707  [960100/1106938]\n",
      "loss: 0.428416  [970100/1106938]\n",
      "loss: 0.472092  [980100/1106938]\n",
      "loss: 0.443808  [990100/1106938]\n",
      "loss: 0.497882  [1000100/1106938]\n",
      "loss: 0.766504  [1010100/1106938]\n",
      "loss: 0.529680  [1020100/1106938]\n",
      "loss: 0.468871  [1030100/1106938]\n",
      "loss: 0.548780  [1040100/1106938]\n",
      "loss: 0.528673  [1050100/1106938]\n",
      "loss: 0.644577  [1060100/1106938]\n",
      "loss: 0.778220  [1070100/1106938]\n",
      "loss: 0.784068  [1080100/1106938]\n",
      "loss: 0.483973  [1090100/1106938]\n",
      "loss: 0.648069  [1100100/1106938]\n",
      "Test Error: \n",
      " Accuracy: 77.0%, Avg loss: 0.693220 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.598016  [  100/1106938]\n",
      "loss: 0.300777  [10100/1106938]\n",
      "loss: 0.502297  [20100/1106938]\n",
      "loss: 0.558338  [30100/1106938]\n",
      "loss: 0.526731  [40100/1106938]\n",
      "loss: 0.515376  [50100/1106938]\n",
      "loss: 0.342421  [60100/1106938]\n",
      "loss: 0.685695  [70100/1106938]\n",
      "loss: 0.609919  [80100/1106938]\n",
      "loss: 0.576161  [90100/1106938]\n",
      "loss: 0.408388  [100100/1106938]\n",
      "loss: 0.524141  [110100/1106938]\n",
      "loss: 0.523556  [120100/1106938]\n",
      "loss: 0.593705  [130100/1106938]\n",
      "loss: 0.364704  [140100/1106938]\n",
      "loss: 0.485241  [150100/1106938]\n",
      "loss: 0.456307  [160100/1106938]\n",
      "loss: 0.604785  [170100/1106938]\n",
      "loss: 0.547920  [180100/1106938]\n",
      "loss: 0.593375  [190100/1106938]\n",
      "loss: 0.367200  [200100/1106938]\n",
      "loss: 0.664513  [210100/1106938]\n",
      "loss: 0.446836  [220100/1106938]\n",
      "loss: 0.473114  [230100/1106938]\n",
      "loss: 0.605874  [240100/1106938]\n",
      "loss: 0.578756  [250100/1106938]\n",
      "loss: 0.495154  [260100/1106938]\n",
      "loss: 0.744886  [270100/1106938]\n",
      "loss: 0.579873  [280100/1106938]\n",
      "loss: 0.717714  [290100/1106938]\n",
      "loss: 0.486967  [300100/1106938]\n",
      "loss: 0.482963  [310100/1106938]\n",
      "loss: 0.717083  [320100/1106938]\n",
      "loss: 0.431413  [330100/1106938]\n",
      "loss: 0.488941  [340100/1106938]\n",
      "loss: 0.563133  [350100/1106938]\n",
      "loss: 0.699070  [360100/1106938]\n",
      "loss: 0.499430  [370100/1106938]\n",
      "loss: 0.623522  [380100/1106938]\n",
      "loss: 0.495650  [390100/1106938]\n",
      "loss: 0.483898  [400100/1106938]\n",
      "loss: 0.395761  [410100/1106938]\n",
      "loss: 0.568950  [420100/1106938]\n",
      "loss: 0.414274  [430100/1106938]\n",
      "loss: 0.696004  [440100/1106938]\n",
      "loss: 0.406967  [450100/1106938]\n",
      "loss: 0.522168  [460100/1106938]\n",
      "loss: 0.744533  [470100/1106938]\n",
      "loss: 0.635605  [480100/1106938]\n",
      "loss: 0.559097  [490100/1106938]\n",
      "loss: 0.468381  [500100/1106938]\n",
      "loss: 0.717134  [510100/1106938]\n",
      "loss: 0.649879  [520100/1106938]\n",
      "loss: 0.591039  [530100/1106938]\n",
      "loss: 0.694236  [540100/1106938]\n",
      "loss: 0.428445  [550100/1106938]\n",
      "loss: 0.391292  [560100/1106938]\n",
      "loss: 0.753829  [570100/1106938]\n",
      "loss: 0.441669  [580100/1106938]\n",
      "loss: 0.555956  [590100/1106938]\n",
      "loss: 0.459555  [600100/1106938]\n",
      "loss: 0.422653  [610100/1106938]\n",
      "loss: 0.632197  [620100/1106938]\n",
      "loss: 0.483320  [630100/1106938]\n",
      "loss: 0.613624  [640100/1106938]\n",
      "loss: 0.365165  [650100/1106938]\n",
      "loss: 0.572568  [660100/1106938]\n",
      "loss: 0.668826  [670100/1106938]\n",
      "loss: 0.516898  [680100/1106938]\n",
      "loss: 0.383630  [690100/1106938]\n",
      "loss: 0.434995  [700100/1106938]\n",
      "loss: 0.592508  [710100/1106938]\n",
      "loss: 0.407208  [720100/1106938]\n",
      "loss: 0.509826  [730100/1106938]\n",
      "loss: 0.418522  [740100/1106938]\n",
      "loss: 0.563036  [750100/1106938]\n",
      "loss: 0.814342  [760100/1106938]\n",
      "loss: 0.543065  [770100/1106938]\n",
      "loss: 0.504199  [780100/1106938]\n",
      "loss: 0.489148  [790100/1106938]\n",
      "loss: 0.501776  [800100/1106938]\n",
      "loss: 0.495828  [810100/1106938]\n",
      "loss: 0.591318  [820100/1106938]\n",
      "loss: 0.370599  [830100/1106938]\n",
      "loss: 0.400015  [840100/1106938]\n",
      "loss: 0.395328  [850100/1106938]\n",
      "loss: 0.761631  [860100/1106938]\n",
      "loss: 0.566645  [870100/1106938]\n",
      "loss: 0.602694  [880100/1106938]\n",
      "loss: 0.616904  [890100/1106938]\n",
      "loss: 0.898675  [900100/1106938]\n",
      "loss: 0.748743  [910100/1106938]\n",
      "loss: 0.421677  [920100/1106938]\n",
      "loss: 0.374504  [930100/1106938]\n",
      "loss: 0.502398  [940100/1106938]\n",
      "loss: 0.746818  [950100/1106938]\n",
      "loss: 0.530776  [960100/1106938]\n",
      "loss: 0.560271  [970100/1106938]\n",
      "loss: 0.474589  [980100/1106938]\n",
      "loss: 0.622978  [990100/1106938]\n",
      "loss: 0.363623  [1000100/1106938]\n",
      "loss: 0.682753  [1010100/1106938]\n",
      "loss: 0.544587  [1020100/1106938]\n",
      "loss: 0.752832  [1030100/1106938]\n",
      "loss: 0.469755  [1040100/1106938]\n",
      "loss: 0.694949  [1050100/1106938]\n",
      "loss: 0.471018  [1060100/1106938]\n",
      "loss: 0.596043  [1070100/1106938]\n",
      "loss: 0.629881  [1080100/1106938]\n",
      "loss: 0.620929  [1090100/1106938]\n",
      "loss: 0.560381  [1100100/1106938]\n",
      "Test Error: \n",
      " Accuracy: 77.0%, Avg loss: 0.697850 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.419424  [  100/1106938]\n",
      "loss: 0.500122  [10100/1106938]\n",
      "loss: 0.482148  [20100/1106938]\n",
      "loss: 0.364803  [30100/1106938]\n",
      "loss: 0.594427  [40100/1106938]\n",
      "loss: 0.558444  [50100/1106938]\n",
      "loss: 0.429520  [60100/1106938]\n",
      "loss: 0.481855  [70100/1106938]\n",
      "loss: 0.456517  [80100/1106938]\n",
      "loss: 0.516308  [90100/1106938]\n",
      "loss: 0.406294  [100100/1106938]\n",
      "loss: 0.705731  [110100/1106938]\n",
      "loss: 0.508056  [120100/1106938]\n",
      "loss: 0.510115  [130100/1106938]\n",
      "loss: 0.529424  [140100/1106938]\n",
      "loss: 0.545449  [150100/1106938]\n",
      "loss: 0.578833  [160100/1106938]\n",
      "loss: 0.390104  [170100/1106938]\n",
      "loss: 0.753525  [180100/1106938]\n",
      "loss: 0.534153  [190100/1106938]\n",
      "loss: 0.375508  [200100/1106938]\n",
      "loss: 0.473645  [210100/1106938]\n",
      "loss: 0.650452  [220100/1106938]\n",
      "loss: 0.547360  [230100/1106938]\n",
      "loss: 0.667508  [240100/1106938]\n",
      "loss: 0.582721  [250100/1106938]\n",
      "loss: 0.635888  [260100/1106938]\n",
      "loss: 0.397339  [270100/1106938]\n",
      "loss: 0.538311  [280100/1106938]\n",
      "loss: 0.379810  [290100/1106938]\n",
      "loss: 0.400926  [300100/1106938]\n",
      "loss: 0.459203  [310100/1106938]\n",
      "loss: 0.426371  [320100/1106938]\n",
      "loss: 0.360975  [330100/1106938]\n",
      "loss: 0.452539  [340100/1106938]\n",
      "loss: 0.709931  [350100/1106938]\n",
      "loss: 0.648299  [360100/1106938]\n",
      "loss: 0.387968  [370100/1106938]\n",
      "loss: 0.473408  [380100/1106938]\n",
      "loss: 0.609355  [390100/1106938]\n",
      "loss: 0.374833  [400100/1106938]\n",
      "loss: 0.524391  [410100/1106938]\n",
      "loss: 0.469405  [420100/1106938]\n",
      "loss: 0.534556  [430100/1106938]\n",
      "loss: 0.719660  [440100/1106938]\n",
      "loss: 0.533747  [450100/1106938]\n",
      "loss: 0.554951  [460100/1106938]\n",
      "loss: 0.541729  [470100/1106938]\n",
      "loss: 0.506307  [480100/1106938]\n",
      "loss: 0.569282  [490100/1106938]\n",
      "loss: 0.397646  [500100/1106938]\n",
      "loss: 0.747505  [510100/1106938]\n",
      "loss: 0.527986  [520100/1106938]\n",
      "loss: 0.499743  [530100/1106938]\n",
      "loss: 0.712650  [540100/1106938]\n",
      "loss: 0.586145  [550100/1106938]\n",
      "loss: 0.472094  [560100/1106938]\n",
      "loss: 0.446131  [570100/1106938]\n",
      "loss: 0.636204  [580100/1106938]\n",
      "loss: 0.440737  [590100/1106938]\n",
      "loss: 0.581070  [600100/1106938]\n",
      "loss: 0.469711  [610100/1106938]\n",
      "loss: 0.631692  [620100/1106938]\n",
      "loss: 0.410093  [630100/1106938]\n",
      "loss: 0.596879  [640100/1106938]\n",
      "loss: 0.530508  [650100/1106938]\n",
      "loss: 0.487813  [660100/1106938]\n",
      "loss: 0.517852  [670100/1106938]\n",
      "loss: 0.449109  [680100/1106938]\n",
      "loss: 0.363907  [690100/1106938]\n",
      "loss: 0.407305  [700100/1106938]\n",
      "loss: 0.421383  [710100/1106938]\n",
      "loss: 0.600555  [720100/1106938]\n",
      "loss: 0.766133  [730100/1106938]\n",
      "loss: 0.669204  [740100/1106938]\n",
      "loss: 0.456584  [750100/1106938]\n",
      "loss: 0.713085  [760100/1106938]\n",
      "loss: 0.618335  [770100/1106938]\n",
      "loss: 0.382990  [780100/1106938]\n",
      "loss: 0.380560  [790100/1106938]\n",
      "loss: 0.466238  [800100/1106938]\n",
      "loss: 0.467933  [810100/1106938]\n",
      "loss: 0.488305  [820100/1106938]\n",
      "loss: 0.551721  [830100/1106938]\n",
      "loss: 0.482534  [840100/1106938]\n",
      "loss: 0.460997  [850100/1106938]\n",
      "loss: 0.478535  [860100/1106938]\n",
      "loss: 0.384348  [870100/1106938]\n",
      "loss: 0.410608  [880100/1106938]\n",
      "loss: 0.532592  [890100/1106938]\n",
      "loss: 0.476074  [900100/1106938]\n",
      "loss: 0.504497  [910100/1106938]\n",
      "loss: 0.511454  [920100/1106938]\n",
      "loss: 0.437512  [930100/1106938]\n",
      "loss: 0.507630  [940100/1106938]\n",
      "loss: 0.548877  [950100/1106938]\n",
      "loss: 0.401014  [960100/1106938]\n",
      "loss: 0.470662  [970100/1106938]\n",
      "loss: 0.517077  [980100/1106938]\n",
      "loss: 0.473225  [990100/1106938]\n",
      "loss: 0.754213  [1000100/1106938]\n",
      "loss: 0.482187  [1010100/1106938]\n",
      "loss: 0.725955  [1020100/1106938]\n",
      "loss: 0.696974  [1030100/1106938]\n",
      "loss: 0.778771  [1040100/1106938]\n",
      "loss: 0.513640  [1050100/1106938]\n",
      "loss: 0.643266  [1060100/1106938]\n",
      "loss: 0.687863  [1070100/1106938]\n",
      "loss: 0.615960  [1080100/1106938]\n",
      "loss: 0.544533  [1090100/1106938]\n",
      "loss: 0.557037  [1100100/1106938]\n",
      "Test Error: \n",
      " Accuracy: 77.4%, Avg loss: 0.692800 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "min_loss = 1.0\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    dev_loss = dev(dev_dataloader, model, loss_fn)\n",
    "\n",
    "    if dev_loss < min_loss:\n",
    "        min_loss = dev_loss\n",
    "        torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([36, 36, 36,  ..., 38, 30, 25])\n"
     ]
    }
   ],
   "source": [
    "def test(dataloader, model):\n",
    "    del model\n",
    "    model = NeuralNetwork()\n",
    "    ckpt = torch.load(\"model.pth\")\n",
    "    model.load_state_dict(ckpt)\n",
    "    model.eval()\n",
    "\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for X in dataloader:\n",
    "            pred = model(X)\n",
    "            pred = pred.squeeze()\n",
    "            preds.append(pred.argmax(1))\n",
    "    \n",
    "    preds = torch.cat(preds, dim=0).long()\n",
    "    print(preds)\n",
    "    save_result(preds)\n",
    "    \n",
    "test(test_dataloader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
