{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hw2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train data:  (1229932, 429)\n",
      "Size of train label:  (1229932,)\n",
      "Size of test data:  (451552, 429)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "def save_result(preds):\n",
    "    with open('test_result.csv', 'w', encoding='utf-8') as f:\n",
    "        f.write('Id,Class\\n')\n",
    "        for i, pred in enumerate(preds):\n",
    "            f.write('%d,%d\\n' % (i, pred))\n",
    "    \n",
    "    return\n",
    "\n",
    "data_root = './timit_11/'\n",
    "\n",
    "train = np.load(data_root + 'train_11.npy')\n",
    "train_label = np.load(data_root + 'train_label_11.npy')\n",
    "test = np.load(data_root + 'test_11.npy')\n",
    "\n",
    "print('Size of train data: ', train.shape)\n",
    "print('Size of train label: ', train_label.shape)\n",
    "print('Size of test data: ', test.shape)\n",
    "\n",
    "class TIMITDataset(Dataset): \n",
    "    def __init__(self, data, label, mode=\"train\"):\n",
    "        self.mode = mode\n",
    "        self.data = pd.DataFrame(data)\n",
    "        self.label = pd.DataFrame(label)\n",
    "\n",
    "        # 需要区分 train data 和 test data\n",
    "        if self.mode == \"train\":\n",
    "            indices = [i for i in range(len(self.data)) if i % 10 != 0]\n",
    "            self.data = self.data.iloc[indices, :]\n",
    "            self.label = self.label.iloc[indices]\n",
    "\n",
    "        elif self.mode == \"dev\":\n",
    "            indices = [i for i in range(len(self.data)) if i % 10 == 0]\n",
    "            self.data = self.data.iloc[indices, :]\n",
    "            self.label = self.label.iloc[indices]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == \"train\" or self.mode == \"dev\":\n",
    "            X = torch.tensor(self.data.iloc[idx].values.reshape(11, 39)[3:8, :].flatten(), dtype=torch.float32)\n",
    "            y = torch.tensor(int(self.label.iloc[idx]), dtype=torch.int32)\n",
    "            return X, y\n",
    "        elif self.mode == \"test\":\n",
    "            return torch.tensor(self.data.iloc[idx].values.reshape(11, 39)[3:8, :].flatten(), dtype=torch.float32)\n",
    "        \n",
    "train_data = TIMITDataset(train, train_label, mode=\"train\")\n",
    "dev_data = TIMITDataset(train, train_label, mode=\"dev\")\n",
    "test_data = TIMITDataset(test, None, mode=\"test\")\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=100, shuffle=True)\n",
    "dev_dataloader = DataLoader(dev_data, batch_size=100, shuffle=False)\n",
    "test_dataloader = DataLoader(test_data, batch_size=100, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1106938\n",
      "1         -1.060318\n",
      "2         -1.061233\n",
      "3         -1.062128\n",
      "4         -1.061916\n",
      "5         -1.050420\n",
      "             ...   \n",
      "1229926    0.177523\n",
      "1229927    0.138306\n",
      "1229928    0.065482\n",
      "1229929    0.057637\n",
      "1229931    0.111426\n",
      "Name: 0, Length: 1106938, dtype: float64\n",
      "[-8.15812826e-01 -3.48689795e-01  3.95477504e-01 -1.63738783e-02\n",
      "  8.60317469e-01  1.44315893e-02  3.84072900e-01  9.11684811e-01\n",
      "  2.74616122e-01  7.21527755e-01  1.62349343e+00  1.94353950e+00\n",
      "  9.64264333e-01 -7.72595452e-03 -1.81192949e-01  5.74482083e-01\n",
      " -2.26134375e-01  4.03959244e-01  3.92566890e-01  4.68575805e-01\n",
      "  9.03372943e-01  9.12166536e-02  6.29455924e-01  6.50277019e-01\n",
      " -7.98417151e-01 -5.07435977e-01 -1.93714548e-03 -4.96793658e-01\n",
      " -1.30517155e-01  1.90668270e-01  3.52899522e-01  1.43388236e+00\n",
      "  5.82235932e-01 -4.42176044e-01  3.92167211e-01  3.82801175e-01\n",
      " -5.74597836e-01 -9.66160059e-01  8.36981013e-02 -8.17853749e-01\n",
      " -3.47191900e-01  5.85275650e-01 -1.76819056e-01  6.26596749e-01\n",
      " -2.08324268e-01  1.41138196e-01  8.14121485e-01  1.20860793e-01\n",
      "  7.84978092e-01  1.85303009e+00  2.17269802e+00  1.76232314e+00\n",
      "  4.56195819e-04 -3.57095480e-01  4.66112584e-01  4.36793976e-02\n",
      "  2.44239345e-01  6.95920229e-01  5.79180181e-01  4.36306536e-01\n",
      "  2.77873218e-01  5.59000432e-01  2.24675238e-01 -8.69635940e-01\n",
      " -2.87959218e-01  2.19163373e-02 -3.04890573e-01 -3.68410289e-01\n",
      "  6.86901152e-01  1.93446845e-01  1.22113764e+00  6.87861562e-01\n",
      " -8.80873322e-01  2.27267519e-01 -3.34653795e-01 -1.30035055e+00\n",
      " -7.05772698e-01 -4.29470271e-01 -8.19877267e-01 -5.21579981e-01\n",
      "  6.54818118e-01 -2.47517794e-01  7.76656926e-01  4.93845195e-01\n",
      "  6.38528287e-01  1.34373748e+00  1.16990530e+00  1.61719799e+00\n",
      "  1.58583629e+00  7.26012826e-01  3.88572276e-01  4.44570705e-02\n",
      " -8.08226705e-01  2.70812511e-01  5.06846428e-01 -3.04862764e-02\n",
      "  9.13899004e-01  6.55444324e-01  1.99248157e-02  1.21349134e-01\n",
      "  8.46248269e-01 -6.90799773e-01 -7.40965366e-01 -2.29644522e-01\n",
      "  1.29772574e-01 -1.11926723e+00 -3.95132959e-01  1.44303000e+00\n",
      " -1.17166817e+00 -3.43524873e-01 -3.21841627e-01 -6.46609664e-01\n",
      "  2.92410403e-01  8.45067203e-02 -1.61831212e+00  6.18455052e-01\n",
      "  6.53575718e-01 -8.21439385e-01 -5.79824984e-01  7.55298138e-01\n",
      " -1.08926654e-01  1.01349998e+00  9.15398777e-01  7.18544662e-01\n",
      "  9.52252090e-01  6.39603972e-01  1.29298186e+00  1.33980155e+00\n",
      "  7.12813556e-01  8.59630764e-01  1.09658375e-01 -1.40015316e+00\n",
      "  1.04810074e-01  8.90806198e-01 -5.54478347e-01  5.59858501e-01\n",
      "  4.76976901e-01 -4.27572317e-02  4.54809010e-01  8.39707017e-01\n",
      " -1.09382379e+00 -3.02106410e-01 -2.24989846e-01  2.37577826e-01\n",
      " -1.64795864e+00 -3.36687922e-01  1.51167357e+00 -1.72091830e+00\n",
      " -1.14630747e+00 -4.85988587e-01 -5.19942343e-01 -2.66638696e-01\n",
      " -1.06537372e-01 -7.84751356e-01  1.46447480e+00  3.82050872e-01\n",
      " -8.22048783e-01 -6.27880633e-01  7.42801547e-01 -3.90241779e-02\n",
      "  1.18365037e+00  1.21759546e+00  1.00882006e+00  1.14920890e+00\n",
      "  7.69170105e-01  1.35666311e+00  1.38351047e+00  7.51709640e-01\n",
      "  8.97786200e-01  1.88909724e-01 -1.84347391e+00 -5.92376217e-02\n",
      "  1.00558782e+00 -1.07068503e+00  2.94810176e-01  4.48351741e-01\n",
      " -2.13550150e-01  1.71433255e-01  7.12499261e-01 -7.72415757e-01\n",
      " -1.92916185e-01 -5.73165357e-01  3.06467086e-01 -1.47141361e+00\n",
      " -5.06693542e-01  4.55461711e-01 -1.67400038e+00 -1.24507964e+00\n",
      " -3.37909043e-01 -9.42163840e-02 -7.19930530e-01 -3.06202561e-01\n",
      "  7.89523959e-01  1.54539812e+00 -4.80546117e-01 -8.01864445e-01\n",
      " -7.07150519e-01  6.28714204e-01  3.89305472e-01  7.86976397e-01\n",
      "  1.08158088e+00  1.06749725e+00  5.51742494e-01  8.54246080e-01\n",
      "  9.38302457e-01  7.02012718e-01  8.34094465e-01  7.78473735e-01\n",
      "  2.46743500e-01 -2.00786448e+00 -2.59372681e-01  1.01744902e+00\n",
      " -1.21515656e+00  1.41148657e-01  2.77194500e-01 -6.88229918e-01\n",
      " -7.03711510e-01  2.29979977e-01 -6.55366600e-01  5.08279741e-01\n",
      " -7.98558593e-01  2.42155805e-01 -6.39601648e-01 -3.57569396e-01\n",
      " -5.19201696e-01 -1.06722796e+00 -8.76354277e-01 -6.17329240e-01\n",
      " -6.06594145e-01 -1.51309419e+00 -7.37058938e-01  1.06613851e+00\n",
      "  1.54067111e+00 -2.66122669e-01 -7.07047403e-01 -1.49290991e+00\n",
      "  5.77214837e-01  1.01184523e+00  2.28464417e-02  4.62869883e-01\n",
      "  5.73139429e-01  6.86030209e-01  6.59196913e-01  2.08747530e+00\n",
      " -8.50434899e-02  1.52945018e+00  1.38857460e+00  3.07250828e-01\n",
      " -1.93132412e+00 -2.75336593e-01  7.48762012e-01 -1.39199424e+00\n",
      " -2.62844265e-01 -3.80406469e-01 -9.25785244e-01 -1.07260036e+00\n",
      "  1.93184674e-01 -2.34122247e-01  8.09500456e-01 -3.79938722e-01\n",
      "  1.68058470e-01  3.26376736e-01 -1.23876520e-02 -1.41517222e+00\n",
      "  4.59585823e-02 -7.19508082e-02 -8.91312122e-01 -1.21071845e-01\n",
      " -4.21448231e-01  1.49552613e-01  1.02639949e+00 -6.27906859e-01\n",
      " -7.97479987e-01 -6.25015140e-01 -1.97806239e+00  5.62663913e-01\n",
      "  1.08202255e+00 -3.15571904e-01  1.60826400e-01  7.12553084e-01\n",
      "  8.63311768e-01  8.94219697e-01  1.68949175e+00  5.86832941e-01\n",
      "  1.82698238e+00  7.64400065e-01  3.78606379e-01 -1.93906581e+00\n",
      " -2.54375786e-01  2.99605757e-01 -1.52195930e+00 -3.10934246e-01\n",
      " -4.56030488e-01 -3.86570841e-01 -1.04673004e+00 -8.25209916e-03\n",
      "  4.21362609e-01  7.00726211e-01 -6.20774686e-01  1.31356940e-01\n",
      "  9.06182945e-01  2.47731835e-01 -1.79323173e+00  8.11256707e-01\n",
      "  6.08145833e-01 -3.43659341e-01  5.73926151e-01 -4.04250085e-01\n",
      " -6.75222874e-01  1.18168211e+00 -8.53463888e-01 -8.56958423e-03\n",
      " -5.41229427e-01 -2.12455940e+00  3.38244736e-01  6.46516800e-01\n",
      " -5.64446926e-01  4.48248535e-01  1.05095375e+00  8.40595901e-01\n",
      "  3.11493397e-01  1.61268258e+00  1.73551130e+00  1.29770410e+00\n",
      " -6.98527768e-02  4.62385297e-01 -1.74325550e+00 -9.84729901e-02\n",
      " -1.19800247e-01 -1.28110456e+00 -1.72435850e-01 -5.89624405e-01\n",
      " -1.42206907e-01 -1.27849662e+00 -3.17881048e-01  5.10424614e-01\n",
      "  3.22074503e-01 -4.75895911e-01  1.56983048e-01  1.39186656e+00\n",
      "  4.53995079e-01 -1.52112699e+00  1.43880963e+00  8.51973772e-01\n",
      "  2.16113012e-02  8.90362442e-01 -5.90594001e-02 -8.75823736e-01\n",
      "  2.90102303e-01 -1.13367736e+00  6.45637989e-01 -5.26147008e-01\n",
      " -2.06020331e+00  2.99371749e-01  6.83552027e-01 -3.23276311e-01\n",
      "  7.36325026e-01  6.55510664e-01 -1.81245700e-01 -9.77546930e-01\n",
      "  9.46608126e-01  9.21990693e-01  2.14190197e+00  2.17398033e-01\n",
      "  5.16363978e-01 -1.36978412e+00  1.74136609e-01 -4.52454537e-01\n",
      " -9.24559832e-01 -8.75526816e-02 -8.43120098e-01  3.46856117e-01\n",
      " -9.60662067e-01 -4.31923479e-01  8.75602186e-01 -3.55560482e-02\n",
      " -7.17057049e-01  1.60263851e-01  1.31419182e+00  6.40638173e-01\n",
      " -3.64474535e-01  8.50858510e-01  1.52416348e-01 -3.61995876e-01\n",
      "  7.94132769e-01  8.49550247e-01 -5.72304070e-01 -4.94386554e-01\n",
      " -8.14543962e-01  2.29587063e-01 -4.44377780e-01 -2.02960443e+00\n",
      "  5.13314545e-01  4.00894850e-01 -4.04429317e-01  5.56219280e-01\n",
      " -2.18392864e-01  2.01895729e-01  9.18949246e-02  2.01063347e+00\n",
      "  1.16167784e+00  9.79497254e-01  4.92651969e-01  4.97839808e-01\n",
      " -7.56824553e-01  3.33294928e-01 -6.37785673e-01 -3.69842678e-01\n",
      "  1.70299485e-01 -6.15268171e-01  8.54063451e-01 -4.88241553e-01\n",
      " -1.02353287e+00  2.20091596e-01 -8.97782147e-01 -9.17488098e-01\n",
      "  5.88676706e-02  5.84376633e-01  5.10970116e-01  6.05664790e-01\n",
      "  5.34149706e-01 -3.20446759e-01 -1.88363567e-02  1.45047593e+00\n",
      "  1.28690445e+00 -4.73928377e-02 -1.81417489e+00 -8.92933309e-01\n",
      "  2.31251955e-01]\n"
     ]
    }
   ],
   "source": [
    "print(train_data.__len__())\n",
    "print(train_data.data[0])\n",
    "# 判断变量类型\n",
    "# print(train[0])\n",
    "# print(type(train[0]))\n",
    "\n",
    "# 判断变量类型\n",
    "print(test[0])\n",
    "# print(type(test[0]))\n",
    "\n",
    "\n",
    "# print(train_label[0])\n",
    "# print(type(train_label[0]))\n",
    "# print(type(train_data))\n",
    "# print(type(train_data[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=195, out_features=1024, bias=True)\n",
      "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): Dropout(p=0.2, inplace=False)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (9): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (12): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=128, out_features=39, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(5*39, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 39)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.2, momentum=0.05)\n",
    "\n",
    "loss_record = {\"train\": [], \"dev\": []}\n",
    "pred_record = []\n",
    "target_record = []\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss_record[\"train\"].append(loss.item())\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "def dev(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "    return test_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.718244  [  100/1106938]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2c/pn5qg61n16nfl0b9y726w55r0000gn/T/ipykernel_12492/1375298387.py:48: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
      "  y = torch.tensor(int(self.label.iloc[idx]), dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.924306  [10100/1106938]\n",
      "loss: 1.629065  [20100/1106938]\n",
      "loss: 1.581387  [30100/1106938]\n",
      "loss: 1.278219  [40100/1106938]\n",
      "loss: 1.398767  [50100/1106938]\n",
      "loss: 1.523835  [60100/1106938]\n",
      "loss: 1.592161  [70100/1106938]\n",
      "loss: 1.461328  [80100/1106938]\n",
      "loss: 1.513250  [90100/1106938]\n",
      "loss: 1.552409  [100100/1106938]\n",
      "loss: 1.358105  [110100/1106938]\n",
      "loss: 1.402215  [120100/1106938]\n",
      "loss: 1.210059  [130100/1106938]\n",
      "loss: 1.369584  [140100/1106938]\n",
      "loss: 1.331123  [150100/1106938]\n",
      "loss: 1.217478  [160100/1106938]\n",
      "loss: 1.246017  [170100/1106938]\n",
      "loss: 1.187148  [180100/1106938]\n",
      "loss: 1.485128  [190100/1106938]\n",
      "loss: 1.252234  [200100/1106938]\n",
      "loss: 1.257684  [210100/1106938]\n",
      "loss: 1.434850  [220100/1106938]\n",
      "loss: 1.208052  [230100/1106938]\n",
      "loss: 1.276450  [240100/1106938]\n",
      "loss: 1.359486  [250100/1106938]\n",
      "loss: 1.121016  [260100/1106938]\n",
      "loss: 1.124274  [270100/1106938]\n",
      "loss: 1.174506  [280100/1106938]\n",
      "loss: 1.395367  [290100/1106938]\n",
      "loss: 1.599495  [300100/1106938]\n",
      "loss: 1.424653  [310100/1106938]\n",
      "loss: 1.117874  [320100/1106938]\n",
      "loss: 1.337607  [330100/1106938]\n",
      "loss: 1.223075  [340100/1106938]\n",
      "loss: 1.166163  [350100/1106938]\n",
      "loss: 1.309383  [360100/1106938]\n",
      "loss: 1.238647  [370100/1106938]\n",
      "loss: 1.246361  [380100/1106938]\n",
      "loss: 1.560916  [390100/1106938]\n",
      "loss: 1.313251  [400100/1106938]\n",
      "loss: 1.198925  [410100/1106938]\n",
      "loss: 0.974887  [420100/1106938]\n",
      "loss: 1.223059  [430100/1106938]\n",
      "loss: 1.316302  [440100/1106938]\n",
      "loss: 1.053151  [450100/1106938]\n",
      "loss: 1.431458  [460100/1106938]\n",
      "loss: 1.343105  [470100/1106938]\n",
      "loss: 1.111777  [480100/1106938]\n",
      "loss: 0.864527  [490100/1106938]\n",
      "loss: 1.500221  [500100/1106938]\n",
      "loss: 1.084469  [510100/1106938]\n",
      "loss: 1.154571  [520100/1106938]\n",
      "loss: 1.225778  [530100/1106938]\n",
      "loss: 1.052058  [540100/1106938]\n",
      "loss: 1.123876  [550100/1106938]\n",
      "loss: 1.041624  [560100/1106938]\n",
      "loss: 1.202405  [570100/1106938]\n",
      "loss: 1.112879  [580100/1106938]\n",
      "loss: 1.201679  [590100/1106938]\n",
      "loss: 0.964234  [600100/1106938]\n",
      "loss: 0.968756  [610100/1106938]\n",
      "loss: 1.089309  [620100/1106938]\n",
      "loss: 1.260087  [630100/1106938]\n",
      "loss: 1.050188  [640100/1106938]\n",
      "loss: 1.036326  [650100/1106938]\n",
      "loss: 1.135257  [660100/1106938]\n",
      "loss: 1.363396  [670100/1106938]\n",
      "loss: 1.505143  [680100/1106938]\n",
      "loss: 1.201035  [690100/1106938]\n",
      "loss: 1.519969  [700100/1106938]\n",
      "loss: 1.221827  [710100/1106938]\n",
      "loss: 1.085870  [720100/1106938]\n",
      "loss: 0.997832  [730100/1106938]\n",
      "loss: 0.975788  [740100/1106938]\n",
      "loss: 1.273625  [750100/1106938]\n",
      "loss: 1.265224  [760100/1106938]\n",
      "loss: 1.209792  [770100/1106938]\n",
      "loss: 1.382327  [780100/1106938]\n",
      "loss: 1.370193  [790100/1106938]\n",
      "loss: 1.100603  [800100/1106938]\n",
      "loss: 1.167622  [810100/1106938]\n",
      "loss: 1.228891  [820100/1106938]\n",
      "loss: 1.177920  [830100/1106938]\n",
      "loss: 1.172127  [840100/1106938]\n",
      "loss: 1.144198  [850100/1106938]\n",
      "loss: 1.013062  [860100/1106938]\n",
      "loss: 0.875042  [870100/1106938]\n",
      "loss: 1.016502  [880100/1106938]\n",
      "loss: 1.415902  [890100/1106938]\n",
      "loss: 1.259040  [900100/1106938]\n",
      "loss: 1.034632  [910100/1106938]\n",
      "loss: 1.071225  [920100/1106938]\n",
      "loss: 0.930371  [930100/1106938]\n",
      "loss: 1.029186  [940100/1106938]\n",
      "loss: 0.936675  [950100/1106938]\n",
      "loss: 1.033302  [960100/1106938]\n",
      "loss: 1.444803  [970100/1106938]\n",
      "loss: 1.061739  [980100/1106938]\n",
      "loss: 1.250095  [990100/1106938]\n",
      "loss: 0.879297  [1000100/1106938]\n",
      "loss: 1.181885  [1010100/1106938]\n",
      "loss: 1.116537  [1020100/1106938]\n",
      "loss: 0.987252  [1030100/1106938]\n",
      "loss: 1.221287  [1040100/1106938]\n",
      "loss: 1.111850  [1050100/1106938]\n",
      "loss: 0.947977  [1060100/1106938]\n",
      "loss: 1.135368  [1070100/1106938]\n",
      "loss: 1.097255  [1080100/1106938]\n",
      "loss: 1.118373  [1090100/1106938]\n",
      "loss: 1.123798  [1100100/1106938]\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: 1.003094 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.121232  [  100/1106938]\n",
      "loss: 1.075924  [10100/1106938]\n",
      "loss: 1.225128  [20100/1106938]\n",
      "loss: 1.010169  [30100/1106938]\n",
      "loss: 1.061553  [40100/1106938]\n",
      "loss: 1.096873  [50100/1106938]\n",
      "loss: 0.970600  [60100/1106938]\n",
      "loss: 0.939387  [70100/1106938]\n",
      "loss: 1.117094  [80100/1106938]\n",
      "loss: 0.959796  [90100/1106938]\n",
      "loss: 1.074952  [100100/1106938]\n",
      "loss: 1.156664  [110100/1106938]\n",
      "loss: 1.167574  [120100/1106938]\n",
      "loss: 1.111626  [130100/1106938]\n",
      "loss: 0.950277  [140100/1106938]\n",
      "loss: 1.053661  [150100/1106938]\n",
      "loss: 1.126488  [160100/1106938]\n",
      "loss: 1.173417  [170100/1106938]\n",
      "loss: 1.255684  [180100/1106938]\n",
      "loss: 0.992880  [190100/1106938]\n",
      "loss: 1.161772  [200100/1106938]\n",
      "loss: 1.139206  [210100/1106938]\n",
      "loss: 1.194807  [220100/1106938]\n",
      "loss: 0.962421  [230100/1106938]\n",
      "loss: 1.024132  [240100/1106938]\n",
      "loss: 1.443038  [250100/1106938]\n",
      "loss: 0.869033  [260100/1106938]\n",
      "loss: 0.931092  [270100/1106938]\n",
      "loss: 1.140642  [280100/1106938]\n",
      "loss: 1.085847  [290100/1106938]\n",
      "loss: 1.494213  [300100/1106938]\n",
      "loss: 1.282208  [310100/1106938]\n",
      "loss: 0.944950  [320100/1106938]\n",
      "loss: 1.229003  [330100/1106938]\n",
      "loss: 0.840398  [340100/1106938]\n",
      "loss: 1.067729  [350100/1106938]\n",
      "loss: 1.233194  [360100/1106938]\n",
      "loss: 0.838222  [370100/1106938]\n",
      "loss: 1.018001  [380100/1106938]\n",
      "loss: 1.285523  [390100/1106938]\n",
      "loss: 1.074130  [400100/1106938]\n",
      "loss: 0.900877  [410100/1106938]\n",
      "loss: 0.975665  [420100/1106938]\n",
      "loss: 0.866275  [430100/1106938]\n",
      "loss: 0.825103  [440100/1106938]\n",
      "loss: 1.051629  [450100/1106938]\n",
      "loss: 0.932015  [460100/1106938]\n",
      "loss: 1.277897  [470100/1106938]\n",
      "loss: 0.897472  [480100/1106938]\n",
      "loss: 1.277435  [490100/1106938]\n",
      "loss: 0.996761  [500100/1106938]\n",
      "loss: 1.116293  [510100/1106938]\n",
      "loss: 1.031723  [520100/1106938]\n",
      "loss: 1.003021  [530100/1106938]\n",
      "loss: 0.958399  [540100/1106938]\n",
      "loss: 1.042201  [550100/1106938]\n",
      "loss: 1.146091  [560100/1106938]\n",
      "loss: 1.128144  [570100/1106938]\n",
      "loss: 1.001599  [580100/1106938]\n",
      "loss: 1.297285  [590100/1106938]\n",
      "loss: 0.980702  [600100/1106938]\n",
      "loss: 1.095762  [610100/1106938]\n",
      "loss: 1.200066  [620100/1106938]\n",
      "loss: 1.127885  [630100/1106938]\n",
      "loss: 0.897797  [640100/1106938]\n",
      "loss: 0.910279  [650100/1106938]\n",
      "loss: 0.994255  [660100/1106938]\n",
      "loss: 1.104702  [670100/1106938]\n",
      "loss: 1.058256  [680100/1106938]\n",
      "loss: 1.049678  [690100/1106938]\n",
      "loss: 0.946914  [700100/1106938]\n",
      "loss: 1.034352  [710100/1106938]\n",
      "loss: 1.060050  [720100/1106938]\n",
      "loss: 1.002504  [730100/1106938]\n",
      "loss: 0.827641  [740100/1106938]\n",
      "loss: 1.155449  [750100/1106938]\n",
      "loss: 1.020124  [760100/1106938]\n",
      "loss: 0.941154  [770100/1106938]\n",
      "loss: 0.811162  [780100/1106938]\n",
      "loss: 1.137940  [790100/1106938]\n",
      "loss: 1.145067  [800100/1106938]\n",
      "loss: 0.865262  [810100/1106938]\n",
      "loss: 1.194498  [820100/1106938]\n",
      "loss: 1.035097  [830100/1106938]\n",
      "loss: 0.781448  [840100/1106938]\n",
      "loss: 1.125926  [850100/1106938]\n",
      "loss: 1.214636  [860100/1106938]\n",
      "loss: 1.082002  [870100/1106938]\n",
      "loss: 1.019551  [880100/1106938]\n",
      "loss: 1.044859  [890100/1106938]\n",
      "loss: 1.207404  [900100/1106938]\n",
      "loss: 1.105680  [910100/1106938]\n",
      "loss: 0.770598  [920100/1106938]\n",
      "loss: 0.933775  [930100/1106938]\n",
      "loss: 0.984313  [940100/1106938]\n",
      "loss: 0.972811  [950100/1106938]\n",
      "loss: 0.860425  [960100/1106938]\n",
      "loss: 0.947480  [970100/1106938]\n",
      "loss: 1.023719  [980100/1106938]\n",
      "loss: 0.944707  [990100/1106938]\n",
      "loss: 1.152758  [1000100/1106938]\n",
      "loss: 0.985112  [1010100/1106938]\n",
      "loss: 0.951804  [1020100/1106938]\n",
      "loss: 0.947148  [1030100/1106938]\n",
      "loss: 0.926291  [1040100/1106938]\n",
      "loss: 1.203259  [1050100/1106938]\n",
      "loss: 1.120565  [1060100/1106938]\n",
      "loss: 0.895998  [1070100/1106938]\n",
      "loss: 0.888424  [1080100/1106938]\n",
      "loss: 1.019854  [1090100/1106938]\n",
      "loss: 1.082312  [1100100/1106938]\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 0.925327 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.087717  [  100/1106938]\n",
      "loss: 1.121263  [10100/1106938]\n",
      "loss: 1.238069  [20100/1106938]\n",
      "loss: 0.953353  [30100/1106938]\n",
      "loss: 1.117522  [40100/1106938]\n",
      "loss: 0.936239  [50100/1106938]\n",
      "loss: 1.014971  [60100/1106938]\n",
      "loss: 1.089564  [70100/1106938]\n",
      "loss: 0.949541  [80100/1106938]\n",
      "loss: 1.072155  [90100/1106938]\n",
      "loss: 0.901490  [100100/1106938]\n",
      "loss: 1.113401  [110100/1106938]\n",
      "loss: 0.968136  [120100/1106938]\n",
      "loss: 0.999675  [130100/1106938]\n",
      "loss: 0.841279  [140100/1106938]\n",
      "loss: 0.824750  [150100/1106938]\n",
      "loss: 0.968656  [160100/1106938]\n",
      "loss: 0.970164  [170100/1106938]\n",
      "loss: 0.871166  [180100/1106938]\n",
      "loss: 1.120034  [190100/1106938]\n",
      "loss: 1.141910  [200100/1106938]\n",
      "loss: 0.929960  [210100/1106938]\n",
      "loss: 0.913438  [220100/1106938]\n",
      "loss: 0.885303  [230100/1106938]\n",
      "loss: 1.009540  [240100/1106938]\n",
      "loss: 1.050180  [250100/1106938]\n",
      "loss: 1.232058  [260100/1106938]\n",
      "loss: 1.031941  [270100/1106938]\n",
      "loss: 1.179077  [280100/1106938]\n",
      "loss: 1.123107  [290100/1106938]\n",
      "loss: 0.952590  [300100/1106938]\n",
      "loss: 0.988446  [310100/1106938]\n",
      "loss: 1.073101  [320100/1106938]\n",
      "loss: 1.014518  [330100/1106938]\n",
      "loss: 0.816410  [340100/1106938]\n",
      "loss: 1.132277  [350100/1106938]\n",
      "loss: 1.008019  [360100/1106938]\n",
      "loss: 0.881150  [370100/1106938]\n",
      "loss: 0.979765  [380100/1106938]\n",
      "loss: 0.843467  [390100/1106938]\n",
      "loss: 0.929783  [400100/1106938]\n",
      "loss: 0.910583  [410100/1106938]\n",
      "loss: 1.020207  [420100/1106938]\n",
      "loss: 1.017269  [430100/1106938]\n",
      "loss: 1.131777  [440100/1106938]\n",
      "loss: 1.067311  [450100/1106938]\n",
      "loss: 1.203268  [460100/1106938]\n",
      "loss: 1.225472  [470100/1106938]\n",
      "loss: 0.973154  [480100/1106938]\n",
      "loss: 1.063720  [490100/1106938]\n",
      "loss: 1.026429  [500100/1106938]\n",
      "loss: 0.945013  [510100/1106938]\n",
      "loss: 0.827996  [520100/1106938]\n",
      "loss: 1.050675  [530100/1106938]\n",
      "loss: 1.109766  [540100/1106938]\n",
      "loss: 0.909517  [550100/1106938]\n",
      "loss: 1.302320  [560100/1106938]\n",
      "loss: 0.982940  [570100/1106938]\n",
      "loss: 0.978738  [580100/1106938]\n",
      "loss: 1.035834  [590100/1106938]\n",
      "loss: 0.973347  [600100/1106938]\n",
      "loss: 1.054878  [610100/1106938]\n",
      "loss: 1.267154  [620100/1106938]\n",
      "loss: 1.024897  [630100/1106938]\n",
      "loss: 1.099030  [640100/1106938]\n",
      "loss: 0.982448  [650100/1106938]\n",
      "loss: 1.281703  [660100/1106938]\n",
      "loss: 0.945686  [670100/1106938]\n",
      "loss: 0.851151  [680100/1106938]\n",
      "loss: 1.157486  [690100/1106938]\n",
      "loss: 0.890456  [700100/1106938]\n",
      "loss: 1.138479  [710100/1106938]\n",
      "loss: 1.043535  [720100/1106938]\n",
      "loss: 0.928746  [730100/1106938]\n",
      "loss: 0.977895  [740100/1106938]\n",
      "loss: 0.998870  [750100/1106938]\n",
      "loss: 0.924154  [760100/1106938]\n",
      "loss: 0.926687  [770100/1106938]\n",
      "loss: 0.980500  [780100/1106938]\n",
      "loss: 1.114992  [790100/1106938]\n",
      "loss: 1.014558  [800100/1106938]\n",
      "loss: 1.004374  [810100/1106938]\n",
      "loss: 1.035518  [820100/1106938]\n",
      "loss: 1.161035  [830100/1106938]\n",
      "loss: 0.869527  [840100/1106938]\n",
      "loss: 0.883210  [850100/1106938]\n",
      "loss: 1.043951  [860100/1106938]\n",
      "loss: 0.979796  [870100/1106938]\n",
      "loss: 1.052612  [880100/1106938]\n",
      "loss: 0.768972  [890100/1106938]\n",
      "loss: 0.948093  [900100/1106938]\n",
      "loss: 0.913647  [910100/1106938]\n",
      "loss: 0.968947  [920100/1106938]\n",
      "loss: 1.056112  [930100/1106938]\n",
      "loss: 1.210334  [940100/1106938]\n",
      "loss: 1.026851  [950100/1106938]\n",
      "loss: 1.228769  [960100/1106938]\n",
      "loss: 1.115322  [970100/1106938]\n",
      "loss: 0.960209  [980100/1106938]\n",
      "loss: 1.098907  [990100/1106938]\n",
      "loss: 0.876491  [1000100/1106938]\n",
      "loss: 0.998933  [1010100/1106938]\n",
      "loss: 0.919932  [1020100/1106938]\n",
      "loss: 0.747136  [1030100/1106938]\n",
      "loss: 1.003601  [1040100/1106938]\n",
      "loss: 1.027328  [1050100/1106938]\n",
      "loss: 1.178176  [1060100/1106938]\n",
      "loss: 1.188337  [1070100/1106938]\n",
      "loss: 1.222624  [1080100/1106938]\n",
      "loss: 0.892136  [1090100/1106938]\n",
      "loss: 0.877717  [1100100/1106938]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 0.884406 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.058618  [  100/1106938]\n",
      "loss: 0.876580  [10100/1106938]\n",
      "loss: 0.871431  [20100/1106938]\n",
      "loss: 0.815971  [30100/1106938]\n",
      "loss: 1.073135  [40100/1106938]\n",
      "loss: 0.794758  [50100/1106938]\n",
      "loss: 0.876629  [60100/1106938]\n",
      "loss: 0.693421  [70100/1106938]\n",
      "loss: 1.054035  [80100/1106938]\n",
      "loss: 1.003955  [90100/1106938]\n",
      "loss: 1.097556  [100100/1106938]\n",
      "loss: 0.817859  [110100/1106938]\n",
      "loss: 1.030387  [120100/1106938]\n",
      "loss: 0.904879  [130100/1106938]\n",
      "loss: 1.053192  [140100/1106938]\n",
      "loss: 0.976336  [150100/1106938]\n",
      "loss: 0.905995  [160100/1106938]\n",
      "loss: 0.887521  [170100/1106938]\n",
      "loss: 1.186830  [180100/1106938]\n",
      "loss: 0.869048  [190100/1106938]\n",
      "loss: 0.713929  [200100/1106938]\n",
      "loss: 1.080163  [210100/1106938]\n",
      "loss: 1.036865  [220100/1106938]\n",
      "loss: 0.716133  [230100/1106938]\n",
      "loss: 0.927325  [240100/1106938]\n",
      "loss: 1.040734  [250100/1106938]\n",
      "loss: 0.989338  [260100/1106938]\n",
      "loss: 1.207619  [270100/1106938]\n",
      "loss: 1.136074  [280100/1106938]\n",
      "loss: 0.978939  [290100/1106938]\n",
      "loss: 1.045885  [300100/1106938]\n",
      "loss: 1.171893  [310100/1106938]\n",
      "loss: 0.816903  [320100/1106938]\n",
      "loss: 1.082546  [330100/1106938]\n",
      "loss: 0.821744  [340100/1106938]\n",
      "loss: 1.149564  [350100/1106938]\n",
      "loss: 1.105571  [360100/1106938]\n",
      "loss: 1.112192  [370100/1106938]\n",
      "loss: 0.822905  [380100/1106938]\n",
      "loss: 1.054366  [390100/1106938]\n",
      "loss: 0.911974  [400100/1106938]\n",
      "loss: 1.131779  [410100/1106938]\n",
      "loss: 0.952102  [420100/1106938]\n",
      "loss: 1.085018  [430100/1106938]\n",
      "loss: 0.671607  [440100/1106938]\n",
      "loss: 0.904284  [450100/1106938]\n",
      "loss: 1.057906  [460100/1106938]\n",
      "loss: 0.918180  [470100/1106938]\n",
      "loss: 0.923399  [480100/1106938]\n",
      "loss: 1.021725  [490100/1106938]\n",
      "loss: 0.921044  [500100/1106938]\n",
      "loss: 1.292607  [510100/1106938]\n",
      "loss: 1.013159  [520100/1106938]\n",
      "loss: 0.975944  [530100/1106938]\n",
      "loss: 1.136947  [540100/1106938]\n",
      "loss: 1.271936  [550100/1106938]\n",
      "loss: 0.879421  [560100/1106938]\n",
      "loss: 0.743885  [570100/1106938]\n",
      "loss: 0.971561  [580100/1106938]\n",
      "loss: 0.936644  [590100/1106938]\n",
      "loss: 0.878912  [600100/1106938]\n",
      "loss: 0.979352  [610100/1106938]\n",
      "loss: 0.961701  [620100/1106938]\n",
      "loss: 1.157529  [630100/1106938]\n",
      "loss: 0.913692  [640100/1106938]\n",
      "loss: 0.987293  [650100/1106938]\n",
      "loss: 0.744237  [660100/1106938]\n",
      "loss: 0.904656  [670100/1106938]\n",
      "loss: 1.053194  [680100/1106938]\n",
      "loss: 0.764465  [690100/1106938]\n",
      "loss: 1.157536  [700100/1106938]\n",
      "loss: 0.900499  [710100/1106938]\n",
      "loss: 0.987484  [720100/1106938]\n",
      "loss: 1.142273  [730100/1106938]\n",
      "loss: 1.004175  [740100/1106938]\n",
      "loss: 1.025864  [750100/1106938]\n",
      "loss: 0.884811  [760100/1106938]\n",
      "loss: 0.875638  [770100/1106938]\n",
      "loss: 1.078628  [780100/1106938]\n",
      "loss: 0.923005  [790100/1106938]\n",
      "loss: 1.039601  [800100/1106938]\n",
      "loss: 1.032786  [810100/1106938]\n",
      "loss: 0.957179  [820100/1106938]\n",
      "loss: 1.063290  [830100/1106938]\n",
      "loss: 1.002970  [840100/1106938]\n",
      "loss: 0.822877  [850100/1106938]\n",
      "loss: 0.986283  [860100/1106938]\n",
      "loss: 1.027395  [870100/1106938]\n",
      "loss: 0.877983  [880100/1106938]\n",
      "loss: 1.192894  [890100/1106938]\n",
      "loss: 0.967730  [900100/1106938]\n",
      "loss: 0.759871  [910100/1106938]\n",
      "loss: 1.124956  [920100/1106938]\n",
      "loss: 0.967512  [930100/1106938]\n",
      "loss: 0.884418  [940100/1106938]\n",
      "loss: 0.986114  [950100/1106938]\n",
      "loss: 0.928476  [960100/1106938]\n",
      "loss: 1.022664  [970100/1106938]\n",
      "loss: 0.759744  [980100/1106938]\n",
      "loss: 0.881970  [990100/1106938]\n",
      "loss: 0.746812  [1000100/1106938]\n",
      "loss: 0.971901  [1010100/1106938]\n",
      "loss: 0.871606  [1020100/1106938]\n",
      "loss: 0.857953  [1030100/1106938]\n",
      "loss: 1.041844  [1040100/1106938]\n",
      "loss: 1.151878  [1050100/1106938]\n",
      "loss: 1.037645  [1060100/1106938]\n",
      "loss: 0.875866  [1070100/1106938]\n",
      "loss: 0.948754  [1080100/1106938]\n",
      "loss: 0.881026  [1090100/1106938]\n",
      "loss: 0.968215  [1100100/1106938]\n",
      "Test Error: \n",
      " Accuracy: 71.7%, Avg loss: 0.850921 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.979352  [  100/1106938]\n",
      "loss: 0.960243  [10100/1106938]\n",
      "loss: 0.944841  [20100/1106938]\n",
      "loss: 0.674557  [30100/1106938]\n",
      "loss: 0.798584  [40100/1106938]\n",
      "loss: 0.946282  [50100/1106938]\n",
      "loss: 1.018537  [60100/1106938]\n",
      "loss: 0.908068  [70100/1106938]\n",
      "loss: 0.868187  [80100/1106938]\n",
      "loss: 0.856637  [90100/1106938]\n",
      "loss: 0.901745  [100100/1106938]\n",
      "loss: 0.753647  [110100/1106938]\n",
      "loss: 0.819009  [120100/1106938]\n",
      "loss: 0.953788  [130100/1106938]\n",
      "loss: 0.988012  [140100/1106938]\n",
      "loss: 0.764769  [150100/1106938]\n",
      "loss: 0.815288  [160100/1106938]\n",
      "loss: 0.908728  [170100/1106938]\n",
      "loss: 1.065000  [180100/1106938]\n",
      "loss: 0.786627  [190100/1106938]\n",
      "loss: 0.908119  [200100/1106938]\n",
      "loss: 0.875776  [210100/1106938]\n",
      "loss: 0.706771  [220100/1106938]\n",
      "loss: 0.822611  [230100/1106938]\n",
      "loss: 0.918658  [240100/1106938]\n",
      "loss: 1.142262  [250100/1106938]\n",
      "loss: 1.202412  [260100/1106938]\n",
      "loss: 1.112889  [270100/1106938]\n",
      "loss: 0.853642  [280100/1106938]\n",
      "loss: 0.839012  [290100/1106938]\n",
      "loss: 0.736293  [300100/1106938]\n",
      "loss: 0.949493  [310100/1106938]\n",
      "loss: 0.975242  [320100/1106938]\n",
      "loss: 0.890416  [330100/1106938]\n",
      "loss: 0.898998  [340100/1106938]\n",
      "loss: 1.090268  [350100/1106938]\n",
      "loss: 0.954223  [360100/1106938]\n",
      "loss: 1.070688  [370100/1106938]\n",
      "loss: 0.916853  [380100/1106938]\n",
      "loss: 0.956431  [390100/1106938]\n",
      "loss: 0.956659  [400100/1106938]\n",
      "loss: 0.981182  [410100/1106938]\n",
      "loss: 0.866169  [420100/1106938]\n",
      "loss: 1.039480  [430100/1106938]\n",
      "loss: 0.877691  [440100/1106938]\n",
      "loss: 0.990240  [450100/1106938]\n",
      "loss: 0.738612  [460100/1106938]\n",
      "loss: 1.033354  [470100/1106938]\n",
      "loss: 0.766496  [480100/1106938]\n",
      "loss: 0.909434  [490100/1106938]\n",
      "loss: 0.983694  [500100/1106938]\n",
      "loss: 1.057158  [510100/1106938]\n",
      "loss: 0.926776  [520100/1106938]\n",
      "loss: 0.749538  [530100/1106938]\n",
      "loss: 0.865283  [540100/1106938]\n",
      "loss: 0.825485  [550100/1106938]\n",
      "loss: 0.882889  [560100/1106938]\n",
      "loss: 0.918519  [570100/1106938]\n",
      "loss: 0.778645  [580100/1106938]\n",
      "loss: 0.812761  [590100/1106938]\n",
      "loss: 0.998127  [600100/1106938]\n",
      "loss: 0.926984  [610100/1106938]\n",
      "loss: 0.790044  [620100/1106938]\n",
      "loss: 1.126903  [630100/1106938]\n",
      "loss: 1.102275  [640100/1106938]\n",
      "loss: 0.908615  [650100/1106938]\n",
      "loss: 0.797696  [660100/1106938]\n",
      "loss: 1.038210  [670100/1106938]\n",
      "loss: 1.001711  [680100/1106938]\n",
      "loss: 0.941342  [690100/1106938]\n",
      "loss: 0.937245  [700100/1106938]\n",
      "loss: 0.870118  [710100/1106938]\n",
      "loss: 0.901642  [720100/1106938]\n",
      "loss: 0.928260  [730100/1106938]\n",
      "loss: 0.971309  [740100/1106938]\n",
      "loss: 1.026630  [750100/1106938]\n",
      "loss: 0.895221  [760100/1106938]\n",
      "loss: 0.798135  [770100/1106938]\n",
      "loss: 0.834361  [780100/1106938]\n",
      "loss: 1.069872  [790100/1106938]\n",
      "loss: 0.917528  [800100/1106938]\n",
      "loss: 0.832484  [810100/1106938]\n",
      "loss: 0.689986  [820100/1106938]\n",
      "loss: 0.898194  [830100/1106938]\n",
      "loss: 1.048949  [840100/1106938]\n",
      "loss: 0.913293  [850100/1106938]\n",
      "loss: 1.033783  [860100/1106938]\n",
      "loss: 1.023033  [870100/1106938]\n",
      "loss: 0.856604  [880100/1106938]\n",
      "loss: 0.960720  [890100/1106938]\n",
      "loss: 1.035489  [900100/1106938]\n",
      "loss: 0.654087  [910100/1106938]\n",
      "loss: 1.096786  [920100/1106938]\n",
      "loss: 0.826510  [930100/1106938]\n",
      "loss: 0.781325  [940100/1106938]\n",
      "loss: 0.974111  [950100/1106938]\n",
      "loss: 0.834321  [960100/1106938]\n",
      "loss: 1.209943  [970100/1106938]\n",
      "loss: 0.705031  [980100/1106938]\n",
      "loss: 1.118917  [990100/1106938]\n",
      "loss: 0.877231  [1000100/1106938]\n",
      "loss: 1.273665  [1010100/1106938]\n",
      "loss: 0.951861  [1020100/1106938]\n",
      "loss: 1.046255  [1030100/1106938]\n",
      "loss: 1.001585  [1040100/1106938]\n",
      "loss: 0.946811  [1050100/1106938]\n",
      "loss: 0.830595  [1060100/1106938]\n",
      "loss: 0.897987  [1070100/1106938]\n",
      "loss: 0.942164  [1080100/1106938]\n",
      "loss: 0.864310  [1090100/1106938]\n",
      "loss: 1.034100  [1100100/1106938]\n",
      "Test Error: \n",
      " Accuracy: 72.6%, Avg loss: 0.824931 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.022370  [  100/1106938]\n",
      "loss: 0.850952  [10100/1106938]\n",
      "loss: 0.847856  [20100/1106938]\n",
      "loss: 1.006276  [30100/1106938]\n",
      "loss: 0.860584  [40100/1106938]\n",
      "loss: 1.062854  [50100/1106938]\n",
      "loss: 0.703998  [60100/1106938]\n",
      "loss: 0.630693  [70100/1106938]\n",
      "loss: 1.007765  [80100/1106938]\n",
      "loss: 0.886752  [90100/1106938]\n",
      "loss: 0.822584  [100100/1106938]\n",
      "loss: 0.837229  [110100/1106938]\n",
      "loss: 0.938516  [120100/1106938]\n",
      "loss: 0.700937  [130100/1106938]\n",
      "loss: 0.923475  [140100/1106938]\n",
      "loss: 0.947354  [150100/1106938]\n",
      "loss: 0.840611  [160100/1106938]\n",
      "loss: 0.941749  [170100/1106938]\n",
      "loss: 0.745826  [180100/1106938]\n",
      "loss: 0.888752  [190100/1106938]\n",
      "loss: 0.963541  [200100/1106938]\n",
      "loss: 1.032270  [210100/1106938]\n",
      "loss: 0.966304  [220100/1106938]\n",
      "loss: 1.030327  [230100/1106938]\n",
      "loss: 0.836446  [240100/1106938]\n",
      "loss: 0.899281  [250100/1106938]\n",
      "loss: 0.666793  [260100/1106938]\n",
      "loss: 0.784460  [270100/1106938]\n",
      "loss: 0.835210  [280100/1106938]\n",
      "loss: 0.949320  [290100/1106938]\n",
      "loss: 0.904019  [300100/1106938]\n",
      "loss: 1.023558  [310100/1106938]\n",
      "loss: 0.819797  [320100/1106938]\n",
      "loss: 0.693722  [330100/1106938]\n",
      "loss: 0.822557  [340100/1106938]\n",
      "loss: 1.040852  [350100/1106938]\n",
      "loss: 0.879163  [360100/1106938]\n",
      "loss: 1.053402  [370100/1106938]\n",
      "loss: 0.765569  [380100/1106938]\n",
      "loss: 1.007545  [390100/1106938]\n",
      "loss: 0.650633  [400100/1106938]\n",
      "loss: 0.926164  [410100/1106938]\n",
      "loss: 0.968263  [420100/1106938]\n",
      "loss: 0.630935  [430100/1106938]\n",
      "loss: 1.185676  [440100/1106938]\n",
      "loss: 0.868587  [450100/1106938]\n",
      "loss: 0.847362  [460100/1106938]\n",
      "loss: 1.033804  [470100/1106938]\n",
      "loss: 0.760912  [480100/1106938]\n",
      "loss: 0.776448  [490100/1106938]\n",
      "loss: 1.013254  [500100/1106938]\n",
      "loss: 0.865528  [510100/1106938]\n",
      "loss: 1.019223  [520100/1106938]\n",
      "loss: 0.954118  [530100/1106938]\n",
      "loss: 0.579608  [540100/1106938]\n",
      "loss: 0.930722  [550100/1106938]\n",
      "loss: 1.112456  [560100/1106938]\n",
      "loss: 0.978316  [570100/1106938]\n",
      "loss: 0.922021  [580100/1106938]\n",
      "loss: 1.154326  [590100/1106938]\n",
      "loss: 0.968854  [600100/1106938]\n",
      "loss: 0.797549  [610100/1106938]\n",
      "loss: 1.056094  [620100/1106938]\n",
      "loss: 0.783110  [630100/1106938]\n",
      "loss: 0.718355  [640100/1106938]\n",
      "loss: 0.928400  [650100/1106938]\n",
      "loss: 0.848825  [660100/1106938]\n",
      "loss: 0.977113  [670100/1106938]\n",
      "loss: 0.847278  [680100/1106938]\n",
      "loss: 0.811764  [690100/1106938]\n",
      "loss: 0.953540  [700100/1106938]\n",
      "loss: 1.053694  [710100/1106938]\n",
      "loss: 0.880242  [720100/1106938]\n",
      "loss: 0.928008  [730100/1106938]\n",
      "loss: 1.047184  [740100/1106938]\n",
      "loss: 0.891144  [750100/1106938]\n",
      "loss: 0.968622  [760100/1106938]\n",
      "loss: 1.107478  [770100/1106938]\n",
      "loss: 0.827830  [780100/1106938]\n",
      "loss: 0.844613  [790100/1106938]\n",
      "loss: 0.955337  [800100/1106938]\n",
      "loss: 0.909903  [810100/1106938]\n",
      "loss: 0.982189  [820100/1106938]\n",
      "loss: 1.182636  [830100/1106938]\n",
      "loss: 1.162316  [840100/1106938]\n",
      "loss: 0.910406  [850100/1106938]\n",
      "loss: 0.917297  [860100/1106938]\n",
      "loss: 0.846620  [870100/1106938]\n",
      "loss: 0.783913  [880100/1106938]\n",
      "loss: 1.034691  [890100/1106938]\n",
      "loss: 0.821823  [900100/1106938]\n",
      "loss: 0.926451  [910100/1106938]\n",
      "loss: 1.313746  [920100/1106938]\n",
      "loss: 0.897464  [930100/1106938]\n",
      "loss: 0.755473  [940100/1106938]\n",
      "loss: 0.818890  [950100/1106938]\n",
      "loss: 0.833888  [960100/1106938]\n",
      "loss: 0.675845  [970100/1106938]\n",
      "loss: 0.897889  [980100/1106938]\n",
      "loss: 0.910471  [990100/1106938]\n",
      "loss: 0.768843  [1000100/1106938]\n",
      "loss: 0.920187  [1010100/1106938]\n",
      "loss: 1.106185  [1020100/1106938]\n",
      "loss: 1.036876  [1030100/1106938]\n",
      "loss: 1.214715  [1040100/1106938]\n",
      "loss: 1.062066  [1050100/1106938]\n",
      "loss: 0.782438  [1060100/1106938]\n",
      "loss: 0.930873  [1070100/1106938]\n",
      "loss: 0.767537  [1080100/1106938]\n",
      "loss: 0.870758  [1090100/1106938]\n",
      "loss: 0.894528  [1100100/1106938]\n",
      "Test Error: \n",
      " Accuracy: 72.9%, Avg loss: 0.812562 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.871664  [  100/1106938]\n",
      "loss: 1.025773  [10100/1106938]\n",
      "loss: 0.729058  [20100/1106938]\n",
      "loss: 0.777938  [30100/1106938]\n",
      "loss: 1.237534  [40100/1106938]\n",
      "loss: 1.056091  [50100/1106938]\n",
      "loss: 0.829557  [60100/1106938]\n",
      "loss: 1.062139  [70100/1106938]\n",
      "loss: 0.873173  [80100/1106938]\n",
      "loss: 1.033746  [90100/1106938]\n",
      "loss: 0.855179  [100100/1106938]\n",
      "loss: 0.754017  [110100/1106938]\n",
      "loss: 0.781510  [120100/1106938]\n",
      "loss: 0.696969  [130100/1106938]\n",
      "loss: 0.803287  [140100/1106938]\n",
      "loss: 0.970039  [150100/1106938]\n",
      "loss: 0.848184  [160100/1106938]\n",
      "loss: 0.841761  [170100/1106938]\n",
      "loss: 1.001053  [180100/1106938]\n",
      "loss: 0.948013  [190100/1106938]\n",
      "loss: 0.804780  [200100/1106938]\n",
      "loss: 0.899263  [210100/1106938]\n",
      "loss: 0.861269  [220100/1106938]\n",
      "loss: 0.897651  [230100/1106938]\n",
      "loss: 0.849315  [240100/1106938]\n",
      "loss: 0.930331  [250100/1106938]\n",
      "loss: 0.814718  [260100/1106938]\n",
      "loss: 1.099093  [270100/1106938]\n",
      "loss: 1.000829  [280100/1106938]\n",
      "loss: 0.800003  [290100/1106938]\n",
      "loss: 1.081407  [300100/1106938]\n",
      "loss: 0.860288  [310100/1106938]\n",
      "loss: 0.875037  [320100/1106938]\n",
      "loss: 0.957180  [330100/1106938]\n",
      "loss: 0.792471  [340100/1106938]\n",
      "loss: 0.920291  [350100/1106938]\n",
      "loss: 0.842824  [360100/1106938]\n",
      "loss: 1.023561  [370100/1106938]\n",
      "loss: 0.901918  [380100/1106938]\n",
      "loss: 0.860309  [390100/1106938]\n",
      "loss: 1.012844  [400100/1106938]\n",
      "loss: 0.823680  [410100/1106938]\n",
      "loss: 0.858527  [420100/1106938]\n",
      "loss: 1.073251  [430100/1106938]\n",
      "loss: 0.924142  [440100/1106938]\n",
      "loss: 0.829203  [450100/1106938]\n",
      "loss: 0.747291  [460100/1106938]\n",
      "loss: 0.932374  [470100/1106938]\n",
      "loss: 0.861281  [480100/1106938]\n",
      "loss: 1.081676  [490100/1106938]\n",
      "loss: 0.981614  [500100/1106938]\n",
      "loss: 0.944490  [510100/1106938]\n",
      "loss: 0.888702  [520100/1106938]\n",
      "loss: 0.836113  [530100/1106938]\n",
      "loss: 0.778160  [540100/1106938]\n",
      "loss: 0.988173  [550100/1106938]\n",
      "loss: 0.968784  [560100/1106938]\n",
      "loss: 0.917323  [570100/1106938]\n",
      "loss: 0.689227  [580100/1106938]\n",
      "loss: 0.990330  [590100/1106938]\n",
      "loss: 0.798153  [600100/1106938]\n",
      "loss: 0.973802  [610100/1106938]\n",
      "loss: 0.821166  [620100/1106938]\n",
      "loss: 0.892718  [630100/1106938]\n",
      "loss: 0.859156  [640100/1106938]\n",
      "loss: 0.886801  [650100/1106938]\n",
      "loss: 0.773787  [660100/1106938]\n",
      "loss: 1.064415  [670100/1106938]\n",
      "loss: 0.927799  [680100/1106938]\n",
      "loss: 0.962868  [690100/1106938]\n",
      "loss: 0.923726  [700100/1106938]\n",
      "loss: 0.973859  [710100/1106938]\n",
      "loss: 0.852271  [720100/1106938]\n",
      "loss: 0.735342  [730100/1106938]\n",
      "loss: 0.610569  [740100/1106938]\n",
      "loss: 0.666266  [750100/1106938]\n",
      "loss: 0.768146  [760100/1106938]\n",
      "loss: 0.889193  [770100/1106938]\n",
      "loss: 0.962173  [780100/1106938]\n",
      "loss: 0.988322  [790100/1106938]\n",
      "loss: 0.959540  [800100/1106938]\n",
      "loss: 0.739056  [810100/1106938]\n",
      "loss: 0.903918  [820100/1106938]\n",
      "loss: 0.848143  [830100/1106938]\n",
      "loss: 0.675559  [840100/1106938]\n",
      "loss: 0.992111  [850100/1106938]\n",
      "loss: 0.923917  [860100/1106938]\n",
      "loss: 1.106666  [870100/1106938]\n",
      "loss: 1.004083  [880100/1106938]\n",
      "loss: 0.962205  [890100/1106938]\n",
      "loss: 0.747175  [900100/1106938]\n",
      "loss: 0.837578  [910100/1106938]\n",
      "loss: 0.771385  [920100/1106938]\n",
      "loss: 0.779677  [930100/1106938]\n",
      "loss: 0.719655  [940100/1106938]\n",
      "loss: 1.096604  [950100/1106938]\n",
      "loss: 0.785223  [960100/1106938]\n",
      "loss: 1.034522  [970100/1106938]\n",
      "loss: 0.985113  [980100/1106938]\n",
      "loss: 0.802049  [990100/1106938]\n",
      "loss: 0.945826  [1000100/1106938]\n",
      "loss: 0.753004  [1010100/1106938]\n",
      "loss: 0.930314  [1020100/1106938]\n",
      "loss: 0.972322  [1030100/1106938]\n",
      "loss: 0.879347  [1040100/1106938]\n",
      "loss: 0.962400  [1050100/1106938]\n",
      "loss: 0.689654  [1060100/1106938]\n",
      "loss: 0.746909  [1070100/1106938]\n",
      "loss: 0.922219  [1080100/1106938]\n",
      "loss: 0.740343  [1090100/1106938]\n",
      "loss: 0.728360  [1100100/1106938]\n",
      "Test Error: \n",
      " Accuracy: 73.5%, Avg loss: 0.792473 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.960246  [  100/1106938]\n",
      "loss: 0.820595  [10100/1106938]\n",
      "loss: 0.613537  [20100/1106938]\n",
      "loss: 1.141590  [30100/1106938]\n",
      "loss: 1.039467  [40100/1106938]\n",
      "loss: 0.887691  [50100/1106938]\n",
      "loss: 0.752931  [60100/1106938]\n",
      "loss: 1.004490  [70100/1106938]\n",
      "loss: 0.721217  [80100/1106938]\n",
      "loss: 1.061809  [90100/1106938]\n",
      "loss: 1.091515  [100100/1106938]\n",
      "loss: 0.922252  [110100/1106938]\n",
      "loss: 0.750716  [120100/1106938]\n",
      "loss: 0.803031  [130100/1106938]\n",
      "loss: 0.663994  [140100/1106938]\n",
      "loss: 0.729921  [150100/1106938]\n",
      "loss: 1.184180  [160100/1106938]\n",
      "loss: 1.013011  [170100/1106938]\n",
      "loss: 0.736728  [180100/1106938]\n",
      "loss: 0.856115  [190100/1106938]\n",
      "loss: 0.807734  [200100/1106938]\n",
      "loss: 0.728490  [210100/1106938]\n",
      "loss: 0.692336  [220100/1106938]\n",
      "loss: 0.990871  [230100/1106938]\n",
      "loss: 0.769702  [240100/1106938]\n",
      "loss: 0.858965  [250100/1106938]\n",
      "loss: 0.892591  [260100/1106938]\n",
      "loss: 0.863959  [270100/1106938]\n",
      "loss: 0.782860  [280100/1106938]\n",
      "loss: 1.001129  [290100/1106938]\n",
      "loss: 1.033372  [300100/1106938]\n",
      "loss: 0.741680  [310100/1106938]\n",
      "loss: 0.787619  [320100/1106938]\n",
      "loss: 0.838360  [330100/1106938]\n",
      "loss: 0.786529  [340100/1106938]\n",
      "loss: 0.739435  [350100/1106938]\n",
      "loss: 0.752581  [360100/1106938]\n",
      "loss: 0.914146  [370100/1106938]\n",
      "loss: 0.955245  [380100/1106938]\n",
      "loss: 0.771978  [390100/1106938]\n",
      "loss: 0.928972  [400100/1106938]\n",
      "loss: 0.964061  [410100/1106938]\n",
      "loss: 0.887151  [420100/1106938]\n",
      "loss: 0.772134  [430100/1106938]\n",
      "loss: 1.071028  [440100/1106938]\n",
      "loss: 0.893205  [450100/1106938]\n",
      "loss: 0.967836  [460100/1106938]\n",
      "loss: 0.914820  [470100/1106938]\n",
      "loss: 0.800495  [480100/1106938]\n",
      "loss: 0.682818  [490100/1106938]\n",
      "loss: 0.788266  [500100/1106938]\n",
      "loss: 0.906692  [510100/1106938]\n",
      "loss: 0.748363  [520100/1106938]\n",
      "loss: 0.912634  [530100/1106938]\n",
      "loss: 0.889043  [540100/1106938]\n",
      "loss: 0.852718  [550100/1106938]\n",
      "loss: 0.931404  [560100/1106938]\n",
      "loss: 1.063920  [570100/1106938]\n",
      "loss: 0.894682  [580100/1106938]\n",
      "loss: 0.846605  [590100/1106938]\n",
      "loss: 0.812643  [600100/1106938]\n",
      "loss: 0.797127  [610100/1106938]\n",
      "loss: 0.794292  [620100/1106938]\n",
      "loss: 0.905915  [630100/1106938]\n",
      "loss: 0.915984  [640100/1106938]\n",
      "loss: 0.820435  [650100/1106938]\n",
      "loss: 0.808498  [660100/1106938]\n",
      "loss: 0.989140  [670100/1106938]\n",
      "loss: 0.577761  [680100/1106938]\n",
      "loss: 0.888999  [690100/1106938]\n",
      "loss: 0.922355  [700100/1106938]\n",
      "loss: 0.855956  [710100/1106938]\n",
      "loss: 0.822701  [720100/1106938]\n",
      "loss: 0.839106  [730100/1106938]\n",
      "loss: 0.910843  [740100/1106938]\n",
      "loss: 0.898144  [750100/1106938]\n",
      "loss: 0.869707  [760100/1106938]\n",
      "loss: 0.932063  [770100/1106938]\n",
      "loss: 1.058714  [780100/1106938]\n",
      "loss: 0.854613  [790100/1106938]\n",
      "loss: 0.919228  [800100/1106938]\n",
      "loss: 0.815053  [810100/1106938]\n",
      "loss: 0.915886  [820100/1106938]\n",
      "loss: 0.770339  [830100/1106938]\n",
      "loss: 0.892639  [840100/1106938]\n",
      "loss: 0.994240  [850100/1106938]\n",
      "loss: 1.007403  [860100/1106938]\n",
      "loss: 0.868931  [870100/1106938]\n",
      "loss: 0.928724  [880100/1106938]\n",
      "loss: 0.761625  [890100/1106938]\n",
      "loss: 0.783572  [900100/1106938]\n",
      "loss: 0.759683  [910100/1106938]\n",
      "loss: 1.037404  [920100/1106938]\n",
      "loss: 0.827533  [930100/1106938]\n",
      "loss: 0.852347  [940100/1106938]\n",
      "loss: 0.902694  [950100/1106938]\n",
      "loss: 0.820994  [960100/1106938]\n",
      "loss: 0.827959  [970100/1106938]\n",
      "loss: 0.837004  [980100/1106938]\n",
      "loss: 0.991302  [990100/1106938]\n",
      "loss: 1.065168  [1000100/1106938]\n",
      "loss: 0.956762  [1010100/1106938]\n",
      "loss: 0.851457  [1020100/1106938]\n",
      "loss: 0.896015  [1030100/1106938]\n",
      "loss: 0.806226  [1040100/1106938]\n",
      "loss: 0.841717  [1050100/1106938]\n",
      "loss: 0.758002  [1060100/1106938]\n",
      "loss: 0.902117  [1070100/1106938]\n",
      "loss: 1.107532  [1080100/1106938]\n",
      "loss: 0.995605  [1090100/1106938]\n",
      "loss: 0.716762  [1100100/1106938]\n",
      "Test Error: \n",
      " Accuracy: 73.6%, Avg loss: 0.785941 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.144326  [  100/1106938]\n",
      "loss: 0.760052  [10100/1106938]\n",
      "loss: 1.030258  [20100/1106938]\n",
      "loss: 0.858547  [30100/1106938]\n",
      "loss: 0.868185  [40100/1106938]\n",
      "loss: 0.776872  [50100/1106938]\n",
      "loss: 0.907040  [60100/1106938]\n",
      "loss: 0.800593  [70100/1106938]\n",
      "loss: 0.794972  [80100/1106938]\n",
      "loss: 0.798398  [90100/1106938]\n",
      "loss: 0.940576  [100100/1106938]\n",
      "loss: 0.748474  [110100/1106938]\n",
      "loss: 0.869281  [120100/1106938]\n",
      "loss: 0.774055  [130100/1106938]\n",
      "loss: 0.952820  [140100/1106938]\n",
      "loss: 0.885810  [150100/1106938]\n",
      "loss: 0.630122  [160100/1106938]\n",
      "loss: 1.100090  [170100/1106938]\n",
      "loss: 0.812642  [180100/1106938]\n",
      "loss: 0.883886  [190100/1106938]\n",
      "loss: 0.990134  [200100/1106938]\n",
      "loss: 0.993674  [210100/1106938]\n",
      "loss: 0.860465  [220100/1106938]\n",
      "loss: 0.700906  [230100/1106938]\n",
      "loss: 1.053775  [240100/1106938]\n",
      "loss: 0.901419  [250100/1106938]\n",
      "loss: 0.836264  [260100/1106938]\n",
      "loss: 1.075187  [270100/1106938]\n",
      "loss: 0.751493  [280100/1106938]\n",
      "loss: 0.879871  [290100/1106938]\n",
      "loss: 0.858489  [300100/1106938]\n",
      "loss: 0.910013  [310100/1106938]\n",
      "loss: 0.663991  [320100/1106938]\n",
      "loss: 0.858337  [330100/1106938]\n",
      "loss: 0.849773  [340100/1106938]\n",
      "loss: 0.802612  [350100/1106938]\n",
      "loss: 0.601966  [360100/1106938]\n",
      "loss: 0.837604  [370100/1106938]\n",
      "loss: 0.861711  [380100/1106938]\n",
      "loss: 0.897441  [390100/1106938]\n",
      "loss: 0.716069  [400100/1106938]\n",
      "loss: 0.849819  [410100/1106938]\n",
      "loss: 1.048260  [420100/1106938]\n",
      "loss: 0.862189  [430100/1106938]\n",
      "loss: 0.939838  [440100/1106938]\n",
      "loss: 0.847183  [450100/1106938]\n",
      "loss: 1.025937  [460100/1106938]\n",
      "loss: 0.846376  [470100/1106938]\n",
      "loss: 0.827761  [480100/1106938]\n",
      "loss: 0.929174  [490100/1106938]\n",
      "loss: 0.980489  [500100/1106938]\n",
      "loss: 0.870577  [510100/1106938]\n",
      "loss: 0.875737  [520100/1106938]\n",
      "loss: 0.694760  [530100/1106938]\n",
      "loss: 0.877409  [540100/1106938]\n",
      "loss: 0.657801  [550100/1106938]\n",
      "loss: 0.972696  [560100/1106938]\n",
      "loss: 0.903259  [570100/1106938]\n",
      "loss: 0.802032  [580100/1106938]\n",
      "loss: 0.925742  [590100/1106938]\n",
      "loss: 1.073726  [600100/1106938]\n",
      "loss: 0.957268  [610100/1106938]\n",
      "loss: 1.022873  [620100/1106938]\n",
      "loss: 0.717401  [630100/1106938]\n",
      "loss: 0.977893  [640100/1106938]\n",
      "loss: 0.827701  [650100/1106938]\n",
      "loss: 0.927084  [660100/1106938]\n",
      "loss: 0.832685  [670100/1106938]\n",
      "loss: 0.831453  [680100/1106938]\n",
      "loss: 0.820889  [690100/1106938]\n",
      "loss: 0.851699  [700100/1106938]\n",
      "loss: 1.065786  [710100/1106938]\n",
      "loss: 0.812999  [720100/1106938]\n",
      "loss: 0.971774  [730100/1106938]\n",
      "loss: 0.860262  [740100/1106938]\n",
      "loss: 1.321786  [750100/1106938]\n",
      "loss: 0.973351  [760100/1106938]\n",
      "loss: 0.812412  [770100/1106938]\n",
      "loss: 0.978574  [780100/1106938]\n",
      "loss: 0.616722  [790100/1106938]\n",
      "loss: 0.887494  [800100/1106938]\n",
      "loss: 0.898190  [810100/1106938]\n",
      "loss: 0.569884  [820100/1106938]\n",
      "loss: 0.985167  [830100/1106938]\n",
      "loss: 0.626904  [840100/1106938]\n",
      "loss: 0.977157  [850100/1106938]\n",
      "loss: 0.921439  [860100/1106938]\n",
      "loss: 0.998832  [870100/1106938]\n",
      "loss: 0.895844  [880100/1106938]\n",
      "loss: 0.760746  [890100/1106938]\n",
      "loss: 0.641020  [900100/1106938]\n",
      "loss: 0.744796  [910100/1106938]\n",
      "loss: 0.849561  [920100/1106938]\n",
      "loss: 0.892188  [930100/1106938]\n",
      "loss: 1.069242  [940100/1106938]\n",
      "loss: 0.954623  [950100/1106938]\n",
      "loss: 0.951922  [960100/1106938]\n",
      "loss: 0.887282  [970100/1106938]\n",
      "loss: 0.751742  [980100/1106938]\n",
      "loss: 0.862153  [990100/1106938]\n",
      "loss: 0.701881  [1000100/1106938]\n",
      "loss: 0.783297  [1010100/1106938]\n",
      "loss: 0.830753  [1020100/1106938]\n",
      "loss: 0.832855  [1030100/1106938]\n",
      "loss: 1.028370  [1040100/1106938]\n",
      "loss: 0.884858  [1050100/1106938]\n",
      "loss: 0.900582  [1060100/1106938]\n",
      "loss: 0.818867  [1070100/1106938]\n",
      "loss: 1.175985  [1080100/1106938]\n",
      "loss: 0.891234  [1090100/1106938]\n",
      "loss: 0.628175  [1100100/1106938]\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.771248 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.815921  [  100/1106938]\n",
      "loss: 1.080862  [10100/1106938]\n",
      "loss: 0.847330  [20100/1106938]\n",
      "loss: 0.616156  [30100/1106938]\n",
      "loss: 0.937123  [40100/1106938]\n",
      "loss: 0.913778  [50100/1106938]\n",
      "loss: 0.718908  [60100/1106938]\n",
      "loss: 0.688803  [70100/1106938]\n",
      "loss: 0.865834  [80100/1106938]\n",
      "loss: 0.862856  [90100/1106938]\n",
      "loss: 0.722515  [100100/1106938]\n",
      "loss: 0.763757  [110100/1106938]\n",
      "loss: 0.883914  [120100/1106938]\n",
      "loss: 0.789094  [130100/1106938]\n",
      "loss: 0.777370  [140100/1106938]\n",
      "loss: 0.777082  [150100/1106938]\n",
      "loss: 1.085636  [160100/1106938]\n",
      "loss: 0.833089  [170100/1106938]\n",
      "loss: 0.607819  [180100/1106938]\n",
      "loss: 0.904139  [190100/1106938]\n",
      "loss: 1.038410  [200100/1106938]\n",
      "loss: 0.812580  [210100/1106938]\n",
      "loss: 0.883688  [220100/1106938]\n",
      "loss: 0.890464  [230100/1106938]\n",
      "loss: 1.037886  [240100/1106938]\n",
      "loss: 0.705665  [250100/1106938]\n",
      "loss: 1.051453  [260100/1106938]\n",
      "loss: 0.802679  [270100/1106938]\n",
      "loss: 0.769664  [280100/1106938]\n",
      "loss: 0.988555  [290100/1106938]\n",
      "loss: 0.687975  [300100/1106938]\n",
      "loss: 0.841946  [310100/1106938]\n",
      "loss: 0.818320  [320100/1106938]\n",
      "loss: 0.794880  [330100/1106938]\n",
      "loss: 0.680041  [340100/1106938]\n",
      "loss: 0.958736  [350100/1106938]\n",
      "loss: 0.889324  [360100/1106938]\n",
      "loss: 0.876930  [370100/1106938]\n",
      "loss: 0.749890  [380100/1106938]\n",
      "loss: 0.818871  [390100/1106938]\n",
      "loss: 0.786172  [400100/1106938]\n",
      "loss: 0.990224  [410100/1106938]\n",
      "loss: 0.823975  [420100/1106938]\n",
      "loss: 0.801853  [430100/1106938]\n",
      "loss: 0.707793  [440100/1106938]\n",
      "loss: 0.723988  [450100/1106938]\n",
      "loss: 0.945949  [460100/1106938]\n",
      "loss: 0.997295  [470100/1106938]\n",
      "loss: 0.722991  [480100/1106938]\n",
      "loss: 0.785894  [490100/1106938]\n",
      "loss: 0.773935  [500100/1106938]\n",
      "loss: 0.764117  [510100/1106938]\n",
      "loss: 0.907099  [520100/1106938]\n",
      "loss: 0.858372  [530100/1106938]\n",
      "loss: 0.811415  [540100/1106938]\n",
      "loss: 0.715500  [550100/1106938]\n",
      "loss: 0.831319  [560100/1106938]\n",
      "loss: 0.849828  [570100/1106938]\n",
      "loss: 0.782053  [580100/1106938]\n",
      "loss: 0.936075  [590100/1106938]\n",
      "loss: 0.660713  [600100/1106938]\n",
      "loss: 0.901099  [610100/1106938]\n",
      "loss: 0.864248  [620100/1106938]\n",
      "loss: 0.751727  [630100/1106938]\n",
      "loss: 0.838087  [640100/1106938]\n",
      "loss: 1.025846  [650100/1106938]\n",
      "loss: 0.898127  [660100/1106938]\n",
      "loss: 0.879985  [670100/1106938]\n",
      "loss: 0.729576  [680100/1106938]\n",
      "loss: 0.815478  [690100/1106938]\n",
      "loss: 0.810826  [700100/1106938]\n",
      "loss: 0.763896  [710100/1106938]\n",
      "loss: 0.915390  [720100/1106938]\n",
      "loss: 0.994509  [730100/1106938]\n",
      "loss: 0.892823  [740100/1106938]\n",
      "loss: 0.796739  [750100/1106938]\n",
      "loss: 0.691597  [760100/1106938]\n",
      "loss: 0.956001  [770100/1106938]\n",
      "loss: 1.017410  [780100/1106938]\n",
      "loss: 0.724151  [790100/1106938]\n",
      "loss: 0.813817  [800100/1106938]\n",
      "loss: 1.062576  [810100/1106938]\n",
      "loss: 1.170490  [820100/1106938]\n",
      "loss: 0.626115  [830100/1106938]\n",
      "loss: 0.856416  [840100/1106938]\n",
      "loss: 0.765033  [850100/1106938]\n",
      "loss: 0.613984  [860100/1106938]\n",
      "loss: 0.813642  [870100/1106938]\n",
      "loss: 1.068574  [880100/1106938]\n",
      "loss: 0.715770  [890100/1106938]\n",
      "loss: 0.781243  [900100/1106938]\n",
      "loss: 0.716711  [910100/1106938]\n",
      "loss: 0.804386  [920100/1106938]\n",
      "loss: 0.629364  [930100/1106938]\n",
      "loss: 0.830687  [940100/1106938]\n",
      "loss: 0.843429  [950100/1106938]\n",
      "loss: 0.901815  [960100/1106938]\n",
      "loss: 0.841754  [970100/1106938]\n",
      "loss: 1.050609  [980100/1106938]\n",
      "loss: 0.812803  [990100/1106938]\n",
      "loss: 0.750410  [1000100/1106938]\n",
      "loss: 0.723654  [1010100/1106938]\n",
      "loss: 1.034381  [1020100/1106938]\n",
      "loss: 0.793132  [1030100/1106938]\n",
      "loss: 0.876016  [1040100/1106938]\n",
      "loss: 0.855484  [1050100/1106938]\n",
      "loss: 0.932400  [1060100/1106938]\n",
      "loss: 0.989243  [1070100/1106938]\n",
      "loss: 0.759713  [1080100/1106938]\n",
      "loss: 0.794499  [1090100/1106938]\n",
      "loss: 0.754873  [1100100/1106938]\n",
      "Test Error: \n",
      " Accuracy: 74.4%, Avg loss: 0.758896 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.718662  [  100/1106938]\n",
      "loss: 0.772223  [10100/1106938]\n",
      "loss: 0.820501  [20100/1106938]\n",
      "loss: 0.677814  [30100/1106938]\n",
      "loss: 1.009634  [40100/1106938]\n",
      "loss: 0.792972  [50100/1106938]\n",
      "loss: 0.851621  [60100/1106938]\n",
      "loss: 0.931707  [70100/1106938]\n",
      "loss: 0.904993  [80100/1106938]\n",
      "loss: 0.642053  [90100/1106938]\n",
      "loss: 0.810717  [100100/1106938]\n",
      "loss: 0.722947  [110100/1106938]\n",
      "loss: 0.797892  [120100/1106938]\n",
      "loss: 1.023164  [130100/1106938]\n",
      "loss: 0.813580  [140100/1106938]\n",
      "loss: 0.499914  [150100/1106938]\n",
      "loss: 0.728728  [160100/1106938]\n",
      "loss: 1.034655  [170100/1106938]\n",
      "loss: 0.974117  [180100/1106938]\n",
      "loss: 0.992543  [190100/1106938]\n",
      "loss: 0.783803  [200100/1106938]\n",
      "loss: 0.847278  [210100/1106938]\n",
      "loss: 0.892573  [220100/1106938]\n",
      "loss: 0.785681  [230100/1106938]\n",
      "loss: 0.784647  [240100/1106938]\n",
      "loss: 0.957342  [250100/1106938]\n",
      "loss: 0.835834  [260100/1106938]\n",
      "loss: 0.658511  [270100/1106938]\n",
      "loss: 0.979633  [280100/1106938]\n",
      "loss: 0.839707  [290100/1106938]\n",
      "loss: 0.737997  [300100/1106938]\n",
      "loss: 0.730006  [310100/1106938]\n",
      "loss: 0.931504  [320100/1106938]\n",
      "loss: 1.058845  [330100/1106938]\n",
      "loss: 0.938308  [340100/1106938]\n",
      "loss: 0.826906  [350100/1106938]\n",
      "loss: 0.883153  [360100/1106938]\n",
      "loss: 0.741583  [370100/1106938]\n",
      "loss: 0.843916  [380100/1106938]\n",
      "loss: 0.717856  [390100/1106938]\n",
      "loss: 0.943478  [400100/1106938]\n",
      "loss: 1.074946  [410100/1106938]\n",
      "loss: 0.643444  [420100/1106938]\n",
      "loss: 0.859142  [430100/1106938]\n",
      "loss: 0.697939  [440100/1106938]\n",
      "loss: 0.969845  [450100/1106938]\n",
      "loss: 0.782162  [460100/1106938]\n",
      "loss: 0.535726  [470100/1106938]\n",
      "loss: 0.828984  [480100/1106938]\n",
      "loss: 0.771731  [490100/1106938]\n",
      "loss: 0.640769  [500100/1106938]\n",
      "loss: 0.576194  [510100/1106938]\n",
      "loss: 0.842015  [520100/1106938]\n",
      "loss: 0.921189  [530100/1106938]\n",
      "loss: 0.712988  [540100/1106938]\n",
      "loss: 0.851550  [550100/1106938]\n",
      "loss: 0.929034  [560100/1106938]\n",
      "loss: 0.595288  [570100/1106938]\n",
      "loss: 0.782239  [580100/1106938]\n",
      "loss: 0.907903  [590100/1106938]\n",
      "loss: 1.022294  [600100/1106938]\n",
      "loss: 0.941962  [610100/1106938]\n",
      "loss: 0.674640  [620100/1106938]\n",
      "loss: 0.791986  [630100/1106938]\n",
      "loss: 0.992845  [640100/1106938]\n",
      "loss: 0.803934  [650100/1106938]\n",
      "loss: 0.785887  [660100/1106938]\n",
      "loss: 0.764954  [670100/1106938]\n",
      "loss: 0.870169  [680100/1106938]\n",
      "loss: 0.887463  [690100/1106938]\n",
      "loss: 0.651817  [700100/1106938]\n",
      "loss: 0.697369  [710100/1106938]\n",
      "loss: 0.808625  [720100/1106938]\n",
      "loss: 0.649314  [730100/1106938]\n",
      "loss: 0.970838  [740100/1106938]\n",
      "loss: 0.857074  [750100/1106938]\n",
      "loss: 0.718577  [760100/1106938]\n",
      "loss: 0.807648  [770100/1106938]\n",
      "loss: 0.657558  [780100/1106938]\n",
      "loss: 0.780120  [790100/1106938]\n",
      "loss: 0.981726  [800100/1106938]\n",
      "loss: 0.976689  [810100/1106938]\n",
      "loss: 0.989602  [820100/1106938]\n",
      "loss: 0.807123  [830100/1106938]\n",
      "loss: 0.912613  [840100/1106938]\n",
      "loss: 0.876532  [850100/1106938]\n",
      "loss: 1.102535  [860100/1106938]\n",
      "loss: 1.050355  [870100/1106938]\n",
      "loss: 0.861201  [880100/1106938]\n",
      "loss: 0.762334  [890100/1106938]\n",
      "loss: 0.868891  [900100/1106938]\n",
      "loss: 0.993432  [910100/1106938]\n",
      "loss: 0.968629  [920100/1106938]\n",
      "loss: 0.932944  [930100/1106938]\n",
      "loss: 0.830257  [940100/1106938]\n",
      "loss: 0.866620  [950100/1106938]\n",
      "loss: 0.719706  [960100/1106938]\n",
      "loss: 0.783557  [970100/1106938]\n",
      "loss: 0.811694  [980100/1106938]\n",
      "loss: 0.959294  [990100/1106938]\n",
      "loss: 0.782111  [1000100/1106938]\n",
      "loss: 0.849948  [1010100/1106938]\n",
      "loss: 0.907861  [1020100/1106938]\n",
      "loss: 0.746241  [1030100/1106938]\n",
      "loss: 0.821192  [1040100/1106938]\n",
      "loss: 0.754516  [1050100/1106938]\n",
      "loss: 1.103887  [1060100/1106938]\n",
      "loss: 0.722555  [1070100/1106938]\n",
      "loss: 0.880973  [1080100/1106938]\n",
      "loss: 0.676501  [1090100/1106938]\n",
      "loss: 1.013551  [1100100/1106938]\n",
      "Test Error: \n",
      " Accuracy: 74.8%, Avg loss: 0.748784 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.836786  [  100/1106938]\n",
      "loss: 0.788103  [10100/1106938]\n",
      "loss: 0.986575  [20100/1106938]\n",
      "loss: 0.794276  [30100/1106938]\n",
      "loss: 0.683654  [40100/1106938]\n",
      "loss: 0.755511  [50100/1106938]\n",
      "loss: 0.650183  [60100/1106938]\n",
      "loss: 0.895393  [70100/1106938]\n",
      "loss: 0.763706  [80100/1106938]\n",
      "loss: 0.720637  [90100/1106938]\n",
      "loss: 0.744420  [100100/1106938]\n",
      "loss: 0.765736  [110100/1106938]\n",
      "loss: 0.734762  [120100/1106938]\n",
      "loss: 1.234466  [130100/1106938]\n",
      "loss: 1.085086  [140100/1106938]\n",
      "loss: 0.850051  [150100/1106938]\n",
      "loss: 0.804087  [160100/1106938]\n",
      "loss: 0.907051  [170100/1106938]\n",
      "loss: 0.903251  [180100/1106938]\n",
      "loss: 0.899948  [190100/1106938]\n",
      "loss: 0.746408  [200100/1106938]\n",
      "loss: 0.850467  [210100/1106938]\n",
      "loss: 0.819883  [220100/1106938]\n",
      "loss: 0.890963  [230100/1106938]\n",
      "loss: 0.657629  [240100/1106938]\n",
      "loss: 0.726589  [250100/1106938]\n",
      "loss: 0.686073  [260100/1106938]\n",
      "loss: 0.981422  [270100/1106938]\n",
      "loss: 1.028110  [280100/1106938]\n",
      "loss: 0.729548  [290100/1106938]\n",
      "loss: 0.677988  [300100/1106938]\n",
      "loss: 0.842840  [310100/1106938]\n",
      "loss: 0.913811  [320100/1106938]\n",
      "loss: 0.828101  [330100/1106938]\n",
      "loss: 0.769635  [340100/1106938]\n",
      "loss: 0.793364  [350100/1106938]\n",
      "loss: 0.621921  [360100/1106938]\n",
      "loss: 0.927433  [370100/1106938]\n",
      "loss: 0.928473  [380100/1106938]\n",
      "loss: 0.666117  [390100/1106938]\n",
      "loss: 0.735936  [400100/1106938]\n",
      "loss: 0.626966  [410100/1106938]\n",
      "loss: 0.922180  [420100/1106938]\n",
      "loss: 0.811140  [430100/1106938]\n",
      "loss: 0.855305  [440100/1106938]\n",
      "loss: 0.862067  [450100/1106938]\n",
      "loss: 1.022538  [460100/1106938]\n",
      "loss: 0.851647  [470100/1106938]\n",
      "loss: 0.684968  [480100/1106938]\n",
      "loss: 0.751878  [490100/1106938]\n",
      "loss: 0.866664  [500100/1106938]\n",
      "loss: 1.027734  [510100/1106938]\n",
      "loss: 0.875222  [520100/1106938]\n",
      "loss: 0.936027  [530100/1106938]\n",
      "loss: 0.683790  [540100/1106938]\n",
      "loss: 0.697942  [550100/1106938]\n",
      "loss: 0.881414  [560100/1106938]\n",
      "loss: 0.751400  [570100/1106938]\n",
      "loss: 0.872257  [580100/1106938]\n",
      "loss: 0.881238  [590100/1106938]\n",
      "loss: 0.742506  [600100/1106938]\n",
      "loss: 0.795341  [610100/1106938]\n",
      "loss: 0.906787  [620100/1106938]\n",
      "loss: 0.771233  [630100/1106938]\n",
      "loss: 0.862615  [640100/1106938]\n",
      "loss: 0.716298  [650100/1106938]\n",
      "loss: 0.810615  [660100/1106938]\n",
      "loss: 0.719288  [670100/1106938]\n",
      "loss: 0.653635  [680100/1106938]\n",
      "loss: 0.859808  [690100/1106938]\n",
      "loss: 0.823378  [700100/1106938]\n",
      "loss: 0.760668  [710100/1106938]\n",
      "loss: 0.771450  [720100/1106938]\n",
      "loss: 0.908799  [730100/1106938]\n",
      "loss: 0.953000  [740100/1106938]\n",
      "loss: 0.753884  [750100/1106938]\n",
      "loss: 0.882196  [760100/1106938]\n",
      "loss: 0.914285  [770100/1106938]\n",
      "loss: 0.934073  [780100/1106938]\n",
      "loss: 0.809119  [790100/1106938]\n",
      "loss: 0.846776  [800100/1106938]\n",
      "loss: 0.761622  [810100/1106938]\n",
      "loss: 0.901613  [820100/1106938]\n",
      "loss: 0.792962  [830100/1106938]\n",
      "loss: 0.834617  [840100/1106938]\n",
      "loss: 0.925764  [850100/1106938]\n",
      "loss: 0.777551  [860100/1106938]\n",
      "loss: 1.024251  [870100/1106938]\n",
      "loss: 1.163921  [880100/1106938]\n",
      "loss: 0.912269  [890100/1106938]\n",
      "loss: 0.542856  [900100/1106938]\n",
      "loss: 0.776955  [910100/1106938]\n",
      "loss: 0.926418  [920100/1106938]\n",
      "loss: 0.933962  [930100/1106938]\n",
      "loss: 0.736844  [940100/1106938]\n",
      "loss: 0.961473  [950100/1106938]\n",
      "loss: 0.955218  [960100/1106938]\n",
      "loss: 0.801050  [970100/1106938]\n",
      "loss: 0.898390  [980100/1106938]\n",
      "loss: 0.926891  [990100/1106938]\n",
      "loss: 0.717238  [1000100/1106938]\n",
      "loss: 0.840212  [1010100/1106938]\n",
      "loss: 0.800167  [1020100/1106938]\n",
      "loss: 0.809008  [1030100/1106938]\n",
      "loss: 0.823796  [1040100/1106938]\n",
      "loss: 0.756294  [1050100/1106938]\n",
      "loss: 0.890819  [1060100/1106938]\n",
      "loss: 0.578657  [1070100/1106938]\n",
      "loss: 0.633491  [1080100/1106938]\n",
      "loss: 0.773259  [1090100/1106938]\n",
      "loss: 0.821048  [1100100/1106938]\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.742578 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.800947  [  100/1106938]\n",
      "loss: 0.829689  [10100/1106938]\n",
      "loss: 1.036965  [20100/1106938]\n",
      "loss: 0.879959  [30100/1106938]\n",
      "loss: 0.693651  [40100/1106938]\n",
      "loss: 0.925126  [50100/1106938]\n",
      "loss: 0.763398  [60100/1106938]\n",
      "loss: 0.915567  [70100/1106938]\n",
      "loss: 0.965761  [80100/1106938]\n",
      "loss: 0.799749  [90100/1106938]\n",
      "loss: 0.762577  [100100/1106938]\n",
      "loss: 0.861985  [110100/1106938]\n",
      "loss: 0.688288  [120100/1106938]\n",
      "loss: 0.899827  [130100/1106938]\n",
      "loss: 0.762787  [140100/1106938]\n",
      "loss: 0.802504  [150100/1106938]\n",
      "loss: 0.810656  [160100/1106938]\n",
      "loss: 0.684073  [170100/1106938]\n",
      "loss: 0.782394  [180100/1106938]\n",
      "loss: 0.711052  [190100/1106938]\n",
      "loss: 0.780209  [200100/1106938]\n",
      "loss: 0.810171  [210100/1106938]\n",
      "loss: 0.868631  [220100/1106938]\n",
      "loss: 0.940269  [230100/1106938]\n",
      "loss: 0.822007  [240100/1106938]\n",
      "loss: 0.866942  [250100/1106938]\n",
      "loss: 0.890774  [260100/1106938]\n",
      "loss: 0.988142  [270100/1106938]\n",
      "loss: 0.853458  [280100/1106938]\n",
      "loss: 0.780127  [290100/1106938]\n",
      "loss: 0.701687  [300100/1106938]\n",
      "loss: 0.998289  [310100/1106938]\n",
      "loss: 0.660308  [320100/1106938]\n",
      "loss: 0.646650  [330100/1106938]\n",
      "loss: 0.845839  [340100/1106938]\n",
      "loss: 0.728434  [350100/1106938]\n",
      "loss: 0.721692  [360100/1106938]\n",
      "loss: 0.654004  [370100/1106938]\n",
      "loss: 0.912961  [380100/1106938]\n",
      "loss: 0.720045  [390100/1106938]\n",
      "loss: 0.735699  [400100/1106938]\n",
      "loss: 0.932493  [410100/1106938]\n",
      "loss: 0.823077  [420100/1106938]\n",
      "loss: 1.048449  [430100/1106938]\n",
      "loss: 0.597165  [440100/1106938]\n",
      "loss: 0.882057  [450100/1106938]\n",
      "loss: 1.110186  [460100/1106938]\n",
      "loss: 0.963717  [470100/1106938]\n",
      "loss: 0.863762  [480100/1106938]\n",
      "loss: 0.801926  [490100/1106938]\n",
      "loss: 0.949727  [500100/1106938]\n",
      "loss: 0.636909  [510100/1106938]\n",
      "loss: 0.849194  [520100/1106938]\n",
      "loss: 0.798022  [530100/1106938]\n",
      "loss: 0.854427  [540100/1106938]\n",
      "loss: 0.851066  [550100/1106938]\n",
      "loss: 1.003881  [560100/1106938]\n",
      "loss: 0.974759  [570100/1106938]\n",
      "loss: 0.787196  [580100/1106938]\n",
      "loss: 0.880042  [590100/1106938]\n",
      "loss: 0.738754  [600100/1106938]\n",
      "loss: 0.863519  [610100/1106938]\n",
      "loss: 0.788816  [620100/1106938]\n",
      "loss: 0.920681  [630100/1106938]\n",
      "loss: 0.895278  [640100/1106938]\n",
      "loss: 1.008851  [650100/1106938]\n",
      "loss: 0.961704  [660100/1106938]\n",
      "loss: 0.989888  [670100/1106938]\n",
      "loss: 0.718856  [680100/1106938]\n",
      "loss: 0.733678  [690100/1106938]\n",
      "loss: 0.832300  [700100/1106938]\n",
      "loss: 0.846413  [710100/1106938]\n",
      "loss: 0.983375  [720100/1106938]\n",
      "loss: 0.775560  [730100/1106938]\n",
      "loss: 0.886926  [740100/1106938]\n",
      "loss: 0.761424  [750100/1106938]\n",
      "loss: 0.749109  [760100/1106938]\n",
      "loss: 0.979283  [770100/1106938]\n",
      "loss: 0.898396  [780100/1106938]\n",
      "loss: 0.775072  [790100/1106938]\n",
      "loss: 1.042756  [800100/1106938]\n",
      "loss: 0.935598  [810100/1106938]\n",
      "loss: 0.911279  [820100/1106938]\n",
      "loss: 0.818222  [830100/1106938]\n",
      "loss: 0.704609  [840100/1106938]\n",
      "loss: 0.936990  [850100/1106938]\n",
      "loss: 0.766915  [860100/1106938]\n",
      "loss: 0.647454  [870100/1106938]\n",
      "loss: 0.767687  [880100/1106938]\n",
      "loss: 0.916112  [890100/1106938]\n",
      "loss: 0.723552  [900100/1106938]\n",
      "loss: 1.002314  [910100/1106938]\n",
      "loss: 0.896347  [920100/1106938]\n",
      "loss: 0.813096  [930100/1106938]\n",
      "loss: 0.880197  [940100/1106938]\n",
      "loss: 0.637281  [950100/1106938]\n",
      "loss: 0.979512  [960100/1106938]\n",
      "loss: 0.828955  [970100/1106938]\n",
      "loss: 0.690644  [980100/1106938]\n",
      "loss: 0.912581  [990100/1106938]\n",
      "loss: 0.956951  [1000100/1106938]\n",
      "loss: 0.860266  [1010100/1106938]\n",
      "loss: 0.703185  [1020100/1106938]\n",
      "loss: 0.791669  [1030100/1106938]\n",
      "loss: 0.887687  [1040100/1106938]\n",
      "loss: 1.047930  [1050100/1106938]\n",
      "loss: 0.712049  [1060100/1106938]\n",
      "loss: 0.824909  [1070100/1106938]\n",
      "loss: 1.105806  [1080100/1106938]\n",
      "loss: 0.890281  [1090100/1106938]\n",
      "loss: 0.843209  [1100100/1106938]\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.740853 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.787367  [  100/1106938]\n",
      "loss: 0.802937  [10100/1106938]\n",
      "loss: 0.811685  [20100/1106938]\n",
      "loss: 0.816132  [30100/1106938]\n",
      "loss: 0.661000  [40100/1106938]\n",
      "loss: 0.733093  [50100/1106938]\n",
      "loss: 0.839444  [60100/1106938]\n",
      "loss: 0.564268  [70100/1106938]\n",
      "loss: 0.788831  [80100/1106938]\n",
      "loss: 0.695634  [90100/1106938]\n",
      "loss: 0.769405  [100100/1106938]\n",
      "loss: 0.785281  [110100/1106938]\n",
      "loss: 0.725606  [120100/1106938]\n",
      "loss: 0.842837  [130100/1106938]\n",
      "loss: 0.949864  [140100/1106938]\n",
      "loss: 0.806816  [150100/1106938]\n",
      "loss: 1.044620  [160100/1106938]\n",
      "loss: 0.603885  [170100/1106938]\n",
      "loss: 0.780445  [180100/1106938]\n",
      "loss: 1.090632  [190100/1106938]\n",
      "loss: 0.683287  [200100/1106938]\n",
      "loss: 0.924602  [210100/1106938]\n",
      "loss: 0.840454  [220100/1106938]\n",
      "loss: 0.778572  [230100/1106938]\n",
      "loss: 0.779594  [240100/1106938]\n",
      "loss: 0.716923  [250100/1106938]\n",
      "loss: 0.639115  [260100/1106938]\n",
      "loss: 0.657942  [270100/1106938]\n",
      "loss: 0.957680  [280100/1106938]\n",
      "loss: 1.007617  [290100/1106938]\n",
      "loss: 0.879863  [300100/1106938]\n",
      "loss: 0.805682  [310100/1106938]\n",
      "loss: 0.804431  [320100/1106938]\n",
      "loss: 0.688133  [330100/1106938]\n",
      "loss: 0.741993  [340100/1106938]\n",
      "loss: 0.880750  [350100/1106938]\n",
      "loss: 0.836307  [360100/1106938]\n",
      "loss: 0.823564  [370100/1106938]\n",
      "loss: 0.870582  [380100/1106938]\n",
      "loss: 1.001121  [390100/1106938]\n",
      "loss: 0.797340  [400100/1106938]\n",
      "loss: 0.676514  [410100/1106938]\n",
      "loss: 0.913408  [420100/1106938]\n",
      "loss: 0.820616  [430100/1106938]\n",
      "loss: 0.948071  [440100/1106938]\n",
      "loss: 0.724533  [450100/1106938]\n",
      "loss: 0.785319  [460100/1106938]\n",
      "loss: 0.665025  [470100/1106938]\n",
      "loss: 0.825789  [480100/1106938]\n",
      "loss: 0.764315  [490100/1106938]\n",
      "loss: 0.781193  [500100/1106938]\n",
      "loss: 0.831791  [510100/1106938]\n",
      "loss: 0.886128  [520100/1106938]\n",
      "loss: 0.811631  [530100/1106938]\n",
      "loss: 0.942362  [540100/1106938]\n",
      "loss: 0.795019  [550100/1106938]\n",
      "loss: 0.951947  [560100/1106938]\n",
      "loss: 0.802592  [570100/1106938]\n",
      "loss: 1.009292  [580100/1106938]\n",
      "loss: 0.956776  [590100/1106938]\n",
      "loss: 0.871508  [600100/1106938]\n",
      "loss: 0.830445  [610100/1106938]\n",
      "loss: 1.017452  [620100/1106938]\n",
      "loss: 1.008933  [630100/1106938]\n",
      "loss: 0.718570  [640100/1106938]\n",
      "loss: 0.824767  [650100/1106938]\n",
      "loss: 0.831514  [660100/1106938]\n",
      "loss: 0.822023  [670100/1106938]\n",
      "loss: 0.830398  [680100/1106938]\n",
      "loss: 1.010479  [690100/1106938]\n",
      "loss: 0.935429  [700100/1106938]\n",
      "loss: 1.021313  [710100/1106938]\n",
      "loss: 0.850585  [720100/1106938]\n",
      "loss: 0.813510  [730100/1106938]\n",
      "loss: 0.823453  [740100/1106938]\n",
      "loss: 0.701027  [750100/1106938]\n",
      "loss: 0.866334  [760100/1106938]\n",
      "loss: 0.788385  [770100/1106938]\n",
      "loss: 0.798783  [780100/1106938]\n",
      "loss: 0.742899  [790100/1106938]\n",
      "loss: 0.931528  [800100/1106938]\n",
      "loss: 0.758548  [810100/1106938]\n",
      "loss: 0.760831  [820100/1106938]\n",
      "loss: 0.719671  [830100/1106938]\n",
      "loss: 0.644968  [840100/1106938]\n",
      "loss: 0.741835  [850100/1106938]\n",
      "loss: 0.867092  [860100/1106938]\n",
      "loss: 0.790793  [870100/1106938]\n",
      "loss: 0.876691  [880100/1106938]\n",
      "loss: 0.945463  [890100/1106938]\n",
      "loss: 0.845164  [900100/1106938]\n",
      "loss: 0.912960  [910100/1106938]\n",
      "loss: 0.835364  [920100/1106938]\n",
      "loss: 0.628301  [930100/1106938]\n",
      "loss: 0.801672  [940100/1106938]\n",
      "loss: 0.732606  [950100/1106938]\n",
      "loss: 0.800231  [960100/1106938]\n",
      "loss: 0.726849  [970100/1106938]\n",
      "loss: 0.676244  [980100/1106938]\n",
      "loss: 0.738101  [990100/1106938]\n",
      "loss: 0.700708  [1000100/1106938]\n",
      "loss: 1.020188  [1010100/1106938]\n",
      "loss: 1.036942  [1020100/1106938]\n",
      "loss: 1.000175  [1030100/1106938]\n",
      "loss: 1.047089  [1040100/1106938]\n",
      "loss: 0.951686  [1050100/1106938]\n",
      "loss: 0.903329  [1060100/1106938]\n",
      "loss: 0.755260  [1070100/1106938]\n",
      "loss: 0.995979  [1080100/1106938]\n",
      "loss: 0.821532  [1090100/1106938]\n",
      "loss: 0.718180  [1100100/1106938]\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.731398 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.687663  [  100/1106938]\n",
      "loss: 0.819505  [10100/1106938]\n",
      "loss: 0.778193  [20100/1106938]\n",
      "loss: 0.893642  [30100/1106938]\n",
      "loss: 0.803515  [40100/1106938]\n",
      "loss: 0.726586  [50100/1106938]\n",
      "loss: 0.593100  [60100/1106938]\n",
      "loss: 0.888831  [70100/1106938]\n",
      "loss: 0.661524  [80100/1106938]\n",
      "loss: 0.939929  [90100/1106938]\n",
      "loss: 0.871509  [100100/1106938]\n",
      "loss: 0.805881  [110100/1106938]\n",
      "loss: 0.699024  [120100/1106938]\n",
      "loss: 0.806562  [130100/1106938]\n",
      "loss: 0.627369  [140100/1106938]\n",
      "loss: 0.664593  [150100/1106938]\n",
      "loss: 0.677606  [160100/1106938]\n",
      "loss: 0.853217  [170100/1106938]\n",
      "loss: 0.834016  [180100/1106938]\n",
      "loss: 0.858628  [190100/1106938]\n",
      "loss: 0.790537  [200100/1106938]\n",
      "loss: 0.873699  [210100/1106938]\n",
      "loss: 0.682860  [220100/1106938]\n",
      "loss: 0.844772  [230100/1106938]\n",
      "loss: 0.620112  [240100/1106938]\n",
      "loss: 1.023743  [250100/1106938]\n",
      "loss: 1.011311  [260100/1106938]\n",
      "loss: 0.923805  [270100/1106938]\n",
      "loss: 0.775840  [280100/1106938]\n",
      "loss: 0.842514  [290100/1106938]\n",
      "loss: 0.731975  [300100/1106938]\n",
      "loss: 0.686040  [310100/1106938]\n",
      "loss: 1.178062  [320100/1106938]\n",
      "loss: 0.657621  [330100/1106938]\n",
      "loss: 0.764748  [340100/1106938]\n",
      "loss: 0.887368  [350100/1106938]\n",
      "loss: 0.634340  [360100/1106938]\n",
      "loss: 1.002726  [370100/1106938]\n",
      "loss: 0.793081  [380100/1106938]\n",
      "loss: 0.948618  [390100/1106938]\n",
      "loss: 0.893543  [400100/1106938]\n",
      "loss: 0.722561  [410100/1106938]\n",
      "loss: 0.793867  [420100/1106938]\n",
      "loss: 0.905538  [430100/1106938]\n",
      "loss: 0.745001  [440100/1106938]\n",
      "loss: 0.840899  [450100/1106938]\n",
      "loss: 0.688362  [460100/1106938]\n",
      "loss: 0.644792  [470100/1106938]\n",
      "loss: 1.045517  [480100/1106938]\n",
      "loss: 0.711582  [490100/1106938]\n",
      "loss: 0.776249  [500100/1106938]\n",
      "loss: 0.773666  [510100/1106938]\n",
      "loss: 0.794962  [520100/1106938]\n",
      "loss: 0.782037  [530100/1106938]\n",
      "loss: 0.899941  [540100/1106938]\n",
      "loss: 0.806052  [550100/1106938]\n",
      "loss: 0.780688  [560100/1106938]\n",
      "loss: 0.570274  [570100/1106938]\n",
      "loss: 0.947872  [580100/1106938]\n",
      "loss: 0.679602  [590100/1106938]\n",
      "loss: 0.698548  [600100/1106938]\n",
      "loss: 0.858089  [610100/1106938]\n",
      "loss: 0.705072  [620100/1106938]\n",
      "loss: 0.797666  [630100/1106938]\n",
      "loss: 0.777717  [640100/1106938]\n",
      "loss: 0.733867  [650100/1106938]\n",
      "loss: 0.958121  [660100/1106938]\n",
      "loss: 0.778469  [670100/1106938]\n",
      "loss: 0.692323  [680100/1106938]\n",
      "loss: 0.893443  [690100/1106938]\n",
      "loss: 0.712977  [700100/1106938]\n",
      "loss: 0.890277  [710100/1106938]\n",
      "loss: 0.931458  [720100/1106938]\n",
      "loss: 0.697638  [730100/1106938]\n",
      "loss: 0.726773  [740100/1106938]\n",
      "loss: 0.688404  [750100/1106938]\n",
      "loss: 0.569236  [760100/1106938]\n",
      "loss: 0.624174  [770100/1106938]\n",
      "loss: 1.024580  [780100/1106938]\n",
      "loss: 0.813101  [790100/1106938]\n",
      "loss: 0.817537  [800100/1106938]\n",
      "loss: 1.088247  [810100/1106938]\n",
      "loss: 0.737222  [820100/1106938]\n",
      "loss: 0.722378  [830100/1106938]\n",
      "loss: 0.805533  [840100/1106938]\n",
      "loss: 0.941798  [850100/1106938]\n",
      "loss: 0.812616  [860100/1106938]\n",
      "loss: 0.884416  [870100/1106938]\n",
      "loss: 0.990200  [880100/1106938]\n",
      "loss: 0.769378  [890100/1106938]\n",
      "loss: 1.017149  [900100/1106938]\n",
      "loss: 0.712594  [910100/1106938]\n",
      "loss: 0.704579  [920100/1106938]\n",
      "loss: 0.785328  [930100/1106938]\n",
      "loss: 0.775476  [940100/1106938]\n",
      "loss: 0.685650  [950100/1106938]\n",
      "loss: 0.650613  [960100/1106938]\n",
      "loss: 0.902197  [970100/1106938]\n",
      "loss: 0.692699  [980100/1106938]\n",
      "loss: 0.991171  [990100/1106938]\n",
      "loss: 0.789876  [1000100/1106938]\n",
      "loss: 0.946722  [1010100/1106938]\n",
      "loss: 0.560222  [1020100/1106938]\n",
      "loss: 0.859964  [1030100/1106938]\n",
      "loss: 0.747222  [1040100/1106938]\n",
      "loss: 0.921551  [1050100/1106938]\n",
      "loss: 0.921827  [1060100/1106938]\n",
      "loss: 0.704196  [1070100/1106938]\n",
      "loss: 0.905646  [1080100/1106938]\n",
      "loss: 0.688008  [1090100/1106938]\n",
      "loss: 0.717607  [1100100/1106938]\n",
      "Test Error: \n",
      " Accuracy: 75.5%, Avg loss: 0.722367 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.685560  [  100/1106938]\n",
      "loss: 0.891409  [10100/1106938]\n",
      "loss: 0.672404  [20100/1106938]\n",
      "loss: 0.870863  [30100/1106938]\n",
      "loss: 0.920792  [40100/1106938]\n",
      "loss: 0.933767  [50100/1106938]\n",
      "loss: 0.683976  [60100/1106938]\n",
      "loss: 0.845043  [70100/1106938]\n",
      "loss: 0.809530  [80100/1106938]\n",
      "loss: 0.859002  [90100/1106938]\n",
      "loss: 0.826814  [100100/1106938]\n",
      "loss: 0.831275  [110100/1106938]\n",
      "loss: 0.761883  [120100/1106938]\n",
      "loss: 0.636878  [130100/1106938]\n",
      "loss: 0.830345  [140100/1106938]\n",
      "loss: 0.752231  [150100/1106938]\n",
      "loss: 0.686414  [160100/1106938]\n",
      "loss: 0.726564  [170100/1106938]\n",
      "loss: 0.628845  [180100/1106938]\n",
      "loss: 0.753224  [190100/1106938]\n",
      "loss: 0.676419  [200100/1106938]\n",
      "loss: 0.870256  [210100/1106938]\n",
      "loss: 0.917964  [220100/1106938]\n",
      "loss: 0.740861  [230100/1106938]\n",
      "loss: 0.875197  [240100/1106938]\n",
      "loss: 0.642179  [250100/1106938]\n",
      "loss: 1.008392  [260100/1106938]\n",
      "loss: 0.768860  [270100/1106938]\n",
      "loss: 0.747595  [280100/1106938]\n",
      "loss: 0.942214  [290100/1106938]\n",
      "loss: 0.764499  [300100/1106938]\n",
      "loss: 0.611418  [310100/1106938]\n",
      "loss: 0.730241  [320100/1106938]\n",
      "loss: 0.854688  [330100/1106938]\n",
      "loss: 0.792721  [340100/1106938]\n",
      "loss: 0.862918  [350100/1106938]\n",
      "loss: 0.765421  [360100/1106938]\n",
      "loss: 0.850068  [370100/1106938]\n",
      "loss: 0.747779  [380100/1106938]\n",
      "loss: 0.770168  [390100/1106938]\n",
      "loss: 0.638429  [400100/1106938]\n",
      "loss: 1.184392  [410100/1106938]\n",
      "loss: 0.876606  [420100/1106938]\n",
      "loss: 0.686012  [430100/1106938]\n",
      "loss: 0.813657  [440100/1106938]\n",
      "loss: 0.974508  [450100/1106938]\n",
      "loss: 0.682644  [460100/1106938]\n",
      "loss: 0.891332  [470100/1106938]\n",
      "loss: 0.716560  [480100/1106938]\n",
      "loss: 0.667413  [490100/1106938]\n",
      "loss: 0.567369  [500100/1106938]\n",
      "loss: 0.848193  [510100/1106938]\n",
      "loss: 0.863027  [520100/1106938]\n",
      "loss: 0.708493  [530100/1106938]\n",
      "loss: 0.934077  [540100/1106938]\n",
      "loss: 0.804510  [550100/1106938]\n",
      "loss: 0.835678  [560100/1106938]\n",
      "loss: 0.788782  [570100/1106938]\n",
      "loss: 1.019921  [580100/1106938]\n",
      "loss: 0.836087  [590100/1106938]\n",
      "loss: 0.849643  [600100/1106938]\n",
      "loss: 0.634716  [610100/1106938]\n",
      "loss: 0.699494  [620100/1106938]\n",
      "loss: 0.525754  [630100/1106938]\n",
      "loss: 0.758900  [640100/1106938]\n",
      "loss: 0.829527  [650100/1106938]\n",
      "loss: 0.714433  [660100/1106938]\n",
      "loss: 0.989694  [670100/1106938]\n",
      "loss: 0.749581  [680100/1106938]\n",
      "loss: 0.702486  [690100/1106938]\n",
      "loss: 0.705654  [700100/1106938]\n",
      "loss: 0.663326  [710100/1106938]\n",
      "loss: 0.574266  [720100/1106938]\n",
      "loss: 0.808005  [730100/1106938]\n",
      "loss: 0.887081  [740100/1106938]\n",
      "loss: 0.647636  [750100/1106938]\n",
      "loss: 0.884012  [760100/1106938]\n",
      "loss: 0.836682  [770100/1106938]\n",
      "loss: 0.957506  [780100/1106938]\n",
      "loss: 0.724139  [790100/1106938]\n",
      "loss: 0.775298  [800100/1106938]\n",
      "loss: 0.991629  [810100/1106938]\n",
      "loss: 0.612661  [820100/1106938]\n",
      "loss: 0.614535  [830100/1106938]\n",
      "loss: 1.115623  [840100/1106938]\n",
      "loss: 0.699557  [850100/1106938]\n",
      "loss: 0.753222  [860100/1106938]\n",
      "loss: 0.856539  [870100/1106938]\n",
      "loss: 0.837157  [880100/1106938]\n",
      "loss: 0.806341  [890100/1106938]\n",
      "loss: 0.902815  [900100/1106938]\n",
      "loss: 0.789821  [910100/1106938]\n",
      "loss: 0.740463  [920100/1106938]\n",
      "loss: 0.745386  [930100/1106938]\n",
      "loss: 0.642370  [940100/1106938]\n",
      "loss: 0.947994  [950100/1106938]\n",
      "loss: 1.019665  [960100/1106938]\n",
      "loss: 0.881585  [970100/1106938]\n",
      "loss: 0.792318  [980100/1106938]\n",
      "loss: 0.653432  [990100/1106938]\n",
      "loss: 1.035759  [1000100/1106938]\n",
      "loss: 0.881682  [1010100/1106938]\n",
      "loss: 0.765730  [1020100/1106938]\n",
      "loss: 0.800943  [1030100/1106938]\n",
      "loss: 0.753836  [1040100/1106938]\n",
      "loss: 0.820056  [1050100/1106938]\n",
      "loss: 0.937770  [1060100/1106938]\n",
      "loss: 0.825502  [1070100/1106938]\n",
      "loss: 0.785468  [1080100/1106938]\n",
      "loss: 0.840442  [1090100/1106938]\n",
      "loss: 0.820297  [1100100/1106938]\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.717900 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.768324  [  100/1106938]\n",
      "loss: 0.771327  [10100/1106938]\n",
      "loss: 0.926497  [20100/1106938]\n",
      "loss: 0.813881  [30100/1106938]\n",
      "loss: 0.757353  [40100/1106938]\n",
      "loss: 0.863688  [50100/1106938]\n",
      "loss: 1.182565  [60100/1106938]\n",
      "loss: 0.811282  [70100/1106938]\n",
      "loss: 0.735856  [80100/1106938]\n",
      "loss: 0.747886  [90100/1106938]\n",
      "loss: 0.828700  [100100/1106938]\n",
      "loss: 0.717029  [110100/1106938]\n",
      "loss: 0.791107  [120100/1106938]\n",
      "loss: 0.643334  [130100/1106938]\n",
      "loss: 0.856898  [140100/1106938]\n",
      "loss: 0.811566  [150100/1106938]\n",
      "loss: 0.790411  [160100/1106938]\n",
      "loss: 0.914315  [170100/1106938]\n",
      "loss: 0.833069  [180100/1106938]\n",
      "loss: 0.690922  [190100/1106938]\n",
      "loss: 0.893598  [200100/1106938]\n",
      "loss: 0.752987  [210100/1106938]\n",
      "loss: 0.843802  [220100/1106938]\n",
      "loss: 0.915017  [230100/1106938]\n",
      "loss: 0.729297  [240100/1106938]\n",
      "loss: 0.765312  [250100/1106938]\n",
      "loss: 0.929857  [260100/1106938]\n",
      "loss: 0.849575  [270100/1106938]\n",
      "loss: 0.632809  [280100/1106938]\n",
      "loss: 0.899738  [290100/1106938]\n",
      "loss: 0.667240  [300100/1106938]\n",
      "loss: 0.727451  [310100/1106938]\n",
      "loss: 1.041785  [320100/1106938]\n",
      "loss: 0.869634  [330100/1106938]\n",
      "loss: 0.677095  [340100/1106938]\n",
      "loss: 0.878360  [350100/1106938]\n",
      "loss: 0.703478  [360100/1106938]\n",
      "loss: 0.860819  [370100/1106938]\n",
      "loss: 0.878566  [380100/1106938]\n",
      "loss: 0.665852  [390100/1106938]\n",
      "loss: 1.040379  [400100/1106938]\n",
      "loss: 0.829515  [410100/1106938]\n",
      "loss: 0.815245  [420100/1106938]\n",
      "loss: 0.806471  [430100/1106938]\n",
      "loss: 0.655040  [440100/1106938]\n",
      "loss: 0.935630  [450100/1106938]\n",
      "loss: 0.778045  [460100/1106938]\n",
      "loss: 0.777511  [470100/1106938]\n",
      "loss: 0.821128  [480100/1106938]\n",
      "loss: 0.698942  [490100/1106938]\n",
      "loss: 0.761021  [500100/1106938]\n",
      "loss: 0.957844  [510100/1106938]\n",
      "loss: 1.005324  [520100/1106938]\n",
      "loss: 0.972367  [530100/1106938]\n",
      "loss: 0.918375  [540100/1106938]\n",
      "loss: 0.800388  [550100/1106938]\n",
      "loss: 0.614855  [560100/1106938]\n",
      "loss: 0.928220  [570100/1106938]\n",
      "loss: 0.817262  [580100/1106938]\n",
      "loss: 0.709785  [590100/1106938]\n",
      "loss: 0.717858  [600100/1106938]\n",
      "loss: 0.546592  [610100/1106938]\n",
      "loss: 0.954516  [620100/1106938]\n",
      "loss: 0.631578  [630100/1106938]\n",
      "loss: 0.645989  [640100/1106938]\n",
      "loss: 0.880624  [650100/1106938]\n",
      "loss: 0.918273  [660100/1106938]\n",
      "loss: 0.919272  [670100/1106938]\n",
      "loss: 0.859625  [680100/1106938]\n",
      "loss: 0.752673  [690100/1106938]\n",
      "loss: 0.876891  [700100/1106938]\n",
      "loss: 0.841313  [710100/1106938]\n",
      "loss: 0.713938  [720100/1106938]\n",
      "loss: 0.638565  [730100/1106938]\n",
      "loss: 0.695915  [740100/1106938]\n",
      "loss: 0.878044  [750100/1106938]\n",
      "loss: 0.849729  [760100/1106938]\n",
      "loss: 0.922316  [770100/1106938]\n",
      "loss: 0.719249  [780100/1106938]\n",
      "loss: 0.905095  [790100/1106938]\n",
      "loss: 0.907494  [800100/1106938]\n",
      "loss: 0.908225  [810100/1106938]\n",
      "loss: 0.727389  [820100/1106938]\n",
      "loss: 1.214536  [830100/1106938]\n",
      "loss: 0.694135  [840100/1106938]\n",
      "loss: 0.774029  [850100/1106938]\n",
      "loss: 0.801272  [860100/1106938]\n",
      "loss: 0.812771  [870100/1106938]\n",
      "loss: 0.835512  [880100/1106938]\n",
      "loss: 1.080714  [890100/1106938]\n",
      "loss: 0.729951  [900100/1106938]\n",
      "loss: 0.845875  [910100/1106938]\n",
      "loss: 0.714083  [920100/1106938]\n",
      "loss: 0.658492  [930100/1106938]\n",
      "loss: 0.759209  [940100/1106938]\n",
      "loss: 0.851031  [950100/1106938]\n",
      "loss: 0.981424  [960100/1106938]\n",
      "loss: 0.973791  [970100/1106938]\n",
      "loss: 0.695576  [980100/1106938]\n",
      "loss: 0.821864  [990100/1106938]\n",
      "loss: 0.863242  [1000100/1106938]\n",
      "loss: 0.668839  [1010100/1106938]\n",
      "loss: 0.664404  [1020100/1106938]\n",
      "loss: 0.567282  [1030100/1106938]\n",
      "loss: 0.928683  [1040100/1106938]\n",
      "loss: 0.741737  [1050100/1106938]\n",
      "loss: 0.848552  [1060100/1106938]\n",
      "loss: 0.864749  [1070100/1106938]\n",
      "loss: 0.893601  [1080100/1106938]\n",
      "loss: 0.785097  [1090100/1106938]\n",
      "loss: 0.930672  [1100100/1106938]\n",
      "Test Error: \n",
      " Accuracy: 75.9%, Avg loss: 0.711852 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.759728  [  100/1106938]\n",
      "loss: 0.987571  [10100/1106938]\n",
      "loss: 0.728082  [20100/1106938]\n",
      "loss: 0.783773  [30100/1106938]\n",
      "loss: 0.842744  [40100/1106938]\n",
      "loss: 0.946089  [50100/1106938]\n",
      "loss: 0.874902  [60100/1106938]\n",
      "loss: 0.669236  [70100/1106938]\n",
      "loss: 0.847689  [80100/1106938]\n",
      "loss: 0.729250  [90100/1106938]\n",
      "loss: 1.033000  [100100/1106938]\n",
      "loss: 1.053754  [110100/1106938]\n",
      "loss: 0.772994  [120100/1106938]\n",
      "loss: 0.730298  [130100/1106938]\n",
      "loss: 0.832578  [140100/1106938]\n",
      "loss: 0.585333  [150100/1106938]\n",
      "loss: 0.902338  [160100/1106938]\n",
      "loss: 0.902809  [170100/1106938]\n",
      "loss: 0.722130  [180100/1106938]\n",
      "loss: 0.758886  [190100/1106938]\n",
      "loss: 0.828180  [200100/1106938]\n",
      "loss: 0.694029  [210100/1106938]\n",
      "loss: 0.682675  [220100/1106938]\n",
      "loss: 0.855225  [230100/1106938]\n",
      "loss: 0.771421  [240100/1106938]\n",
      "loss: 0.774222  [250100/1106938]\n",
      "loss: 0.798347  [260100/1106938]\n",
      "loss: 0.970174  [270100/1106938]\n",
      "loss: 0.801023  [280100/1106938]\n",
      "loss: 0.804145  [290100/1106938]\n",
      "loss: 0.713478  [300100/1106938]\n",
      "loss: 0.796169  [310100/1106938]\n",
      "loss: 0.898787  [320100/1106938]\n",
      "loss: 0.745761  [330100/1106938]\n",
      "loss: 0.700067  [340100/1106938]\n",
      "loss: 0.730278  [350100/1106938]\n",
      "loss: 0.740834  [360100/1106938]\n",
      "loss: 0.860429  [370100/1106938]\n",
      "loss: 0.647902  [380100/1106938]\n",
      "loss: 0.631959  [390100/1106938]\n",
      "loss: 0.782674  [400100/1106938]\n",
      "loss: 0.758613  [410100/1106938]\n",
      "loss: 0.640730  [420100/1106938]\n",
      "loss: 0.845944  [430100/1106938]\n",
      "loss: 0.883843  [440100/1106938]\n",
      "loss: 0.551940  [450100/1106938]\n",
      "loss: 0.632654  [460100/1106938]\n",
      "loss: 0.847174  [470100/1106938]\n",
      "loss: 0.795757  [480100/1106938]\n",
      "loss: 1.078786  [490100/1106938]\n",
      "loss: 0.886118  [500100/1106938]\n",
      "loss: 0.865821  [510100/1106938]\n",
      "loss: 0.702476  [520100/1106938]\n",
      "loss: 0.585291  [530100/1106938]\n",
      "loss: 0.788633  [540100/1106938]\n",
      "loss: 0.871010  [550100/1106938]\n",
      "loss: 0.728771  [560100/1106938]\n",
      "loss: 0.845494  [570100/1106938]\n",
      "loss: 0.931590  [580100/1106938]\n",
      "loss: 0.716357  [590100/1106938]\n",
      "loss: 0.785704  [600100/1106938]\n",
      "loss: 0.675640  [610100/1106938]\n",
      "loss: 0.961886  [620100/1106938]\n",
      "loss: 0.562054  [630100/1106938]\n",
      "loss: 0.726981  [640100/1106938]\n",
      "loss: 0.840475  [650100/1106938]\n",
      "loss: 0.845440  [660100/1106938]\n",
      "loss: 0.743431  [670100/1106938]\n",
      "loss: 0.598869  [680100/1106938]\n",
      "loss: 0.900780  [690100/1106938]\n",
      "loss: 0.891003  [700100/1106938]\n",
      "loss: 0.873119  [710100/1106938]\n",
      "loss: 0.845624  [720100/1106938]\n",
      "loss: 0.932965  [730100/1106938]\n",
      "loss: 0.809197  [740100/1106938]\n",
      "loss: 0.786208  [750100/1106938]\n",
      "loss: 0.685495  [760100/1106938]\n",
      "loss: 0.972611  [770100/1106938]\n",
      "loss: 0.604623  [780100/1106938]\n",
      "loss: 0.758047  [790100/1106938]\n",
      "loss: 0.753729  [800100/1106938]\n",
      "loss: 0.975761  [810100/1106938]\n",
      "loss: 0.804199  [820100/1106938]\n",
      "loss: 0.743250  [830100/1106938]\n",
      "loss: 0.965157  [840100/1106938]\n",
      "loss: 0.866778  [850100/1106938]\n",
      "loss: 0.790319  [860100/1106938]\n",
      "loss: 0.697749  [870100/1106938]\n",
      "loss: 0.860460  [880100/1106938]\n",
      "loss: 0.748096  [890100/1106938]\n",
      "loss: 0.849714  [900100/1106938]\n",
      "loss: 0.868077  [910100/1106938]\n",
      "loss: 0.796430  [920100/1106938]\n",
      "loss: 0.707158  [930100/1106938]\n",
      "loss: 0.746932  [940100/1106938]\n",
      "loss: 0.795934  [950100/1106938]\n",
      "loss: 0.807546  [960100/1106938]\n",
      "loss: 0.594184  [970100/1106938]\n",
      "loss: 0.820736  [980100/1106938]\n",
      "loss: 0.584024  [990100/1106938]\n",
      "loss: 0.754538  [1000100/1106938]\n",
      "loss: 0.938190  [1010100/1106938]\n",
      "loss: 0.754304  [1020100/1106938]\n",
      "loss: 0.811069  [1030100/1106938]\n",
      "loss: 0.738455  [1040100/1106938]\n",
      "loss: 0.748797  [1050100/1106938]\n",
      "loss: 0.759807  [1060100/1106938]\n",
      "loss: 0.927296  [1070100/1106938]\n",
      "loss: 0.627097  [1080100/1106938]\n",
      "loss: 0.780766  [1090100/1106938]\n",
      "loss: 0.789862  [1100100/1106938]\n",
      "Test Error: \n",
      " Accuracy: 75.9%, Avg loss: 0.709053 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.892206  [  100/1106938]\n",
      "loss: 0.801937  [10100/1106938]\n",
      "loss: 0.792332  [20100/1106938]\n",
      "loss: 0.680202  [30100/1106938]\n",
      "loss: 0.628041  [40100/1106938]\n",
      "loss: 0.789618  [50100/1106938]\n",
      "loss: 0.836920  [60100/1106938]\n",
      "loss: 0.703569  [70100/1106938]\n",
      "loss: 0.801957  [80100/1106938]\n",
      "loss: 0.749786  [90100/1106938]\n",
      "loss: 0.674730  [100100/1106938]\n",
      "loss: 0.689765  [110100/1106938]\n",
      "loss: 0.772974  [120100/1106938]\n",
      "loss: 0.893306  [130100/1106938]\n",
      "loss: 0.598891  [140100/1106938]\n",
      "loss: 0.826259  [150100/1106938]\n",
      "loss: 0.800748  [160100/1106938]\n",
      "loss: 0.728001  [170100/1106938]\n",
      "loss: 0.770978  [180100/1106938]\n",
      "loss: 0.861988  [190100/1106938]\n",
      "loss: 0.864204  [200100/1106938]\n",
      "loss: 0.746345  [210100/1106938]\n",
      "loss: 0.684442  [220100/1106938]\n",
      "loss: 0.482299  [230100/1106938]\n",
      "loss: 0.819291  [240100/1106938]\n",
      "loss: 0.915611  [250100/1106938]\n",
      "loss: 0.815045  [260100/1106938]\n",
      "loss: 0.838579  [270100/1106938]\n",
      "loss: 0.864318  [280100/1106938]\n",
      "loss: 0.846256  [290100/1106938]\n",
      "loss: 0.792623  [300100/1106938]\n",
      "loss: 0.705730  [310100/1106938]\n",
      "loss: 0.813782  [320100/1106938]\n",
      "loss: 0.799119  [330100/1106938]\n",
      "loss: 0.682696  [340100/1106938]\n",
      "loss: 0.705058  [350100/1106938]\n",
      "loss: 0.923524  [360100/1106938]\n",
      "loss: 0.723195  [370100/1106938]\n",
      "loss: 0.920945  [380100/1106938]\n",
      "loss: 0.623645  [390100/1106938]\n",
      "loss: 0.892069  [400100/1106938]\n",
      "loss: 0.696744  [410100/1106938]\n",
      "loss: 0.703745  [420100/1106938]\n",
      "loss: 0.759132  [430100/1106938]\n",
      "loss: 0.705398  [440100/1106938]\n",
      "loss: 0.885962  [450100/1106938]\n",
      "loss: 0.867185  [460100/1106938]\n",
      "loss: 0.655697  [470100/1106938]\n",
      "loss: 0.844037  [480100/1106938]\n",
      "loss: 0.810656  [490100/1106938]\n",
      "loss: 0.786726  [500100/1106938]\n",
      "loss: 0.901416  [510100/1106938]\n",
      "loss: 0.878254  [520100/1106938]\n",
      "loss: 0.820322  [530100/1106938]\n",
      "loss: 0.713059  [540100/1106938]\n",
      "loss: 0.662824  [550100/1106938]\n",
      "loss: 0.633209  [560100/1106938]\n",
      "loss: 0.942364  [570100/1106938]\n",
      "loss: 0.574849  [580100/1106938]\n",
      "loss: 0.853541  [590100/1106938]\n",
      "loss: 0.648977  [600100/1106938]\n",
      "loss: 0.850928  [610100/1106938]\n",
      "loss: 0.846065  [620100/1106938]\n",
      "loss: 0.691073  [630100/1106938]\n",
      "loss: 0.687337  [640100/1106938]\n",
      "loss: 0.698238  [650100/1106938]\n",
      "loss: 0.911979  [660100/1106938]\n",
      "loss: 0.762859  [670100/1106938]\n",
      "loss: 0.814936  [680100/1106938]\n",
      "loss: 0.647542  [690100/1106938]\n",
      "loss: 0.800084  [700100/1106938]\n",
      "loss: 0.763990  [710100/1106938]\n",
      "loss: 0.734650  [720100/1106938]\n",
      "loss: 0.538352  [730100/1106938]\n",
      "loss: 0.806379  [740100/1106938]\n",
      "loss: 0.889359  [750100/1106938]\n",
      "loss: 0.676977  [760100/1106938]\n",
      "loss: 0.862587  [770100/1106938]\n",
      "loss: 0.743529  [780100/1106938]\n",
      "loss: 0.696984  [790100/1106938]\n",
      "loss: 0.923156  [800100/1106938]\n",
      "loss: 0.710662  [810100/1106938]\n",
      "loss: 0.703171  [820100/1106938]\n",
      "loss: 0.792230  [830100/1106938]\n",
      "loss: 0.566182  [840100/1106938]\n",
      "loss: 0.764255  [850100/1106938]\n",
      "loss: 0.787505  [860100/1106938]\n",
      "loss: 0.830678  [870100/1106938]\n",
      "loss: 0.777214  [880100/1106938]\n",
      "loss: 0.841417  [890100/1106938]\n",
      "loss: 0.831738  [900100/1106938]\n",
      "loss: 1.008472  [910100/1106938]\n",
      "loss: 0.792339  [920100/1106938]\n",
      "loss: 0.936616  [930100/1106938]\n",
      "loss: 0.843789  [940100/1106938]\n",
      "loss: 0.926916  [950100/1106938]\n",
      "loss: 0.596077  [960100/1106938]\n",
      "loss: 0.720523  [970100/1106938]\n",
      "loss: 0.635027  [980100/1106938]\n",
      "loss: 0.734033  [990100/1106938]\n",
      "loss: 0.708809  [1000100/1106938]\n",
      "loss: 0.726735  [1010100/1106938]\n",
      "loss: 0.785736  [1020100/1106938]\n",
      "loss: 0.781299  [1030100/1106938]\n",
      "loss: 0.618671  [1040100/1106938]\n",
      "loss: 0.779749  [1050100/1106938]\n",
      "loss: 0.711334  [1060100/1106938]\n",
      "loss: 0.732854  [1070100/1106938]\n",
      "loss: 0.883952  [1080100/1106938]\n",
      "loss: 0.745309  [1090100/1106938]\n",
      "loss: 0.721460  [1100100/1106938]\n",
      "Test Error: \n",
      " Accuracy: 76.0%, Avg loss: 0.707899 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.718665  [  100/1106938]\n",
      "loss: 0.693131  [10100/1106938]\n",
      "loss: 0.656883  [20100/1106938]\n",
      "loss: 0.933877  [30100/1106938]\n",
      "loss: 0.728156  [40100/1106938]\n",
      "loss: 0.614421  [50100/1106938]\n",
      "loss: 0.925132  [60100/1106938]\n",
      "loss: 0.731350  [70100/1106938]\n",
      "loss: 0.631400  [80100/1106938]\n",
      "loss: 0.924601  [90100/1106938]\n",
      "loss: 0.870848  [100100/1106938]\n",
      "loss: 0.787307  [110100/1106938]\n",
      "loss: 0.760561  [120100/1106938]\n",
      "loss: 0.739183  [130100/1106938]\n",
      "loss: 0.737089  [140100/1106938]\n",
      "loss: 0.989222  [150100/1106938]\n",
      "loss: 0.802662  [160100/1106938]\n",
      "loss: 0.725216  [170100/1106938]\n",
      "loss: 0.599032  [180100/1106938]\n",
      "loss: 0.913954  [190100/1106938]\n",
      "loss: 0.745659  [200100/1106938]\n",
      "loss: 0.557961  [210100/1106938]\n",
      "loss: 0.853093  [220100/1106938]\n",
      "loss: 0.686943  [230100/1106938]\n",
      "loss: 0.812722  [240100/1106938]\n",
      "loss: 0.644013  [250100/1106938]\n",
      "loss: 0.671233  [260100/1106938]\n",
      "loss: 0.870139  [270100/1106938]\n",
      "loss: 0.836804  [280100/1106938]\n",
      "loss: 0.949525  [290100/1106938]\n",
      "loss: 0.941502  [300100/1106938]\n",
      "loss: 0.649071  [310100/1106938]\n",
      "loss: 0.769681  [320100/1106938]\n",
      "loss: 0.897399  [330100/1106938]\n",
      "loss: 0.747865  [340100/1106938]\n",
      "loss: 0.838903  [350100/1106938]\n",
      "loss: 0.826771  [360100/1106938]\n",
      "loss: 0.823260  [370100/1106938]\n",
      "loss: 0.736533  [380100/1106938]\n",
      "loss: 0.724259  [390100/1106938]\n",
      "loss: 1.048966  [400100/1106938]\n",
      "loss: 0.824738  [410100/1106938]\n",
      "loss: 0.906167  [420100/1106938]\n",
      "loss: 0.811827  [430100/1106938]\n",
      "loss: 0.721761  [440100/1106938]\n",
      "loss: 0.872626  [450100/1106938]\n",
      "loss: 0.737694  [460100/1106938]\n",
      "loss: 0.969995  [470100/1106938]\n",
      "loss: 0.867862  [480100/1106938]\n",
      "loss: 0.679039  [490100/1106938]\n",
      "loss: 0.648359  [500100/1106938]\n",
      "loss: 0.686388  [510100/1106938]\n",
      "loss: 0.898400  [520100/1106938]\n",
      "loss: 0.767945  [530100/1106938]\n",
      "loss: 0.644650  [540100/1106938]\n",
      "loss: 0.887315  [550100/1106938]\n",
      "loss: 0.879546  [560100/1106938]\n",
      "loss: 0.757755  [570100/1106938]\n",
      "loss: 0.646165  [580100/1106938]\n",
      "loss: 0.810298  [590100/1106938]\n",
      "loss: 0.695574  [600100/1106938]\n",
      "loss: 0.736373  [610100/1106938]\n",
      "loss: 0.662601  [620100/1106938]\n",
      "loss: 0.584802  [630100/1106938]\n",
      "loss: 0.716394  [640100/1106938]\n",
      "loss: 0.782669  [650100/1106938]\n",
      "loss: 0.757434  [660100/1106938]\n",
      "loss: 0.800286  [670100/1106938]\n",
      "loss: 0.950865  [680100/1106938]\n",
      "loss: 0.572398  [690100/1106938]\n",
      "loss: 0.833991  [700100/1106938]\n",
      "loss: 0.777266  [710100/1106938]\n",
      "loss: 0.884889  [720100/1106938]\n",
      "loss: 0.751646  [730100/1106938]\n",
      "loss: 0.639336  [740100/1106938]\n",
      "loss: 0.770155  [750100/1106938]\n",
      "loss: 0.748632  [760100/1106938]\n",
      "loss: 0.594662  [770100/1106938]\n",
      "loss: 0.770204  [780100/1106938]\n",
      "loss: 0.661668  [790100/1106938]\n",
      "loss: 0.849584  [800100/1106938]\n",
      "loss: 0.874272  [810100/1106938]\n",
      "loss: 0.829091  [820100/1106938]\n",
      "loss: 0.844078  [830100/1106938]\n",
      "loss: 0.722442  [840100/1106938]\n",
      "loss: 0.664205  [850100/1106938]\n",
      "loss: 0.869849  [860100/1106938]\n",
      "loss: 0.726635  [870100/1106938]\n",
      "loss: 0.774301  [880100/1106938]\n",
      "loss: 0.900115  [890100/1106938]\n",
      "loss: 0.507277  [900100/1106938]\n",
      "loss: 1.048276  [910100/1106938]\n",
      "loss: 0.887859  [920100/1106938]\n",
      "loss: 0.827927  [930100/1106938]\n",
      "loss: 0.943869  [940100/1106938]\n",
      "loss: 0.614447  [950100/1106938]\n",
      "loss: 0.712472  [960100/1106938]\n",
      "loss: 0.781238  [970100/1106938]\n",
      "loss: 0.775694  [980100/1106938]\n",
      "loss: 0.691455  [990100/1106938]\n",
      "loss: 0.735308  [1000100/1106938]\n",
      "loss: 0.656592  [1010100/1106938]\n",
      "loss: 0.668796  [1020100/1106938]\n",
      "loss: 0.895500  [1030100/1106938]\n",
      "loss: 0.732163  [1040100/1106938]\n",
      "loss: 0.716685  [1050100/1106938]\n",
      "loss: 0.907586  [1060100/1106938]\n",
      "loss: 0.883628  [1070100/1106938]\n",
      "loss: 0.681881  [1080100/1106938]\n",
      "loss: 0.886269  [1090100/1106938]\n",
      "loss: 0.900123  [1100100/1106938]\n",
      "Test Error: \n",
      " Accuracy: 76.1%, Avg loss: 0.703524 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "min_loss = 1.0\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    dev_loss = dev(dev_dataloader, model, loss_fn)\n",
    "\n",
    "    if dev_loss < min_loss:\n",
    "        min_loss = dev_loss\n",
    "        torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([36, 36, 36,  ..., 30, 30, 25])\n"
     ]
    }
   ],
   "source": [
    "def test(dataloader, model):\n",
    "    del model\n",
    "    model = NeuralNetwork()\n",
    "    ckpt = torch.load(\"model.pth\")\n",
    "    model.load_state_dict(ckpt)\n",
    "    model.eval()\n",
    "\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for X in dataloader:\n",
    "            pred = model(X)\n",
    "            pred = pred.squeeze()\n",
    "            preds.append(pred.argmax(1))\n",
    "    \n",
    "    preds = torch.cat(preds, dim=0).long()\n",
    "    print(preds)\n",
    "    save_result(preds)\n",
    "    \n",
    "test(test_dataloader, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
